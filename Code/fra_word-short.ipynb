{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation using word level language model and embeddings in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Building a english to french translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>gender</th>\n",
       "      <th>english_first_language</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>female_binary</th>\n",
       "      <th>male_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188608</th>\n",
       "      <td>285223.0</td>\n",
       "      <td>oh never mind  i misread the change  my mist...</td>\n",
       "      <td>2001</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3982</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676542</th>\n",
       "      <td>1266286.0</td>\n",
       "      <td>erik for crying out loud you legally can h...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>1727</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>hs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188610</th>\n",
       "      <td>1735419.0</td>\n",
       "      <td>pfortuny on pfortunys mediation  apologies ...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3982</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860295</th>\n",
       "      <td>2878618.0</td>\n",
       "      <td>the arbitration committee banned plautus sa...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>2727</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>some</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180799</th>\n",
       "      <td>4476405.0</td>\n",
       "      <td>wow youre so clever so smooth stop being an a...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rev_id                                            comment  year  \\\n",
       "188608    285223.0    oh never mind  i misread the change  my mist...  2001   \n",
       "676542   1266286.0      erik for crying out loud you legally can h...  2003   \n",
       "188610   1735419.0     pfortuny on pfortunys mediation  apologies ...  2003   \n",
       "860295   2878618.0     the arbitration committee banned plautus sa...  2004   \n",
       "1180799  4476405.0   wow youre so clever so smooth stop being an a...  2004   \n",
       "\n",
       "         logged_in       ns  sample  split  worker_id  toxicity  \\\n",
       "188608        True  article  random  train       3982         1   \n",
       "676542        True     user  random   test       1727         1   \n",
       "188610        True  article  random  train       3982         1   \n",
       "860295        True     user  random  train       2727         1   \n",
       "1180799      False  article  random  train       4096         1   \n",
       "\n",
       "         toxicity_score  gender  english_first_language age_group  education  \\\n",
       "188608             -2.0  female                     0.0     18-30  bachelors   \n",
       "676542             -2.0  female                     0.0     30-45         hs   \n",
       "188610             -2.0  female                     0.0     18-30  bachelors   \n",
       "860295             -2.0  female                     0.0     18-30       some   \n",
       "1180799            -2.0  female                     1.0     30-45  bachelors   \n",
       "\n",
       "         female_binary  male_binary  \n",
       "188608               1            0  \n",
       "676542               1            0  \n",
       "188610               1            0  \n",
       "860295               1            0  \n",
       "1180799              1            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lines= pd.read_table('fra.txt', names=['eng', 'fr'], index_col=False)\n",
    "# read files and create dataframe\n",
    "toxicity_comments = pd.read_csv('toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "toxicity_annotations = pd.read_csv('toxicity_annotations.tsv',  sep = '\\t')\n",
    "toxicity_demographics = pd.read_csv('toxicity_worker_demographics.tsv', sep = '\\t')\n",
    "\n",
    "toxicity = toxicity_comments.merge(toxicity_annotations, how ='outer', on=\"rev_id\")\n",
    "toxicity = toxicity.merge(toxicity_demographics, how ='outer', on=\"worker_id\").sort_values(by=['rev_id','worker_id'])\n",
    "\n",
    "# remove newline and tab tokens\n",
    "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "# add binary gender columns\n",
    "toxicity = toxicity[toxicity['gender']!='other']\n",
    "toxicity = pd.concat([toxicity, pd.get_dummies(toxicity.gender).rename(columns = \"{}_binary\".format)], axis = 1)\n",
    "\n",
    "# limit size of dataset for testing purposes\n",
    "very_toxic = toxicity[toxicity.toxicity_score == -2.0]\n",
    "lines = very_toxic[very_toxic.female_binary == 1]\n",
    "\n",
    "lines['comment'] = lines['comment'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "lines['comment'] = lines['comment'].replace('\\n',' ', regex=True)\n",
    "lines['comment'] = lines['comment'].str.lower()\n",
    "# female_vtoxic['comment'] = female_vtoxic['comment'].str.split() \n",
    "display(lines.head(5))\n",
    "\n",
    "# comments = female_vtoxic.comment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = lines[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>gender</th>\n",
       "      <th>english_first_language</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>female_binary</th>\n",
       "      <th>male_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344980</th>\n",
       "      <td>524883506.0</td>\n",
       "      <td>umm editor nyttend you posted the following at...</td>\n",
       "      <td>2012</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1314</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>masters</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100426</th>\n",
       "      <td>680945141.0</td>\n",
       "      <td>your moms yummy cum   fucking bastard you s...</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1268</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356695</th>\n",
       "      <td>420900758.0</td>\n",
       "      <td>im going to punch jim wales one day ill find...</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>2235</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>hs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950300</th>\n",
       "      <td>84539510.0</td>\n",
       "      <td>ask your mother miss or sir  and stop talking ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>3372</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372345</th>\n",
       "      <td>327697645.0</td>\n",
       "      <td>it would probably be helpful if you could find...</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>1101</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>hs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576055</th>\n",
       "      <td>60493059.0</td>\n",
       "      <td>youre full of shit</td>\n",
       "      <td>2006</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1598</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45-60</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470869</th>\n",
       "      <td>104976139.0</td>\n",
       "      <td>with regards to your comments on  back the f...</td>\n",
       "      <td>2007</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>3998</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525890</th>\n",
       "      <td>198536345.0</td>\n",
       "      <td>such distortion calls for dispute resolution ...</td>\n",
       "      <td>2008</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>4105</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64237</th>\n",
       "      <td>203234647.0</td>\n",
       "      <td>racist   now you know who you are and where...</td>\n",
       "      <td>2008</td>\n",
       "      <td>False</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>1062</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>masters</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538950</th>\n",
       "      <td>133316080.0</td>\n",
       "      <td>dont argue with that farrakhanistic moron  ...</td>\n",
       "      <td>2007</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>2868</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45-60</td>\n",
       "      <td>hs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rev_id                                            comment  year  \\\n",
       "344980   524883506.0  umm editor nyttend you posted the following at...  2012   \n",
       "1100426  680945141.0     your moms yummy cum   fucking bastard you s...  2015   \n",
       "356695   420900758.0    im going to punch jim wales one day ill find...  2011   \n",
       "950300    84539510.0  ask your mother miss or sir  and stop talking ...  2006   \n",
       "372345   327697645.0  it would probably be helpful if you could find...  2009   \n",
       "576055    60493059.0                               youre full of shit    2006   \n",
       "470869   104976139.0    with regards to your comments on  back the f...  2007   \n",
       "525890   198536345.0   such distortion calls for dispute resolution ...  2008   \n",
       "64237    203234647.0     racist   now you know who you are and where...  2008   \n",
       "1538950  133316080.0     dont argue with that farrakhanistic moron  ...  2007   \n",
       "\n",
       "         logged_in       ns   sample  split  worker_id  toxicity  \\\n",
       "344980        True     user   random  train       1314         1   \n",
       "1100426      False     user  blocked  train       1268         1   \n",
       "356695       False     user  blocked  train       2235         1   \n",
       "950300        True  article  blocked  train       3372         1   \n",
       "372345        True     user   random   test       1101         1   \n",
       "576055        True     user  blocked  train       1598         1   \n",
       "470869        True     user  blocked   test       3998         1   \n",
       "525890        True     user   random  train       4105         1   \n",
       "64237        False     user  blocked    dev       1062         1   \n",
       "1538950       True  article  blocked  train       2868         1   \n",
       "\n",
       "         toxicity_score  gender  english_first_language age_group  education  \\\n",
       "344980             -2.0  female                     0.0     30-45    masters   \n",
       "1100426            -2.0  female                     0.0     18-30  bachelors   \n",
       "356695             -2.0  female                     0.0     30-45         hs   \n",
       "950300             -2.0  female                     0.0     18-30  bachelors   \n",
       "372345             -2.0  female                     0.0     18-30         hs   \n",
       "576055             -2.0  female                     0.0     45-60  bachelors   \n",
       "470869             -2.0  female                     0.0     18-30  bachelors   \n",
       "525890             -2.0  female                     0.0     18-30  bachelors   \n",
       "64237              -2.0  female                     1.0     30-45    masters   \n",
       "1538950            -2.0  female                     0.0     45-60         hs   \n",
       "\n",
       "         female_binary  male_binary  \n",
       "344980               1            0  \n",
       "1100426              1            0  \n",
       "356695               1            0  \n",
       "950300               1            0  \n",
       "372345               1            0  \n",
       "576055               1            0  \n",
       "470869               1            0  \n",
       "525890               1            0  \n",
       "64237                1            0  \n",
       "1538950              1            0  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "#lines.fr=lines.eng.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Take the length as 50\n",
    "lines['eng']=lines.comment.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "lines['fr']=lines.comment.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude = set(string.punctuation)\n",
    "# lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "# lines.fr=lines.fr.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/pandas/core/generic.py:5168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "lines.fr=lines.fr.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>gender</th>\n",
       "      <th>english_first_language</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>female_binary</th>\n",
       "      <th>male_binary</th>\n",
       "      <th>eng</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188608</th>\n",
       "      <td>285223.0</td>\n",
       "      <td>oh never mind  i misread the change  my mist...</td>\n",
       "      <td>2001</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3982</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>oh never mind  i misread the change  my mist...</td>\n",
       "      <td>oh never mind  i misread the change  my mist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676542</th>\n",
       "      <td>1266286.0</td>\n",
       "      <td>erik for crying out loud you legally can h...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>1727</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>hs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>erik for crying out loud you legally can h...</td>\n",
       "      <td>erik for crying out loud you legally can h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188610</th>\n",
       "      <td>1735419.0</td>\n",
       "      <td>pfortuny on pfortunys mediation  apologies ...</td>\n",
       "      <td>2003</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3982</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pfortuny on pfortunys mediation  apologies ...</td>\n",
       "      <td>pfortuny on pfortunys mediation  apologies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860295</th>\n",
       "      <td>2878618.0</td>\n",
       "      <td>the arbitration committee banned plautus sa...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>2727</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>some</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the arbitration committee banned plautus sa...</td>\n",
       "      <td>the arbitration committee banned plautus sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180799</th>\n",
       "      <td>4476405.0</td>\n",
       "      <td>wow youre so clever so smooth stop being an a...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wow youre so clever so smooth stop being an a...</td>\n",
       "      <td>wow youre so clever so smooth stop being an a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rev_id                                            comment  year  \\\n",
       "188608    285223.0    oh never mind  i misread the change  my mist...  2001   \n",
       "676542   1266286.0      erik for crying out loud you legally can h...  2003   \n",
       "188610   1735419.0     pfortuny on pfortunys mediation  apologies ...  2003   \n",
       "860295   2878618.0     the arbitration committee banned plautus sa...  2004   \n",
       "1180799  4476405.0   wow youre so clever so smooth stop being an a...  2004   \n",
       "\n",
       "         logged_in       ns  sample  split  worker_id  toxicity  \\\n",
       "188608        True  article  random  train       3982         1   \n",
       "676542        True     user  random   test       1727         1   \n",
       "188610        True  article  random  train       3982         1   \n",
       "860295        True     user  random  train       2727         1   \n",
       "1180799      False  article  random  train       4096         1   \n",
       "\n",
       "         toxicity_score  gender  english_first_language age_group  education  \\\n",
       "188608             -2.0  female                     0.0     18-30  bachelors   \n",
       "676542             -2.0  female                     0.0     30-45         hs   \n",
       "188610             -2.0  female                     0.0     18-30  bachelors   \n",
       "860295             -2.0  female                     0.0     18-30       some   \n",
       "1180799            -2.0  female                     1.0     30-45  bachelors   \n",
       "\n",
       "         female_binary  male_binary  \\\n",
       "188608               1            0   \n",
       "676542               1            0   \n",
       "188610               1            0   \n",
       "860295               1            0   \n",
       "1180799              1            0   \n",
       "\n",
       "                                                       eng  \\\n",
       "188608     oh never mind  i misread the change  my mist...   \n",
       "676542       erik for crying out loud you legally can h...   \n",
       "188610      pfortuny on pfortunys mediation  apologies ...   \n",
       "860295      the arbitration committee banned plautus sa...   \n",
       "1180799   wow youre so clever so smooth stop being an a...   \n",
       "\n",
       "                                                        fr  \n",
       "188608     oh never mind  i misread the change  my mist...  \n",
       "676542       erik for crying out loud you legally can h...  \n",
       "188610      pfortuny on pfortunys mediation  apologies ...  \n",
       "860295      the arbitration committee banned plautus sa...  \n",
       "1180799   wow youre so clever so smooth stop being an a...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/pandas/core/generic.py:5168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "lines.fr = lines.fr.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31802\n",
      "18392\n",
      "31804\n",
      "18394\n"
     ]
    }
   ],
   "source": [
    "all_eng_words=set()\n",
    "counter = {}\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "            counter[word] = 1\n",
    "        else:\n",
    "            counter[word]+=1\n",
    "print(len(all_eng_words))\n",
    "for key in counter.keys():\n",
    "    if counter[key] == 1:\n",
    "        all_eng_words.remove(key)\n",
    "print(len(all_eng_words))\n",
    "    \n",
    "all_french_words=set()\n",
    "dict_fr = {}\n",
    "for fr in lines.fr:\n",
    "    for word in fr.split():\n",
    "        if word not in all_french_words:\n",
    "            all_french_words.add(word)\n",
    "            dict_fr[word] = 1\n",
    "        else:\n",
    "            dict_fr[word]+=1\n",
    "print(len(all_french_words))\n",
    "for key in dict_fr.keys():\n",
    "    if dict_fr[key] == 1:\n",
    "        all_french_words.remove(key)\n",
    "print(len(all_french_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18392, 18394)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words), len(all_french_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>gender</th>\n",
       "      <th>english_first_language</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>female_binary</th>\n",
       "      <th>male_binary</th>\n",
       "      <th>eng</th>\n",
       "      <th>fr</th>\n",
       "      <th>clean</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188608</th>\n",
       "      <td>285223.0</td>\n",
       "      <td>oh never mind  i misread the change  my mist...</td>\n",
       "      <td>2001</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>3982</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>oh never mind  i misread the change  my mist...</td>\n",
       "      <td>START_   oh never mind  i misread the change  ...</td>\n",
       "      <td>[START_, oh, never, mind, i, misread, the, cha...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860295</th>\n",
       "      <td>2878618.0</td>\n",
       "      <td>the arbitration committee banned plautus sa...</td>\n",
       "      <td>2004</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>2727</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>some</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the arbitration committee banned plautus sa...</td>\n",
       "      <td>START_    the arbitration committee banned pla...</td>\n",
       "      <td>[START_, the, arbitration, committee, banned, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180799</th>\n",
       "      <td>4476405.0</td>\n",
       "      <td>wow youre so clever so smooth stop being an a...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wow youre so clever so smooth stop being an a...</td>\n",
       "      <td>START_  wow youre so clever so smooth stop bei...</td>\n",
       "      <td>[START_, wow, youre, so, clever, so, smooth, s...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704969</th>\n",
       "      <td>4632658.0</td>\n",
       "      <td>i have a dick its bigger than yours hahaha</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>459</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45-60</td>\n",
       "      <td>masters</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i have a dick its bigger than yours hahaha</td>\n",
       "      <td>START_ i have a dick its bigger than yours hah...</td>\n",
       "      <td>[START_, i, have, a, dick, its, bigger, than, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313372</th>\n",
       "      <td>4788677.0</td>\n",
       "      <td>lowlevel languages such as machine code are ...</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>3037</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>lowlevel languages such as machine code are ...</td>\n",
       "      <td>START_   lowlevel languages such as machine co...</td>\n",
       "      <td>[START_, languages, such, as, machine, code, a...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rev_id                                            comment  year  \\\n",
       "188608    285223.0    oh never mind  i misread the change  my mist...  2001   \n",
       "860295   2878618.0     the arbitration committee banned plautus sa...  2004   \n",
       "1180799  4476405.0   wow youre so clever so smooth stop being an a...  2004   \n",
       "704969   4632658.0         i have a dick its bigger than yours hahaha  2004   \n",
       "1313372  4788677.0    lowlevel languages such as machine code are ...  2004   \n",
       "\n",
       "         logged_in       ns   sample  split  worker_id  toxicity  \\\n",
       "188608        True  article   random  train       3982         1   \n",
       "860295        True     user   random  train       2727         1   \n",
       "1180799      False  article   random  train       4096         1   \n",
       "704969       False  article  blocked  train        459         1   \n",
       "1313372      False  article   random    dev       3037         1   \n",
       "\n",
       "         toxicity_score  gender  english_first_language age_group  education  \\\n",
       "188608             -2.0  female                     0.0     18-30  bachelors   \n",
       "860295             -2.0  female                     0.0     18-30       some   \n",
       "1180799            -2.0  female                     1.0     30-45  bachelors   \n",
       "704969             -2.0  female                     0.0     45-60    masters   \n",
       "1313372            -2.0  female                     0.0     18-30  bachelors   \n",
       "\n",
       "         female_binary  male_binary  \\\n",
       "188608               1            0   \n",
       "860295               1            0   \n",
       "1180799              1            0   \n",
       "704969               1            0   \n",
       "1313372              1            0   \n",
       "\n",
       "                                                       eng  \\\n",
       "188608     oh never mind  i misread the change  my mist...   \n",
       "860295      the arbitration committee banned plautus sa...   \n",
       "1180799   wow youre so clever so smooth stop being an a...   \n",
       "704969          i have a dick its bigger than yours hahaha   \n",
       "1313372    lowlevel languages such as machine code are ...   \n",
       "\n",
       "                                                        fr  \\\n",
       "188608   START_   oh never mind  i misread the change  ...   \n",
       "860295   START_    the arbitration committee banned pla...   \n",
       "1180799  START_  wow youre so clever so smooth stop bei...   \n",
       "704969   START_ i have a dick its bigger than yours hah...   \n",
       "1313372  START_   lowlevel languages such as machine co...   \n",
       "\n",
       "                                                     clean  length  \n",
       "188608   [START_, oh, never, mind, i, misread, the, cha...      11  \n",
       "860295   [START_, the, arbitration, committee, banned, ...      29  \n",
       "1180799  [START_, wow, youre, so, clever, so, smooth, s...      16  \n",
       "704969   [START_, i, have, a, dick, its, bigger, than, ...      11  \n",
       "1313372  [START_, languages, such, as, machine, code, a...      25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "clean_list=[]\n",
    "for l in lines.fr:\n",
    "    line = l.split(' ')\n",
    "    new_line = []\n",
    "    for word in line:\n",
    "        if word in all_french_words:\n",
    "            new_line.append(word)\n",
    "    clean_list.append(new_line)\n",
    "lines['clean'] = clean_list\n",
    "lines['length'] = lines.apply (lambda row: len(row.clean), axis=1)\n",
    "lines =lines[lines['length']<60]\n",
    "display(lines.head(5))\n",
    "print(lines['length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['fr'] = lines.apply (lambda row: ' '.join(row.clean), axis=1)\n",
    "lines['eng'] = lines.apply (lambda row: ' '.join(row.clean[1:-1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenght_list=[]\n",
    "# for l in lines.eng:\n",
    "#     line = l.split(' ')\n",
    "#     new_line = []\n",
    "#     for word in line:\n",
    "#         if word in all_eng_words:\n",
    "#             new_line.append(word)\n",
    "#     lenght_list.append(len(new_line))\n",
    "# np.max(lenght_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_french_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_french_words)\n",
    "# del all_eng_words, all_french_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_idx = [(word, i) for i, word in enumerate(input_words)]\n",
    "input_token_index = {}\n",
    "for (word,i) in input_token_idx:\n",
    "    input_token_index[word] = i\n",
    "target_token_idx = [(word, i) for i, word in enumerate(target_words)]\n",
    "target_token_index = {}\n",
    "for (word,i) in target_token_idx:\n",
    "    target_token_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3215271200"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines.fr)*16*num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(lines.eng), 58), # Need to enter max words manually\n",
    "    dtype='uint16') # Changed from float 32\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(lines.fr), 60),\n",
    "    dtype='uint16') # Changed from float 32\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(lines.fr), 60, num_decoder_tokens),\n",
    "    dtype='bool') # Changed from float 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11261 10835 10197 ...     0     0     0]\n",
      " [16086   864  3115 ...     0     0     0]\n",
      " [17949 18196 14878 ...     0     0     0]\n",
      " ...\n",
      " [ 7974 14993 14647 ...     0     0     0]\n",
      " [ 7974 14993 14647 ...     0     0     0]\n",
      " [14562 16903 10197 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build keras encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "en_x=  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n",
    "encoder = LSTM(50, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "dex=  Embedding(num_decoder_tokens, embedding_size)\n",
    "\n",
    "final_dex= dex(decoder_inputs)\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, None, 50)     919600      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, None, 50)     919700      input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, 50), (None,  20200       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, None, 50), ( 20200       embedding_12[0][0]               \n",
      "                                                                 lstm_11[0][1]                    \n",
      "                                                                 lstm_11[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 18394)  938094      lstm_12[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,817,794\n",
      "Trainable params: 2,817,794\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10378 samples, validate on 547 samples\n",
      "Epoch 1/35\n",
      "10378/10378 [==============================] - 69s 7ms/step - loss: 2.6296 - acc: 0.0143 - val_loss: 2.2780 - val_acc: 0.0167\n",
      "Epoch 2/35\n",
      "10378/10378 [==============================] - 65s 6ms/step - loss: 2.2890 - acc: 0.0191 - val_loss: 2.2819 - val_acc: 0.0212\n",
      "Epoch 3/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.2620 - acc: 0.0219 - val_loss: 2.2798 - val_acc: 0.0206\n",
      "Epoch 4/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.2427 - acc: 0.0220 - val_loss: 2.2759 - val_acc: 0.0214\n",
      "Epoch 5/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.2242 - acc: 0.0222 - val_loss: 2.2663 - val_acc: 0.0218\n",
      "Epoch 6/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.2017 - acc: 0.0230 - val_loss: 2.2550 - val_acc: 0.0233\n",
      "Epoch 7/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.1785 - acc: 0.0260 - val_loss: 2.2449 - val_acc: 0.0241\n",
      "Epoch 8/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.1536 - acc: 0.0290 - val_loss: 2.2166 - val_acc: 0.0280\n",
      "Epoch 9/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.1284 - acc: 0.0317 - val_loss: 2.1975 - val_acc: 0.0287\n",
      "Epoch 10/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 2.1027 - acc: 0.0339 - val_loss: 2.1774 - val_acc: 0.0329\n",
      "Epoch 11/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 2.0795 - acc: 0.0364 - val_loss: 2.1622 - val_acc: 0.0353\n",
      "Epoch 12/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 2.0585 - acc: 0.0390 - val_loss: 2.1439 - val_acc: 0.0383\n",
      "Epoch 13/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 2.0378 - acc: 0.0411 - val_loss: 2.1276 - val_acc: 0.0406\n",
      "Epoch 14/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 2.0168 - acc: 0.0434 - val_loss: 2.1217 - val_acc: 0.0400\n",
      "Epoch 15/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 1.9965 - acc: 0.0453 - val_loss: 2.1039 - val_acc: 0.0434\n",
      "Epoch 16/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.9756 - acc: 0.0476 - val_loss: 2.0991 - val_acc: 0.0447\n",
      "Epoch 17/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.9552 - acc: 0.0496 - val_loss: 2.0834 - val_acc: 0.0467\n",
      "Epoch 18/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.9352 - acc: 0.0514 - val_loss: 2.0830 - val_acc: 0.0478\n",
      "Epoch 19/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 1.9166 - acc: 0.0532 - val_loss: 2.0713 - val_acc: 0.0471\n",
      "Epoch 20/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 1.8983 - acc: 0.0546 - val_loss: 2.0545 - val_acc: 0.0514\n",
      "Epoch 21/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 1.8812 - acc: 0.0559 - val_loss: 2.0464 - val_acc: 0.0520\n",
      "Epoch 22/35\n",
      "10378/10378 [==============================] - 66s 6ms/step - loss: 1.8642 - acc: 0.0571 - val_loss: 2.0433 - val_acc: 0.0527\n",
      "Epoch 23/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.8478 - acc: 0.0582 - val_loss: 2.0346 - val_acc: 0.0530\n",
      "Epoch 24/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.8327 - acc: 0.0594 - val_loss: 2.0419 - val_acc: 0.0514\n",
      "Epoch 25/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.8178 - acc: 0.0605 - val_loss: 2.0336 - val_acc: 0.0532\n",
      "Epoch 26/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.8035 - acc: 0.0617 - val_loss: 2.0267 - val_acc: 0.0545\n",
      "Epoch 27/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7897 - acc: 0.0631 - val_loss: 2.0290 - val_acc: 0.0544\n",
      "Epoch 28/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7761 - acc: 0.0645 - val_loss: 2.0223 - val_acc: 0.0554\n",
      "Epoch 29/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7631 - acc: 0.0659 - val_loss: 2.0258 - val_acc: 0.0556\n",
      "Epoch 30/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7507 - acc: 0.0668 - val_loss: 2.0181 - val_acc: 0.0569\n",
      "Epoch 31/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7384 - acc: 0.0681 - val_loss: 2.0161 - val_acc: 0.0565\n",
      "Epoch 32/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7265 - acc: 0.0690 - val_loss: 2.0139 - val_acc: 0.0581\n",
      "Epoch 33/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7148 - acc: 0.0703 - val_loss: 2.0118 - val_acc: 0.0577\n",
      "Epoch 34/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.7038 - acc: 0.0713 - val_loss: 2.0100 - val_acc: 0.0593\n",
      "Epoch 35/35\n",
      "10378/10378 [==============================] - 67s 6ms/step - loss: 1.6920 - acc: 0.0725 - val_loss: 2.0106 - val_acc: 0.0587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd168d525f8>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=35,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, None, 50)          919600    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 939,800\n",
      "Trainable params: 939,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2= dex(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_idx = [(i, char) for char, i in input_token_index.items()]\n",
    "reverse_input_char_index = {}\n",
    "for (word,i) in reverse_input_char_idx:\n",
    "    reverse_input_char_index[word] = i\n",
    "reverse_target_char_idx = [(i, char) for char, i in target_token_index.items()]\n",
    "reverse_target_char_index = {}\n",
    "for (word,i) in reverse_target_char_idx:\n",
    "    reverse_target_char_index[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the some translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing female very toxic\n",
      "-\n",
      "Input sentence: kiss my ass you stupid nigger jew fuckwit\n",
      "Decoded sentence:  you are a fucking faggot _END\n",
      "-\n",
      "Input sentence: aw youre so sweet too bad your a slutty panda hooker go suck a cock\n",
      "Decoded sentence:  your a page and i will be a life _END\n",
      "-\n",
      "Input sentence: im retired fuckers leave me alone\n",
      "Decoded sentence:  your a fucking page is a fucking faggot _END\n",
      "-\n",
      "Input sentence: therefore you are a pedo you fuck little boys\n",
      "Decoded sentence:  you are a fucking cunt and die _END\n",
      "-\n",
      "Input sentence: warning why did you warn me for what im war with and other jerks warn me for what\n",
      "Decoded sentence:  i am going to be a idiot to the article i am a fucking\n",
      "-\n",
      "Input sentence: there are other valid interpretations whats more interpretations that are backed up by sources bush didnt want to spread panic and if he had out of the room the footage would have been endlessly on television and it wouldnt have done much for national\n",
      "Decoded sentence:  the article of the article of the article i have been\n",
      "-\n",
      "Input sentence: i think this article is a somewhat derogatory form of original research phrase and it lacks references to say otherwise if anyone has any references of the use of this term in this fashion please add them\n",
      "Decoded sentence:  the article is a article i have been been a a lot of\n",
      "-\n",
      "Input sentence: oh ouch moreschi and ryanpostlethwaite fucking wankerish name i advise you find a nice boy and change your last name to match his you might like to consider that its time you both admit you have fetishes for curry pussy\n",
      "Decoded sentence:  i am going to get a life and i am going to get a life\n",
      "-\n",
      "Input sentence:  u suck loser go kill urself lol u bloody cow with retards as friends ur inbred dont ever tell the mooose wat not to do loser loser go back to china loser \n",
      "Decoded sentence:  hey fuck you and get a life and i will get a life and\n",
      "-\n",
      "Input sentence: i agree franklin was clearly joking unfortunately youll find that most wikipedians do not respond well to comments such as yours they prefer interpretations and descriptions anyway nice to see someone else on here isnt a complete idiot\n",
      "Decoded sentence:  the article is a article i have been a a lot of the article\n",
      "\n",
      "Testing male very toxic\n",
      "-\n",
      "Input sentence:    best player in the nba   rashard lewis if you dont agree fuck you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/nick/anaconda3/envs/mlenv/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence:  the article is a fucking page and the fuck off _END\n",
      "-\n",
      "Input sentence:    tu mom es a dirty slut    disgusting\n",
      "Decoded sentence:  is a fucking page _END\n",
      "-\n",
      "Input sentence:   congrats for being a total fucking faggot  \n",
      "Decoded sentence:  is a gay is a fucking _END\n",
      "-\n",
      "Input sentence:    what the hell stupid robots i didnt do anything   \n",
      "Decoded sentence:  the article is a fucking moron _END\n",
      "-\n",
      "Input sentence:  mineral county alternative schools \n",
      "Decoded sentence:  is a gay _END\n",
      "-\n",
      "Input sentence:   go to fuck looser\n",
      "Decoded sentence:  is a fucking faggot _END\n",
      "-\n",
      "Input sentence:    hey ole faggot   hey faggot you are an old man so why dont you retire faggot\n",
      "Decoded sentence:  i am a fucking article to be a fucking article to be\n",
      "-\n",
      "Input sentence:   antipederasty  some catholics accuse noncatholics who believe the catholic church should be held accountable for its sins especially pederasty of being anticatholic  likewise persons who object to aspects of catholic dogma are sometimes characterized as anticatholic   \n",
      "Decoded sentence:  the article of the article of the page i am not a fucking\n",
      "-\n",
      "Input sentence:    fuck you danish hater   fuck you arsehole muslim lover muslims control you you are a slave to them denmark is a great country\n",
      "Decoded sentence:  the article is a fucking idiot i am a fucking article\n",
      "-\n",
      "Input sentence:     cheers  die in a fire fianna failure\n",
      "Decoded sentence:  is a fucking faggot _END\n",
      "\n",
      "Testing female healthy\n",
      "-\n",
      "Input sentence:   modern excommunication mgr elio sgreccia of the pontifical academy for life has declared that the canonical laws on abortion would also apply to anyone who takes the abortion pill following the decision by italys drugs regulation agency to authorise its use    \n",
      "Decoded sentence:  is the article is a article is a article of the article\n",
      "-\n",
      "Input sentence: thats not quite right the point of watchmen was not to debunk moral absolutism but to make you question your own ethical values if moore wanted to debunk moral absolutism he would have shown the newspaper editers throwing out rorschachs journal at the end  \n",
      "Decoded sentence:  is the article is a article is a fucking article and\n",
      "-\n",
      "Input sentence:  alright im odd the excitement sa y w me through th day  illadd thoselinks hopefully tomorrow\n",
      "Decoded sentence:  is a page and the page of the ass and _END\n",
      "-\n",
      "Input sentence:    lol   lol seriously bryanfrompalatine  my ip resolves to cologne germany where i happen to reside i use no sockpuppets and when i edit per ip which i do mainly because its faster without all the monobook css gadgets whenever i remember to i include my only active username with the signature but please dont let me keep you from wasting your meagre brain ressources i just fucked my hot girlfriend for two hours she came 5 times me just 3 times life is unfair so im really calm as a hindu cow i laugh in your face pov pushing encyclopediaharming moron    logged out \n",
      "Decoded sentence:  i have to be a article to the article of the article\n",
      "-\n",
      "Input sentence:  yes i know that was what mikka did and you know what she was right surprised i agree with himher but i still thought it could be useful to provide my input there was a moment when i thought i could be like eg majorly etc i now realise thats not the case my place is a different one my kind of occasionally writing something combined with my dontgiveafuck gadfly attitude is just my way and i now realise that i somehow knew what you were talking about in the first place and i do agree with that as well lets make a pact if i ever run for admin we will both oppose it shall we no kidding i know that im not one of those guys so to cut to the chase sorry for all the inconvenience i posed i knew the answer already and i knew you were right with what you said if this appears weird to you you should experience me irl best regards btw now that we know each other i know you can generally be trusted net gain dorftrot \n",
      "Decoded sentence:  i have been been the article of the article of the article\n",
      "-\n",
      "Input sentence:    re   hello thanks for your response i am glad that you are willing to help for the moment there is a collaborative effort to improve the project anything you do for the project is appreciated and about the ideas thing i am certain that you can come up with some great ones and i will wait  them anyway thanks cs1kh hopefully i will see around wikiproject iraq soon   \n",
      "Decoded sentence:  is the article is a article is a article of the article\n",
      "-\n",
      "Input sentence:    like what you are doing   you have been really updating george jones page and i like what you have been doing i have also do alot to this page so keep on doing the great work   23 may 2006 148 pm\n",
      "Decoded sentence:  is the article is a article is a article of the article\n",
      "-\n",
      "Input sentence: i would like to second todds recommendation  among other things we need to verify who you are and that this is not a random person impersonating the real michael husbands if you can explain in more detail the issues that you have with the content on the article we can review it and work with you to clean up parts that may be wrong or inappropriate we do not as a matter of policy automatically let people decide what is ok content on wikipedia articles about them  were an encyclopedia not a pr website  however we do want to have accurate articles and we have a specific policy  wpblp  on biographical articles of living persons with the intent that we not have articles that wrongly or needlessly cause problems for living persons  we do appreciate it when people bring up concerns and talk about what issues they have please email infoenqwikimediaorg and hopefully we can resolve things from there thank you      \n",
      "Decoded sentence:  is the article is not the article is not the article\n",
      "-\n",
      "Input sentence:  but why we have multiple reliable sources that can be used and they have the added bonus of actually being right i would think that two different reliable sources that say otherwise would trump a single source   \n",
      "Decoded sentence:  i am not the article to be a article to do you to be\n",
      "-\n",
      "Input sentence:   alright first of all sorry for my english it seems quite wierd as i do not speak english most of the time as a beginning i do not agree on my lack of contribution even i do not edit the texts i have the texts edited like this moreover the mason trio including msjapan do not aswer my questions  and they have already arouse some discomfort amongst other editors  i may not know the wikipedian rules profoundly yet i know that this is not a personal forum site and no article page is closed to general criticism and brainstorming this is the reason why i got stuck with them  so they blueboar msjapan docboat and the article about freemasonary should be observed more closely this is not about masons i personally have friends who have mason relatives but the point is the mode they create in wikipedia it is not neutral at all   \n",
      "Decoded sentence:  is the article is not the article is not the article\n"
     ]
    }
   ],
   "source": [
    "#for seq_index in [14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
    "import random\n",
    "comments = list(lines.eng.values)\n",
    "sample = random.sample(comments,10)\n",
    "# for seq_index in [500,1000,1500,2000]:\n",
    "#     input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "#     decoded_sentence = decode_sequence(input_seq)\n",
    "#     print('-')\n",
    "#     print('Input sentence:', lines.eng[seq_index: seq_index + 1])\n",
    "#     print('Decoded sentence:', decoded_sentence)\n",
    "#\n",
    "\n",
    "print(\"Testing female very toxic\")\n",
    "encoder_data = np.zeros(\n",
    "    (len(lines.eng), 50), # Need to enter max words manually\n",
    "    dtype='uint16') # Changed from float 32\n",
    "for i,input_text in enumerate(sample):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_data[i, t] = input_token_index[word]\n",
    "for i in range(len(sample)):\n",
    "    print('-')\n",
    "    print('Input sentence:', sample[i])\n",
    "    decoded_sentence = decode_sequence(encoder_data[i:i+1])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    \n",
    "print(\"\\nTesting male very toxic\") \n",
    "male_vtoxic = very_toxic[very_toxic.male_binary == 1]\n",
    "male_vtoxic['comment'] = male_vtoxic['comment'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "male_vtoxic['comment'] = male_vtoxic['comment'].replace('\\n',' ', regex=True)\n",
    "male_vtoxic['comment'] = male_vtoxic['comment'].str.lower()\n",
    "comments = list(male_vtoxic.comment.values)\n",
    "sample = random.sample(comments,10)\n",
    "\n",
    "all_words=set()\n",
    "lengths = []\n",
    "for comment in sample:\n",
    "    for word in comment.split():\n",
    "        if word not in all_words:\n",
    "            all_words.add(word)\n",
    "        lengths.append(len(comment.split()))\n",
    "max_len = max(lengths)\n",
    "input_words = sorted(list(all_words))\n",
    "input_token_idx = [(word, i) for i, word in enumerate(input_words)]\n",
    "input_token_index = {}\n",
    "for (word,i) in input_token_idx:\n",
    "    input_token_index[word] = i\n",
    "    \n",
    "encoder_data = np.zeros(\n",
    "    (len(lines.eng), max_len), # Need to enter max words manually\n",
    "    dtype='uint16') # Changed from float 32\n",
    "for i,input_text in enumerate(sample):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_data[i, t] = input_token_index[word]\n",
    "for i in range(len(sample)):\n",
    "    print('-')\n",
    "    print('Input sentence:', sample[i])\n",
    "    decoded_sentence = decode_sequence(encoder_data[i:i+1])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "\n",
    "print(\"\\nTesting female healthy\")\n",
    "female_healthy = toxicity[toxicity.toxicity_score == 2.0]\n",
    "female_healthy = female_healthy[female_healthy.female_binary == 1]\n",
    "female_healthy['comment'] = female_healthy['comment'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "female_healthy['comment'] = female_healthy['comment'].replace('\\n',' ', regex=True)\n",
    "female_healthy['comment'] = female_healthy['comment'].str.lower()\n",
    "comments = list(female_healthy.comment.values)\n",
    "sample = random.sample(comments,10)\n",
    "\n",
    "all_words=set()\n",
    "lengths = []\n",
    "for comment in sample:\n",
    "    for word in comment.split():\n",
    "        if word not in all_words:\n",
    "            all_words.add(word)\n",
    "        lengths.append(len(comment.split()))\n",
    "max_len = max(lengths)\n",
    "input_words = sorted(list(all_words))\n",
    "input_token_idx = [(word, i) for i, word in enumerate(input_words)]\n",
    "input_token_index = {}\n",
    "for (word,i) in input_token_idx:\n",
    "    input_token_index[word] = i\n",
    "    \n",
    "encoder_data = np.zeros(\n",
    "    (len(lines.eng), max_len), # Need to enter max words manually\n",
    "    dtype='uint16') # Changed from float 32\n",
    "for i,input_text in enumerate(sample):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_data[i, t] = input_token_index[word]\n",
    "for i in range(len(sample)):\n",
    "    print('-')\n",
    "    print('Input sentence:', sample[i])\n",
    "    decoded_sentence = decode_sequence(encoder_data[i:i+1])\n",
    "    print('Decoded sentence:', decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
