{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ak_Ca3w_rNy"
   },
   "source": [
    "<h3> BERT classifier v5 trained on balanced data for male/female in healthy class, label = gender, tested gender predictions for healthy class</h3>\r\n",
    "<p>Using code from: https://mccormickml.com/2019/07/22/BERT-fine-tuning/</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBWkIenb_xH-",
    "outputId": "71954878-863d-4f44-cac5-7fc256f871b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# import libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "#!conda install -y tensorflow\r\n",
    "#!conda install -y pytorch torchvision -c pytorch\r\n",
    "#!pip install transformers\r\n",
    "import tensorflow as tf\r\n",
    "import torch\r\n",
    "\r\n",
    "from google.colab import drive\r\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "NrVW-FsUABeB",
    "outputId": "fd52b244-3413-4696-8f05-5e9a08579cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6579\n",
      "26316\n",
      "5263\n",
      "21053\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "      <th>gender</th>\n",
       "      <th>english_first_language</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "      <th>female_binary</th>\n",
       "      <th>male_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1119969</th>\n",
       "      <td>86077918.0</td>\n",
       "      <td>also where are the gymini on this page? which...</td>\n",
       "      <td>2006</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1348</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516196</th>\n",
       "      <td>631404404.0</td>\n",
       "      <td>`  == WRONG DATA ==  in this article all Perry...</td>\n",
       "      <td>2014</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1797</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45-60</td>\n",
       "      <td>professional</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666883</th>\n",
       "      <td>498354789.0</td>\n",
       "      <td>`  == Help ==  Hello, Tiparrish.I just saw wha...</td>\n",
       "      <td>2012</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>3071</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>professional</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726328</th>\n",
       "      <td>428331260.0</td>\n",
       "      <td>`I get what you're saying, and I was more than...</td>\n",
       "      <td>2011</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>hs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197853</th>\n",
       "      <td>415896680.0</td>\n",
       "      <td>:Yes, they may well do but that has nothing ...</td>\n",
       "      <td>2011</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>2201</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>hs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rev_id  ... male_binary\n",
       "1119969   86077918.0  ...           1\n",
       "1516196  631404404.0  ...           1\n",
       "666883   498354789.0  ...           1\n",
       "726328   428331260.0  ...           0\n",
       "1197853  415896680.0  ...           0\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read files and create dataframe\r\n",
    "toxicity_comments = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)\r\n",
    "toxicity_annotations = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/toxicity_annotations.tsv',  sep = '\\t')\r\n",
    "toxicity_demographics = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/toxicity_worker_demographics.tsv', sep = '\\t')\r\n",
    "\r\n",
    "toxicity = toxicity_comments.merge(toxicity_annotations, how ='outer', on=\"rev_id\")\r\n",
    "toxicity = toxicity.merge(toxicity_demographics, how ='outer', on=\"worker_id\").sort_values(by=['rev_id','worker_id'])\r\n",
    "\r\n",
    "# remove newline and tab tokens\r\n",
    "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\r\n",
    "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\r\n",
    "\r\n",
    "# add binary gender columns\r\n",
    "toxicity = toxicity[toxicity['gender']!='other']\r\n",
    "toxicity = pd.concat([toxicity, pd.get_dummies(toxicity.gender).rename(columns = \"{}_binary\".format)], axis = 1)\r\n",
    "\r\n",
    "# limit size of dataset for testing purposes\r\n",
    "very_toxic = toxicity[toxicity.toxicity_score == -2.0]\r\n",
    "toxic = toxicity[toxicity.toxicity_score == -1.0]\r\n",
    "neutral = toxicity[toxicity.toxicity_score == 0.0]\r\n",
    "nontoxic = toxicity[toxicity.toxicity_score == 1.0]\r\n",
    "healthy = toxicity[toxicity.toxicity_score == 2.0]\r\n",
    "\r\n",
    "female_nontoxic = nontoxic[nontoxic.female_binary == 1]\r\n",
    "male_nontoxic = nontoxic[nontoxic.male_binary == 1]\r\n",
    "female_healthy = healthy[healthy.female_binary == 1]\r\n",
    "male_healthy = healthy[healthy.male_binary == 1]\r\n",
    "\r\n",
    "size = min(female_nontoxic.shape[0],male_nontoxic.shape[0],female_healthy.shape[0],male_healthy.shape[0])\r\n",
    "print(size)\r\n",
    "# size = 25000\r\n",
    "\r\n",
    "healthy_data = pd.concat([female_nontoxic.sample(size), male_nontoxic.sample(size),female_healthy.sample(size), male_healthy.sample(size)])\r\n",
    "healthy_data = healthy_data.sample(frac=1)\r\n",
    "print(healthy_data.shape[0])\r\n",
    "test_data = healthy_data.sample(frac = 0.2)\r\n",
    "print(test_data.shape[0])\r\n",
    "healthy_data = healthy_data.drop(test_data.index)\r\n",
    "\r\n",
    "print(healthy_data.shape[0])\r\n",
    "display (healthy_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HM4vJigyDP3j",
    "outputId": "23e364dc-3f6d-41fd-dc1c-71cd8cb9d368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# try to use gpu\r\n",
    "if torch.cuda.is_available():      \r\n",
    "    device = torch.device(\"cuda\")\r\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
    "\r\n",
    "else:\r\n",
    "    print('No GPU available, using the CPU instead.')\r\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O5Q5ntvhDWxN"
   },
   "outputs": [],
   "source": [
    "# extract relevant information\r\n",
    "comments = healthy_data.comment.values\r\n",
    "labels = healthy_data.female_binary.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687,
     "referenced_widgets": [
      "46eda40062da431cb8e79b51928efd0f",
      "0eae4ed219fc453e9bd26848d533f451",
      "5a0b9189e17b41178b0d07aa3b65e3d6",
      "cb8a49f7be7146f0a23c3fd5d6cb541f",
      "b927b4ca4bad4b519e9ecb564eac41f0",
      "8089cfc0fac941dfbcfc9b183c6fc77f",
      "0b7b023d9c6f4077b238577653e77968",
      "2205967556914c50a79455b6dc095ba2"
     ]
    },
    "id": "UhlZR6pXD1Jl",
    "outputId": "1e86c2d1-c4e0-4327-e3ef-b09cafe730bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 7.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (1.19.5)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 27.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (4.41.1)\n",
      "Collecting tokenizers==0.9.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 49.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.12.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.0.12)\n",
      "Collecting sentencepiece==0.1.91\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 47.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (20.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1) (51.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=66fb1965c96682f4f51584c8809d3878e41bc9089034abcd189222a059af3eff\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46eda40062da431cb8e79b51928efd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==3.5.1\r\n",
    "from transformers import BertTokenizer\r\n",
    "\r\n",
    "# Load the BERT tokenizer.\r\n",
    "print('Loading BERT tokenizer...')\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3gKFb5wD7eS",
    "outputId": "61a8eb08-40f4-4a55-ec94-e3f4afd5cceb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs.\r\n",
    "input_ids = []\r\n",
    "attention_masks = []\r\n",
    "\r\n",
    "# For every sentence...\r\n",
    "for cmt in comments:\r\n",
    "    # `encode_plus` will:\r\n",
    "    #   (1) Tokenize the sentence.\r\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\r\n",
    "    #   (3) Append the `[SEP]` token to the end.\r\n",
    "    #   (4) Map tokens to their IDs.\r\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\r\n",
    "    #   (6) Create attention masks for [PAD] tokens.\r\n",
    "    encoded_dict = tokenizer.encode_plus(\r\n",
    "                        cmt,                      # Sentence to encode.\r\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
    "                        max_length = 250,           # Pad & truncate all sentences.\r\n",
    "                        pad_to_max_length = True,\r\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\r\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\r\n",
    "                   )\r\n",
    "    \r\n",
    "    # Add the encoded sentence to the list.    \r\n",
    "    input_ids.append(encoded_dict['input_ids'])\r\n",
    "    \r\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\r\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
    "\r\n",
    "# Convert the lists into tensors.\r\n",
    "input_ids = torch.cat(input_ids, dim=0)\r\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
    "labels = torch.tensor(labels)\r\n",
    "\r\n",
    "# # Print sentence 0, now as a list of IDs.\r\n",
    "# print('Original: ', comments[0],scores[0])\r\n",
    "# print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaaSOCAIEZq-",
    "outputId": "2ad740fe-d497-469f-883e-0099ec80ded3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18,947 training samples\n",
      "2,106 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\r\n",
    "\r\n",
    "# Combine the training inputs into a TensorDataset.\r\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
    "\r\n",
    "# Create a 90-10 train-validation split.\r\n",
    "\r\n",
    "# Calculate the number of samples to include in each set.\r\n",
    "train_size = int(0.9 * len(dataset))\r\n",
    "val_size = len(dataset) - train_size\r\n",
    "\r\n",
    "# Divide the dataset by randomly selecting samples.\r\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
    "\r\n",
    "print('{:>5,} training samples'.format(train_size))\r\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LHf1KGY_GvSj"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
    "\r\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \r\n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n",
    "# size of 16 or 32.\r\n",
    "batch_size = 16\r\n",
    "\r\n",
    "# Create the DataLoaders for our training and validation sets.\r\n",
    "# We'll take training samples in random order. \r\n",
    "train_dataloader = DataLoader(\r\n",
    "            train_dataset,  # The training samples.\r\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\r\n",
    "            batch_size = batch_size # Trains with this batch size.\r\n",
    "        )\r\n",
    "\r\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\r\n",
    "validation_dataloader = DataLoader(\r\n",
    "            val_dataset, # The validation samples.\r\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n",
    "            batch_size = batch_size # Evaluate with this batch size\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b8fba9f3f87f412a960335af97128fbd",
      "4de423a8edae442294fc162f5c621ea9",
      "63cb15b41d484fdca18a3d3d4411ec42",
      "455ab4311729400a9502b829c1703984",
      "42f969d278a24482b8505c69c012f4da",
      "96a74738aa51488ab52221131b79feb9",
      "fbde772b09d343798e684cabedf04044",
      "3a80d3af2e334a06a0c156a9e9745dc4",
      "51516d65d62a48e49efadbea2151b4ba",
      "cf7adaf9d18e45c3a2b491a108df356c",
      "038a94e3fd194f43ba9048afc7f61d40",
      "2c0f08fcb7ce4b9d9422a139f4398870",
      "ccedb21ba3934cfcaedb351bae8fc6f5",
      "26a4dc9830f44df494cef1f9c55a58ad",
      "cf9ea15ac15242ea9334b76b5b599b9f",
      "5e252a2a98ba4486896994d2bdd5910c"
     ]
    },
    "id": "2Z5LPc0pG265",
    "outputId": "5c585e16-67c8-4c3c-dddb-d771012a459e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fba9f3f87f412a960335af97128fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51516d65d62a48e49efadbea2151b4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
    "\r\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \r\n",
    "# linear classification layer on top. \r\n",
    "model = BertForSequenceClassification.from_pretrained(\r\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\r\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\r\n",
    "                    # You can increase this for multi-class tasks.   \r\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\r\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
    ")\r\n",
    "\r\n",
    "# # Tell pytorch to run this model on the GPU.\r\n",
    "model.cuda()\r\n",
    "#model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AksknLguH3My"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \r\n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\r\n",
    "optimizer = AdamW(model.parameters(),\r\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ioeJ5LI8H5QS"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \r\n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\r\n",
    "# training data.\r\n",
    "epochs = 3\r\n",
    "\r\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
    "# (Note that this is not the same as the number of training samples).\r\n",
    "total_steps = len(train_dataloader) * epochs\r\n",
    "\r\n",
    "# Create the learning rate scheduler.\r\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\r\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AS__7IZPH-v7"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\r\n",
    "def flat_accuracy(preds, labels):\r\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
    "    labels_flat = labels.flatten()\r\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rR6fseNsH__N"
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "import datetime\r\n",
    "\r\n",
    "def format_time(elapsed):\r\n",
    "    '''\r\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
    "    '''\r\n",
    "    # Round to the nearest second.\r\n",
    "    elapsed_rounded = int(round((elapsed)))\r\n",
    "    \r\n",
    "    # Format as hh:mm:ss\r\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBM-WqAWIDbS",
    "outputId": "cc3c1f88-c22f-46a2-dfdf-f220d8874fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,185.    Elapsed: 0:00:26.\n",
      "  Batch    80  of  1,185.    Elapsed: 0:00:53.\n",
      "  Batch   120  of  1,185.    Elapsed: 0:01:21.\n",
      "  Batch   160  of  1,185.    Elapsed: 0:01:51.\n",
      "  Batch   200  of  1,185.    Elapsed: 0:02:21.\n",
      "  Batch   240  of  1,185.    Elapsed: 0:02:50.\n",
      "  Batch   280  of  1,185.    Elapsed: 0:03:19.\n",
      "  Batch   320  of  1,185.    Elapsed: 0:03:49.\n",
      "  Batch   360  of  1,185.    Elapsed: 0:04:18.\n",
      "  Batch   400  of  1,185.    Elapsed: 0:04:48.\n",
      "  Batch   440  of  1,185.    Elapsed: 0:05:17.\n",
      "  Batch   480  of  1,185.    Elapsed: 0:05:47.\n",
      "  Batch   520  of  1,185.    Elapsed: 0:06:17.\n",
      "  Batch   560  of  1,185.    Elapsed: 0:06:46.\n",
      "  Batch   600  of  1,185.    Elapsed: 0:07:16.\n",
      "  Batch   640  of  1,185.    Elapsed: 0:07:45.\n",
      "  Batch   680  of  1,185.    Elapsed: 0:08:15.\n",
      "  Batch   720  of  1,185.    Elapsed: 0:08:44.\n",
      "  Batch   760  of  1,185.    Elapsed: 0:09:14.\n",
      "  Batch   800  of  1,185.    Elapsed: 0:09:43.\n",
      "  Batch   840  of  1,185.    Elapsed: 0:10:13.\n",
      "  Batch   880  of  1,185.    Elapsed: 0:10:42.\n",
      "  Batch   920  of  1,185.    Elapsed: 0:11:12.\n",
      "  Batch   960  of  1,185.    Elapsed: 0:11:41.\n",
      "  Batch 1,000  of  1,185.    Elapsed: 0:12:11.\n",
      "  Batch 1,040  of  1,185.    Elapsed: 0:12:40.\n",
      "  Batch 1,080  of  1,185.    Elapsed: 0:13:10.\n",
      "  Batch 1,120  of  1,185.    Elapsed: 0:13:40.\n",
      "  Batch 1,160  of  1,185.    Elapsed: 0:14:09.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epoch took: 0:14:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:35\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,185.    Elapsed: 0:00:30.\n",
      "  Batch    80  of  1,185.    Elapsed: 0:00:59.\n",
      "  Batch   120  of  1,185.    Elapsed: 0:01:29.\n",
      "  Batch   160  of  1,185.    Elapsed: 0:01:58.\n",
      "  Batch   200  of  1,185.    Elapsed: 0:02:28.\n",
      "  Batch   240  of  1,185.    Elapsed: 0:02:57.\n",
      "  Batch   280  of  1,185.    Elapsed: 0:03:27.\n",
      "  Batch   320  of  1,185.    Elapsed: 0:03:57.\n",
      "  Batch   360  of  1,185.    Elapsed: 0:04:26.\n",
      "  Batch   400  of  1,185.    Elapsed: 0:04:56.\n",
      "  Batch   440  of  1,185.    Elapsed: 0:05:25.\n",
      "  Batch   480  of  1,185.    Elapsed: 0:05:55.\n",
      "  Batch   520  of  1,185.    Elapsed: 0:06:25.\n",
      "  Batch   560  of  1,185.    Elapsed: 0:06:54.\n",
      "  Batch   600  of  1,185.    Elapsed: 0:07:24.\n",
      "  Batch   640  of  1,185.    Elapsed: 0:07:53.\n",
      "  Batch   680  of  1,185.    Elapsed: 0:08:23.\n",
      "  Batch   720  of  1,185.    Elapsed: 0:08:52.\n",
      "  Batch   760  of  1,185.    Elapsed: 0:09:22.\n",
      "  Batch   800  of  1,185.    Elapsed: 0:09:52.\n",
      "  Batch   840  of  1,185.    Elapsed: 0:10:21.\n",
      "  Batch   880  of  1,185.    Elapsed: 0:10:51.\n",
      "  Batch   920  of  1,185.    Elapsed: 0:11:20.\n",
      "  Batch   960  of  1,185.    Elapsed: 0:11:50.\n",
      "  Batch 1,000  of  1,185.    Elapsed: 0:12:19.\n",
      "  Batch 1,040  of  1,185.    Elapsed: 0:12:49.\n",
      "  Batch 1,080  of  1,185.    Elapsed: 0:13:18.\n",
      "  Batch 1,120  of  1,185.    Elapsed: 0:13:48.\n",
      "  Batch 1,160  of  1,185.    Elapsed: 0:14:18.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoch took: 0:14:36\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:35\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of  1,185.    Elapsed: 0:00:30.\n",
      "  Batch    80  of  1,185.    Elapsed: 0:00:59.\n",
      "  Batch   120  of  1,185.    Elapsed: 0:01:29.\n",
      "  Batch   160  of  1,185.    Elapsed: 0:01:58.\n",
      "  Batch   200  of  1,185.    Elapsed: 0:02:28.\n",
      "  Batch   240  of  1,185.    Elapsed: 0:02:57.\n",
      "  Batch   280  of  1,185.    Elapsed: 0:03:27.\n",
      "  Batch   320  of  1,185.    Elapsed: 0:03:56.\n",
      "  Batch   360  of  1,185.    Elapsed: 0:04:26.\n",
      "  Batch   400  of  1,185.    Elapsed: 0:04:56.\n",
      "  Batch   440  of  1,185.    Elapsed: 0:05:25.\n",
      "  Batch   480  of  1,185.    Elapsed: 0:05:55.\n",
      "  Batch   520  of  1,185.    Elapsed: 0:06:25.\n",
      "  Batch   560  of  1,185.    Elapsed: 0:06:54.\n",
      "  Batch   600  of  1,185.    Elapsed: 0:07:24.\n",
      "  Batch   640  of  1,185.    Elapsed: 0:07:53.\n",
      "  Batch   680  of  1,185.    Elapsed: 0:08:23.\n",
      "  Batch   720  of  1,185.    Elapsed: 0:08:52.\n",
      "  Batch   760  of  1,185.    Elapsed: 0:09:22.\n",
      "  Batch   800  of  1,185.    Elapsed: 0:09:52.\n",
      "  Batch   840  of  1,185.    Elapsed: 0:10:21.\n",
      "  Batch   880  of  1,185.    Elapsed: 0:10:51.\n",
      "  Batch   920  of  1,185.    Elapsed: 0:11:20.\n",
      "  Batch   960  of  1,185.    Elapsed: 0:11:50.\n",
      "  Batch 1,000  of  1,185.    Elapsed: 0:12:19.\n",
      "  Batch 1,040  of  1,185.    Elapsed: 0:12:49.\n",
      "  Batch 1,080  of  1,185.    Elapsed: 0:13:19.\n",
      "  Batch 1,120  of  1,185.    Elapsed: 0:13:48.\n",
      "  Batch 1,160  of  1,185.    Elapsed: 0:14:18.\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epoch took: 0:14:36\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.52\n",
      "  Validation Loss: 0.72\n",
      "  Validation took: 0:00:35\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:45:22 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\r\n",
    "\r\n",
    "# This training code is based on the `run_glue.py` script here:\r\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\r\n",
    "\r\n",
    "# Set the seed value all over the place to make this reproducible.\r\n",
    "seed_val = 42\r\n",
    "\r\n",
    "random.seed(seed_val)\r\n",
    "np.random.seed(seed_val)\r\n",
    "torch.manual_seed(seed_val)\r\n",
    "torch.cuda.manual_seed_all(seed_val)\r\n",
    "\r\n",
    "# We'll store a number of quantities such as training and validation loss, \r\n",
    "# validation accuracy, and timings.\r\n",
    "training_stats = []\r\n",
    "\r\n",
    "# Measure the total training time for the whole run.\r\n",
    "total_t0 = time.time()\r\n",
    "\r\n",
    "# For each epoch...\r\n",
    "for epoch_i in range(0, epochs):\r\n",
    "    \r\n",
    "    # ========================================\r\n",
    "    #               Training\r\n",
    "    # ========================================\r\n",
    "    \r\n",
    "    # Perform one full pass over the training set.\r\n",
    "\r\n",
    "    print(\"\")\r\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
    "    print('Training...')\r\n",
    "\r\n",
    "    # Measure how long the training epoch takes.\r\n",
    "    t0 = time.time()\r\n",
    "\r\n",
    "    # Reset the total loss for this epoch.\r\n",
    "    total_train_loss = 0\r\n",
    "\r\n",
    "    # Put the model into training mode. Don't be misled--the call to \r\n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\r\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    # For each batch of training data...\r\n",
    "    for step, batch in enumerate(train_dataloader):\r\n",
    "\r\n",
    "        # Progress update every 40 batches.\r\n",
    "        if step % 40 == 0 and not step == 0:\r\n",
    "            # Calculate elapsed time in minutes.\r\n",
    "            elapsed = format_time(time.time() - t0)\r\n",
    "            \r\n",
    "            # Report progress.\r\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
    "\r\n",
    "        # Unpack this training batch from our dataloader. \r\n",
    "        #\r\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
    "        # `to` method.\r\n",
    "        #\r\n",
    "        # `batch` contains three pytorch tensors:\r\n",
    "        #   [0]: input ids \r\n",
    "        #   [1]: attention masks\r\n",
    "        #   [2]: labels \r\n",
    "        b_input_ids = batch[0].to(device)\r\n",
    "        b_input_mask = batch[1].to(device)\r\n",
    "        b_labels = batch[2].to(device)\r\n",
    "\r\n",
    "        # Always clear any previously calculated gradients before performing a\r\n",
    "        # backward pass. PyTorch doesn't do this automatically because \r\n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \r\n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
    "        model.zero_grad()        \r\n",
    "\r\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\r\n",
    "        # The documentation for this `model` function is here: \r\n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
    "        # It returns different numbers of parameters depending on what arguments\r\n",
    "        # arge given and what flags are set. For our useage here, it returns\r\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\r\n",
    "        # outputs prior to activation.\r\n",
    "        loss, logits = model(b_input_ids, \r\n",
    "                             token_type_ids=None, \r\n",
    "                             attention_mask=b_input_mask, \r\n",
    "                             labels=b_labels.long())\r\n",
    "\r\n",
    "        # Accumulate the training loss over all of the batches so that we can\r\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
    "        # single value; the `.item()` function just returns the Python value \r\n",
    "        # from the tensor.\r\n",
    "        total_train_loss += loss.item()\r\n",
    "\r\n",
    "        # Perform a backward pass to calculate the gradients.\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        # Clip the norm of the gradients to 1.0.\r\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\r\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
    "\r\n",
    "        # Update parameters and take a step using the computed gradient.\r\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
    "        # modified based on their gradients, the learning rate, etc.\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        # Update the learning rate.\r\n",
    "        scheduler.step()\r\n",
    "\r\n",
    "    # Calculate the average loss over all of the batches.\r\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
    "    \r\n",
    "    # Measure how long this epoch took.\r\n",
    "    training_time = format_time(time.time() - t0)\r\n",
    "\r\n",
    "    print(\"\")\r\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\r\n",
    "        \r\n",
    "    # ========================================\r\n",
    "    #               Validation\r\n",
    "    # ========================================\r\n",
    "    # After the completion of each training epoch, measure our performance on\r\n",
    "    # our validation set.\r\n",
    "\r\n",
    "    print(\"\")\r\n",
    "    print(\"Running Validation...\")\r\n",
    "\r\n",
    "    t0 = time.time()\r\n",
    "\r\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\r\n",
    "    # during evaluation.\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    # Tracking variables \r\n",
    "    total_eval_accuracy = 0\r\n",
    "    total_eval_loss = 0\r\n",
    "    nb_eval_steps = 0\r\n",
    "\r\n",
    "    # Evaluate data for one epoch\r\n",
    "    for batch in validation_dataloader:\r\n",
    "        \r\n",
    "        # Unpack this training batch from our dataloader. \r\n",
    "        #\r\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \r\n",
    "        # the `to` method.\r\n",
    "        #\r\n",
    "        # `batch` contains three pytorch tensors:\r\n",
    "        #   [0]: input ids \r\n",
    "        #   [1]: attention masks\r\n",
    "        #   [2]: labels \r\n",
    "        b_input_ids = batch[0].to(device)\r\n",
    "        b_input_mask = batch[1].to(device)\r\n",
    "        b_labels = batch[2].to(device)\r\n",
    "        \r\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\r\n",
    "        # the forward pass, since this is only needed for backprop (training).\r\n",
    "        with torch.no_grad():        \r\n",
    "\r\n",
    "            # Forward pass, calculate logit predictions.\r\n",
    "            # token_type_ids is the same as the \"segment ids\", which \r\n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
    "            # The documentation for this `model` function is here: \r\n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
    "            # values prior to applying an activation function like the softmax.\r\n",
    "            (loss, logits) = model(b_input_ids, \r\n",
    "                                   token_type_ids=None, \r\n",
    "                                   attention_mask=b_input_mask,\r\n",
    "                                   labels=b_labels.long())\r\n",
    "            \r\n",
    "        # Accumulate the validation loss.\r\n",
    "        total_eval_loss += loss.item()\r\n",
    "\r\n",
    "        # Move logits and labels to CPU\r\n",
    "        logits = logits.detach().cpu().numpy()\r\n",
    "        label_ids = b_labels.to('cpu').numpy()\r\n",
    "\r\n",
    "        # Calculate the accuracy for this batch of test sentences, and\r\n",
    "        # accumulate it over all batches.\r\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
    "        \r\n",
    "\r\n",
    "    # Report the final accuracy for this validation run.\r\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
    "\r\n",
    "    # Calculate the average loss over all of the batches.\r\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
    "    \r\n",
    "    # Measure how long the validation run took.\r\n",
    "    validation_time = format_time(time.time() - t0)\r\n",
    "    \r\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
    "\r\n",
    "    # Record all statistics from this epoch.\r\n",
    "    training_stats.append(\r\n",
    "        {\r\n",
    "            'epoch': epoch_i + 1,\r\n",
    "            'Training Loss': avg_train_loss,\r\n",
    "            'Valid. Loss': avg_val_loss,\r\n",
    "            'Valid. Accur.': avg_val_accuracy,\r\n",
    "            'Training Time': training_time,\r\n",
    "            'Validation Time': validation_time\r\n",
    "        }\r\n",
    "    )\r\n",
    "\r\n",
    "print(\"\")\r\n",
    "print(\"Training complete!\")\r\n",
    "\r\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "kyRJP4p6IONm",
    "outputId": "98fa6131-444a-4880-bd43-b697e5aadb3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0:14:27</td>\n",
       "      <td>0:00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0:14:36</td>\n",
       "      <td>0:00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0:14:36</td>\n",
       "      <td>0:00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.70         0.69           0.50       0:14:27         0:00:35\n",
       "2               0.69         0.69           0.52       0:14:36         0:00:35\n",
       "3               0.67         0.72           0.52       0:14:36         0:00:35"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display floats with two decimal places.\r\n",
    "pd.set_option('precision', 2)\r\n",
    "\r\n",
    "# Create a DataFrame from our training statistics.\r\n",
    "df_stats = pd.DataFrame(data=training_stats)\r\n",
    "\r\n",
    "# Use the 'epoch' as the row index.\r\n",
    "df_stats = df_stats.set_index('epoch')\r\n",
    "\r\n",
    "# A hack to force the column headers to wrap.\r\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\r\n",
    "\r\n",
    "# Display the table.\r\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "PCXAxB6tIXY5",
    "outputId": "00c117f7-2558-482c-f736-c586b7488786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on female\n",
      "Number of test sentences: 2,683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 2,683 test sentences...\n",
      "    DONE.\n",
      "Total F1: 0.598\n",
      "Accuracy: 0.43\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on male\n",
      "Number of test sentences: 2,580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 2,580 test sentences...\n",
      "    DONE.\n",
      "Total F1: 0.000\n",
      "Accuracy: 0.59\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create test set\r\n",
    "\r\n",
    "correct_female_pred = []\r\n",
    "correct_male_pred = []\r\n",
    "accuracies = []\r\n",
    "for i in range(2):\r\n",
    "  if i == 0:\r\n",
    "    df = test_data[test_data.female_binary==1]\r\n",
    "    print('testing on female')\r\n",
    "  else:\r\n",
    "    df = test_data[test_data.female_binary==0]\r\n",
    "    print('testing on male')\r\n",
    "\r\n",
    "  df = df.sample(frac=1)\r\n",
    "  print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\r\n",
    "\r\n",
    "  comments = df.comment.values\r\n",
    "  labels = df.female_binary.values\r\n",
    "\r\n",
    "  # Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
    "  input_ids = []\r\n",
    "  attention_masks = []\r\n",
    "\r\n",
    "  # For every sentence...\r\n",
    "  for cmt in comments:\r\n",
    "      # `encode_plus` will:\r\n",
    "      #   (1) Tokenize the sentence.\r\n",
    "      #   (2) Prepend the `[CLS]` token to the start.\r\n",
    "      #   (3) Append the `[SEP]` token to the end.\r\n",
    "      #   (4) Map tokens to their IDs.\r\n",
    "      #   (5) Pad or truncate the sentence to `max_length`\r\n",
    "      #   (6) Create attention masks for [PAD] tokens.\r\n",
    "      encoded_dict = tokenizer.encode_plus(\r\n",
    "                          cmt,                      # Sentence to encode.\r\n",
    "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
    "                          max_length = 250,           # Pad & truncate all sentences.\r\n",
    "                          pad_to_max_length = True,\r\n",
    "                          return_attention_mask = True,   # Construct attn. masks.\r\n",
    "                          return_tensors = 'pt',     # Return pytorch tensors.\r\n",
    "                    )\r\n",
    "      \r\n",
    "      # Add the encoded sentence to the list.    \r\n",
    "      input_ids.append(encoded_dict['input_ids'])\r\n",
    "      \r\n",
    "      # And its attention mask (simply differentiates padding from non-padding).\r\n",
    "      attention_masks.append(encoded_dict['attention_mask'])\r\n",
    "\r\n",
    "  # Convert the lists into tensors.\r\n",
    "  input_ids = torch.cat(input_ids, dim=0)\r\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\r\n",
    "  labels = torch.tensor(labels)\r\n",
    "\r\n",
    "  # Set the batch size.  \r\n",
    "  batch_size = 16\r\n",
    "\r\n",
    "  # Create the DataLoader.\r\n",
    "  prediction_data = TensorDataset(input_ids, attention_masks, labels)\r\n",
    "  prediction_sampler = SequentialSampler(prediction_data)\r\n",
    "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\r\n",
    "\r\n",
    "  # Prediction on test set\r\n",
    "\r\n",
    "  print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\r\n",
    "\r\n",
    "  # Put model in evaluation mode\r\n",
    "  model.eval()\r\n",
    "\r\n",
    "  # Tracking variables \r\n",
    "  predictions , true_labels = [], []\r\n",
    "\r\n",
    "  # Predict \r\n",
    "  for batch in prediction_dataloader:\r\n",
    "    # Add batch to GPU\r\n",
    "    batch = tuple(t.to(device) for t in batch)\r\n",
    "    \r\n",
    "    # Unpack the inputs from our dataloader\r\n",
    "    b_input_ids, b_input_mask, b_labels = batch\r\n",
    "    \r\n",
    "    # Telling the model not to compute or store gradients, saving memory and \r\n",
    "    # speeding up prediction\r\n",
    "    with torch.no_grad():\r\n",
    "        # Forward pass, calculate logit predictions\r\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \r\n",
    "                        attention_mask=b_input_mask)\r\n",
    "\r\n",
    "    logits = outputs[0]\r\n",
    "\r\n",
    "    # Move logits and labels to CPU\r\n",
    "    logits = logits.detach().cpu().numpy()\r\n",
    "    label_ids = b_labels.to('cpu').numpy()\r\n",
    "    \r\n",
    "    # Store predictions and true labels\r\n",
    "    predictions.append(logits)\r\n",
    "    true_labels.append(label_ids)\r\n",
    "\r\n",
    "  print('    DONE.')\r\n",
    "\r\n",
    "  from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\r\n",
    "  import seaborn as sns\r\n",
    "  import matplotlib.pyplot as plt\r\n",
    "  \r\n",
    "  # Combine the results across all batches. \r\n",
    "  flat_pred = np.concatenate(predictions, axis=0)\r\n",
    "  # For each sample, pick the label (0 or 1) with the higher score.\r\n",
    "  flat_predictions = np.argmax(flat_pred, axis=1).flatten()\r\n",
    "\r\n",
    "  # Combine the correct labels for each batch into a single list.\r\n",
    "  flat_true_labels = np.concatenate(true_labels, axis=0)\r\n",
    "\r\n",
    "  for i in range(len(flat_true_labels)):\r\n",
    "    if flat_true_labels[i]==flat_predictions[i]:\r\n",
    "      if flat_true_labels[i] == 1:\r\n",
    "        correct_female_pred.append(tuple(flat_pred[i]))\r\n",
    "      else:\r\n",
    "        correct_male_pred.append(tuple(flat_pred[i]))\r\n",
    "\r\n",
    "  f1 = f1_score(flat_true_labels, flat_predictions)\r\n",
    "  print('Total F1: %.3f' % f1)\r\n",
    "  acc = accuracy_score(flat_true_labels,flat_predictions)\r\n",
    "  print('Accuracy: %.2f' % acc)\r\n",
    "  accuracies.append(acc)\r\n",
    "\r\n",
    "  cf_matrix = confusion_matrix(flat_true_labels, flat_predictions)\r\n",
    "  group_names = ['True Neg','False Pos','False Neg','True Pos']\r\n",
    "  group_counts = ['{0:0.0f}'.format(value) for value in\r\n",
    "                  cf_matrix.flatten()]\r\n",
    "  group_percentages = ['{0:.2%}'.format(value) for value in\r\n",
    "                      cf_matrix.flatten()/np.sum(cf_matrix)]\r\n",
    "  labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\r\n",
    "            zip(group_names,group_counts,group_percentages)]\r\n",
    "  try:\r\n",
    "    labels = np.asarray(labels).reshape(2,2)\r\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\r\n",
    "    plt.show()\r\n",
    "  except ValueError:\r\n",
    "    print(\"could not display\")\r\n",
    "\r\n",
    "  # f1_set = []\r\n",
    "\r\n",
    "  # # Evaluate each test batch\r\n",
    "  # print('Calculating and F1 score for each batch...')\r\n",
    "\r\n",
    "  # # For each input batch...\r\n",
    "  # for i in range(len(true_labels)):\r\n",
    "    \r\n",
    "  #   # The predictions for this batch are a 2-column ndarray (one column for \"0\" \r\n",
    "  #   # and one column for \"1\"). Pick the label with the highest value and turn this\r\n",
    "  #   # in to a list of 0s and 1s.\r\n",
    "  #   pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\r\n",
    "    \r\n",
    "  #   # Calculate and store the coef for this batch.  \r\n",
    "  #   score = f1_score(true_labels[i], pred_labels_i)\r\n",
    "  #   f1_set.append(score)\r\n",
    "\r\n",
    "  # # Create a barplot showing the f1 score for each batch of test samples.\r\n",
    "  # ax = sns.barplot(x=list(range(len(f1_set))), y=f1_set, ci=None)\r\n",
    "\r\n",
    "  # plt.title('f1 Score per Batch')\r\n",
    "  # plt.ylabel('f1 Score (0 to +1)')\r\n",
    "  # plt.xlabel('Batch #')\r\n",
    "\r\n",
    "  # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qlqP9YN4Jv6S",
    "outputId": "4f7eebd1-473f-48d5-9b9c-e54534bbb459"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
       "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
       "            <div id=\"6c264dc4-2fca-4751-a082-d37b207faff4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                \n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"6c264dc4-2fca-4751-a082-d37b207faff4\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '6c264dc4-2fca-4751-a082-d37b207faff4',\n",
       "                        [{\"mode\": \"markers\", \"type\": \"scatter\", \"x\": [0.023508546873927116, 0.7722474932670593, 0.3976196050643921, 0.7195950746536255, 0.7913652658462524, 0.341988205909729, 0.09720020741224289, 0.12844274938106537, 0.10152537375688553, 0.08793101459741592, 0.36924636363983154, 0.005529898218810558, 0.20356209576129913, -0.03348390758037567, 0.17121294140815735, 0.0004834448336623609, 0.8625543713569641, 0.2959938049316406, 0.3555479943752289, -0.013600670732557774, 0.4929032325744629, -0.021380135789513588, 0.3980136215686798, 0.2178458571434021, 0.051878560334444046, -0.06386887282133102, -0.048892851918935776, 0.14588211476802826, 0.23525775969028473, 0.05333956703543663, 0.17373380064964294, -0.09168942272663116, -0.010850210674107075, 0.02982342056930065, 0.15259286761283875, -0.11415800452232361, 0.02980675734579563, 0.6459687352180481, 0.24447952210903168, -0.05216361954808235, 0.34858933091163635, 0.13130751252174377, 0.03665905445814133, 0.7620833516120911, -0.11020133644342422, 0.3282191753387451, 0.3534933626651764, 0.13347643613815308, 0.1065555140376091, -0.040583327412605286, 0.07193320244550705, 0.4392584264278412, -0.031593333929777145, 0.27534592151641846, -0.04691163823008537, 0.038038041442632675, -0.012401928193867207, -0.015186485834419727, -0.1781502515077591, 0.3777411878108978, 0.009212777949869633, 0.0697159543633461, 0.11037895828485489, -0.10703691840171814, 0.22920525074005127, 0.4315633475780487, 0.16099746525287628, 0.43745243549346924, 0.2084043174982071, -0.042373113334178925, 0.05266072601079941, 0.21251530945301056, 0.44330325722694397, 0.23694927990436554, 0.34935253858566284, 0.2844073474407196, 0.4946880340576172, 0.42445269227027893, 0.1384194940328598, -0.0952225774526596, 0.09606295078992844, 0.27692851424217224, 0.09468462318181992, -0.016556506976485252, 0.2599489688873291, 0.32101157307624817, 0.06759094446897507, 0.10434691607952118, 0.024339450523257256, 0.14978648722171783, 0.7542734742164612, -0.06334768980741501, 0.06796170771121979, 0.5755046606063843, 0.23686166107654572, -0.06144586205482483, 0.5201457738876343, 0.07668633759021759, 0.04688822478055954, 0.44594043493270874, 0.09754472225904465, 0.1012781485915184, 0.4495584964752197, 0.5336269736289978, -0.0651778057217598, 0.07306379079818726, -0.0032935412600636482, 0.11191524565219879, 0.18325750529766083, 0.0317998081445694, 0.2776799499988556, 0.3223077058792114, -0.052178286015987396, -0.07510840892791748, 0.30286264419555664, 0.19619056582450867, 0.14758045971393585, 0.07680308073759079, 0.4260639250278473, 0.4528478682041168, 0.3932947516441345, 0.4658428728580475, 0.28551623225212097, 0.11448270827531815, 0.021054251119494438, 0.5480201244354248, 0.06271666288375854, 0.0729178860783577, 0.27000370621681213, 0.136159285902977, 0.2100740671157837, -0.10419661551713943, -0.06084545701742172, 0.3800848424434662, 0.09699460119009018, 0.009023466147482395, 0.22016063332557678, 0.7286552786827087, 0.10121878236532211, 0.09807834774255753, 0.512178897857666, -0.009075422771275043, -0.013564457185566425, -0.006522014737129211, 0.08407943695783615, 0.5538919568061829, 0.9542623162269592, 0.337301641702652, -0.03936561942100525, 0.351685106754303, 0.3079642951488495, 0.27646777033805847, 0.18616588413715363, 0.1398429423570633, 0.22968749701976776, 0.56924968957901, 0.04020281136035919, -0.04862869530916214, 0.02592810057103634, -0.09206826239824295, 0.04476111754775047, 0.3912964463233948, 0.449333518743515, 0.18017016351222992, 0.18286587297916412, 0.17053164541721344, 0.34147319197654724, 0.20658771693706512, 0.2504955530166626, 0.20128245651721954, 0.4829944372177124, 0.04606145620346069, 0.17146949470043182, -0.023900074884295464, 0.6204990744590759, 0.5212675333023071, -0.01154120359569788, 0.5865899920463562, -0.02596631832420826, 0.06341832131147385, 0.12169376760721207, 0.07274802774190903, 0.35872912406921387, 0.4013836979866028, 0.03073030523955822, 0.5626419186592102, 0.15037743747234344, 0.32479774951934814, 0.05056550353765488, -0.07405483722686768, 0.05365138500928879, 0.27097180485725403, 0.06498557329177856, 0.051889568567276, 0.15473856031894684, 0.5916486382484436, 0.014741296879947186, 0.12285292893648148, -0.03898917883634567, 0.13466092944145203, 0.275711327791214, -0.02892274223268032, 0.2344328612089157, 0.5427583456039429, 0.23878896236419678, 0.4970046877861023, -0.04752211272716522, -0.03811509162187576, 0.12547439336776733, 0.07186856120824814, 0.06662695854902267, 0.8526745438575745, 0.2862756550312042, 0.16656455397605896, 0.02311987429857254, 0.11964583396911621, 0.5998843312263489, 0.09664887189865112, -0.05158589407801628, 0.06297322362661362, 0.15826641023159027, 0.0989835187792778, 0.6463305354118347, 0.508703351020813, 0.1287926286458969, 0.39381644129753113, 0.4269983172416687, 0.4580828845500946, 0.34337812662124634, 0.21029344201087952, 0.5239658355712891, -0.030637623742222786, 0.21600908041000366, 0.471457839012146, -0.07483185082674026, 0.0860072523355484, -0.023497259244322777, 0.017809873446822166, -0.009742177091538906, 0.045466966927051544, 0.4522838592529297, 0.5676121711730957, 0.08216943591833115, 0.04061812534928322, 0.037266433238983154, 0.4028787612915039, 0.5546355843544006, 0.13731424510478973, 0.1548718363046646, -0.14751440286636353, 0.27611762285232544, 0.17478252947330475, 0.06761474162340164, -0.022232552990317345, 0.3548383414745331, 0.22352635860443115, -0.11716417968273163, -0.09399830549955368, 0.3085196614265442, 0.10970985889434814, 0.14236462116241455, 0.04009007662534714, 0.004662840627133846, 0.5222452282905579, -0.014717255719006062, 0.17524786293506622, -0.13214777410030365, 0.4946880340576172, -0.01680075190961361, -0.046757761389017105, -0.04501920938491821, -0.047666314989328384, 0.15158994495868683, 0.1707109659910202, -0.10203737020492554, 0.4566425383090973, 0.1274249255657196, 0.7464239001274109, 0.45697736740112305, 0.058697815984487534, -0.11353737860918045, -0.06923742592334747, 0.06397300213575363, 0.16184574365615845, 0.12459414452314377, 0.4433968663215637, -0.00418816227465868, 0.1400398463010788, 0.030732126906514168, 0.13279516994953156, 0.4339221715927124, -0.003010937012732029, -0.01756025291979313, 0.22492285072803497, 0.08875202387571335, 0.017190394923090935, 0.0841856300830841, 0.6778174638748169, 0.33670902252197266, -0.026201173663139343, 0.11549504101276398, 0.5479985475540161, 0.3863784968852997, 0.37647196650505066, 0.49502918124198914, 0.20836831629276276, 0.259182870388031, 0.4949036240577698, 0.36649027466773987, 0.0018725161207839847, 0.08395059406757355, 0.5120688676834106, 0.48653653264045715, 0.11125306040048599, 0.3629154562950134, 0.3891870081424713, 0.34354913234710693, 0.006573120132088661, 0.31438884139060974, 0.2941960394382477, 0.4062977731227875, -0.03915483504533768, 0.22597016394138336, 0.7600613832473755, 0.6930814385414124, 0.13782669603824615, 0.25412777066230774, 0.1721203774213791, -0.055307112634181976, 0.2828477919101715, 0.15759068727493286, 0.34359464049339294, -0.03960631042718887, 0.012297435663640499, 0.05251117795705795, 0.18524067103862762, 0.707619845867157, 0.018839135766029358, 0.18906573951244354, 0.01555060688406229, 0.13618965446949005, 0.045570675283670425, 0.2537149488925934, 0.13359878957271576, 0.5228707790374756, 0.2700998783111572, 0.300507515668869, 0.7793607115745544, 0.09290850907564163, 0.1202181801199913, 0.05266072601079941, 0.18866997957229614, 0.4121350049972534, 0.6765224933624268, 0.1418006420135498, 0.18490643799304962, 0.13526874780654907, 0.22508327662944794, 0.560869574546814, -0.003227955661714077, 0.1968436986207962, 0.4432806372642517, -0.05420297384262085, 0.5283455848693848, 0.06370791047811508, 0.13951630890369415, 0.3051809072494507, -0.01247908640652895, 0.6379520893096924, 0.474258154630661, 0.11767910420894623, 0.007059765048325062, 0.641914427280426, -0.00404712837189436, 0.02993261069059372, 0.23701004683971405, -0.021057041361927986, 0.19226203858852386, 0.23302707076072693, 0.4423576891422272, 0.0403362438082695, 0.6331885457038879, 0.24664688110351562, 0.5401522517204285, -0.022673994302749634, -0.12808047235012054, 0.3425867259502411, 0.04529586806893349, 0.22705881297588348, 0.3138556182384491, 0.4322320222854614, 0.4493521451950073, -0.037927623838186264, 0.3250489830970764, 0.2825295329093933, 0.22337746620178223, 0.1158042773604393, 0.05974368005990982, 0.21937358379364014, -0.022916125133633614, 0.13051654398441315, -0.055307112634181976, 0.7102667093276978, 0.05415405333042145, 0.059650249779224396, -0.043253134936094284, 0.061244405806064606, -0.011975076980888844, 0.026716109365224838, -0.055272337049245834, 0.0626869723200798, 0.16867101192474365, 0.7353392243385315, 0.1348848193883896, 0.12129620462656021, 0.5183163285255432, 0.07073302567005157, 0.09871149808168411, 0.7391878962516785, 0.7883473038673401, -0.015904581174254417, 0.17179791629314423, 0.07060067355632782, -0.01735145039856434, 0.008977244608104229, 0.05777081847190857, 0.2270660400390625, 0.42143934965133667, -0.04993373155593872, 0.0995296761393547, 0.07097674161195755, 0.2727425992488861, 0.10479038208723068, 0.3064064681529999, 0.055957015603780746, 0.21752256155014038, 0.12003057450056076, 0.12423735857009888, 0.8846079707145691, 0.26141539216041565, 0.253421813249588, 0.4828447103500366, 0.2921289801597595, 0.42352649569511414, 0.12971696257591248, 0.006940334104001522, 0.4799859821796417, 0.08465150743722916, -0.06439007073640823, 0.2679647207260132, -0.06817890703678131, 0.3112625479698181, 0.14668194949626923, 0.09693512320518494, 0.12653988599777222, 0.5887872576713562, 0.40353092551231384, 0.24322335422039032, 0.09820645302534103, 0.4928673207759857, 0.21379636228084564, 0.5921602845191956, 0.42623430490493774, 0.4762205183506012, 0.34809595346450806, 0.7571403384208679, 0.32806748151779175, -0.16646569967269897, 0.43987852334976196, 0.05526061728596687, 0.35645225644111633, 0.30193018913269043, 0.0737404152750969, 0.15334485471248627, 0.26577073335647583, -0.048909492790699005, 0.14203952252864838, -0.0001313696411671117, 0.1225489154458046, 0.250266969203949, 0.14452530443668365, 0.7518730163574219, 0.23303337395191193, -0.08733609318733215, 0.009193534962832928, 0.26295843720436096, 0.7938210964202881, -0.02769625000655651, -0.04748823493719101, 0.26434358954429626, 0.3593306541442871, 0.608575165271759, 0.23268406093120575, 0.08968550711870193, 0.2639087736606598, 0.031447649002075195, 0.145415261387825, 0.43359220027923584, -0.07107562571763992, 0.012480565346777439, 0.17420853674411774, 0.4266345202922821, 0.6660519242286682, -0.05875321477651596, -0.039519887417554855, 0.35718005895614624, 0.6196169853210449, 0.6679180860519409, 0.213678777217865, 0.013448798097670078, 0.35075438022613525, -0.0032243849709630013, 0.17724096775054932, 0.19585010409355164, -0.02785666473209858, -0.0032650278881192207, 0.22544032335281372, 0.198872908949852, 0.5124326348304749, 0.1113809198141098, 0.0008857016800902784, -0.029096484184265137, 0.00747245829552412, 0.3304143249988556, 0.07955529540777206, -0.020213263109326363, 0.11588351428508759, 0.336124062538147, 0.44093066453933716, 0.23408393561840057, -0.04384332150220871, 0.0207609124481678, 0.17799250781536102, -0.022173341363668442, 0.008234481327235699, 0.03283973038196564, 0.20828278362751007, 0.08441444486379623, 0.09657807648181915, 0.03317457437515259, 0.08834759891033173, 0.09428137540817261, -0.08290340006351471, 0.1776413470506668, 0.9230073094367981, -0.019043995067477226, 0.14433738589286804, -0.055001601576805115, 0.006121373735368252, 0.44453367590904236, -0.06707309186458588, 0.24418799579143524, 0.08147800713777542, -0.13717420399188995, 0.05466931313276291, 0.08466560393571854, 0.14919860661029816, -0.008663863874971867, 0.01056926604360342, 0.06882919371128082, 0.1580488234758377, 0.012374836020171642, 0.09967776387929916, 0.19349929690361023, -0.056665994226932526, 0.2876307964324951, 0.14884153008460999, 0.0012499828590080142, 0.5474955439567566, 0.8280209898948669, 0.21188300848007202, 0.8080343008041382, 0.3988551199436188, -0.022926559671759605, 0.3635491132736206, 0.0215115025639534, 0.09095630794763565, 0.42006000876426697, 0.35159897804260254, -0.013152490369975567, 0.34992101788520813, -0.13903991878032684, 0.4992101788520813, 0.2102815955877304, 0.11404035985469818, 0.09865646064281464, -0.12025529891252518, 0.07923208922147751, 0.3863912522792816, 0.34436798095703125, 0.06131473556160927, 0.19838625192642212, -0.028639910742640495, -0.08094104379415512, 0.6106760501861572, 0.657127320766449, 0.10652761161327362, 0.36252284049987793, 0.04587002098560333, 0.010127323679625988, 0.07361181825399399, 0.07561872154474258, 0.0918649286031723, 0.04973584786057472, 0.1405729353427887, 0.4954049587249756, -0.0832502618432045, 0.29601237177848816, -0.0036173509433865547, -0.04784759134054184, 0.3104817271232605, 0.12906941771507263, 0.037590377032756805, 0.18315882980823517, -0.06338915973901749, 0.5378773808479309, -0.07649163901805878, 0.18258531391620636, 0.22909626364707947, 0.2975732982158661, -0.028608843684196472, 0.3467874526977539, 0.024342266842722893, 0.1462813913822174, 0.022136924788355827, 0.06412593275308609, 0.24355114996433258, 0.09462118148803711, -0.051749225705862045, 0.5891139507293701, 0.01563219539821148, 0.16539336740970612, 0.022151587530970573, 0.0459677092730999, 0.6909344792366028, 0.7369078993797302, 0.0785377025604248, 0.2910936772823334, 0.10077910870313644, -0.08744534105062485, 0.49499252438545227, 0.09395139664411545, 0.6765086650848389, 0.2886016070842743, 0.13381534814834595, 0.05883331969380379, 0.012601627968251705, 0.6074530482292175, 0.03200553357601166, -0.0439605787396431, 0.1473878175020218, -0.047905467450618744, 0.3284063935279846, 0.19857953488826752, 0.007787465117871761, -0.0018937151180580258, 0.17586521804332733, 0.0032449206337332726, 0.5123686790466309, 0.05379653722047806, 0.8949599862098694, -0.02591443620622158, 0.15240176022052765, 0.04972133785486221, 0.7576050758361816, 0.5937965512275696, -0.08667496591806412, 0.10855323821306229, 0.7461420297622681, 0.8046787977218628, 0.2838903069496155, 0.5137324333190918, 0.1942003220319748, 0.04438971355557442, 0.4734826982021332, -0.049782250076532364, 0.5033605098724365, 0.3007648289203644, -0.03270784765481949, 0.04529586806893349, 0.12456358969211578, 0.17204737663269043, 0.13742110133171082, 0.10015489906072617, 0.35074254870414734, 0.3004142642021179, 0.182541623711586, 0.10192728787660599, 0.30748608708381653, -0.02137703262269497, -0.029385581612586975, 0.18155479431152344, 0.25617462396621704, 0.07215876877307892, 0.9542623162269592, 0.12171021848917007, 0.006334408186376095, 0.6478210091590881, -0.012481833808124065, 0.13513918220996857, 0.0543069988489151, 0.3062398135662079, 0.07632879912853241, 0.05768696963787079, -0.08078770339488983, 0.12461323291063309, -0.010230989195406437, 0.05295850336551666, 0.11305613815784454, 0.5968365669250488, -0.03143322095274925, -0.04583897814154625, -0.03169265389442444, 0.5027667880058289, 0.41916269063949585, 0.24460797011852264, 0.14149163663387299, -0.051117416471242905, 0.025974998250603676, -0.06054304540157318, 0.3556658923625946, 0.4480574131011963, 0.05853071063756943, 0.4184448719024658, 0.7245754599571228, 0.15792681276798248, 0.3154487609863281, 0.17430441081523895, 0.22296784818172455, 0.010834462009370327, -0.06407472491264343, 0.4744713008403778, -0.029491666704416275, -0.1872393935918808, 0.16490082442760468, 0.312674343585968, 0.08494346588850021, 0.3434194326400757, 0.2300368994474411, 0.5616969466209412, 0.41748490929603577, -0.0507347397506237, 0.2731388509273529, -0.05237709730863571, 0.16586703062057495, 0.2699950337409973, 0.10912340879440308, 0.1548263430595398, 0.2620770037174225, 0.22583678364753723, 0.2520694136619568, 0.4000648856163025, 0.04936878755688667, 0.20624257624149323, 0.14887522161006927, 0.15150617063045502, 0.3334513008594513, 0.042395904660224915, 0.5016264915466309, -0.03166104480624199, 0.0412982702255249, 0.17650888860225677, 0.12882769107818604, 0.17842234671115875, 0.24574516713619232, 0.16498519480228424, 0.08230633288621902, 0.22521644830703735, 0.16003082692623138, -0.03028537705540657, 0.01098762359470129, 0.2523219883441925, 0.16535347700119019, 0.16484351456165314, 0.1429373025894165, -0.06852667778730392, -0.02893531136214733, 0.035635676234960556, 0.161272794008255, 0.034010425209999084, 0.05575248599052429, -0.007158656604588032, 0.2715957462787628, 0.17007897794246674, 0.15306971967220306, -0.03313517943024635, 0.35708051919937134, 0.15364478528499603, 0.5598464608192444, 0.007440577261149883, 0.5248444080352783, 0.15156276524066925, 0.06590303033590317, 0.04921433702111244, 0.32565656304359436, 0.17987962067127228, -0.001934484695084393, 0.3730669915676117, 0.35161924362182617, 0.5825338959693909, 0.13401810824871063, 0.1308903843164444, -0.04225049167871475, 0.23153506219387054, 0.43624147772789, -0.06234551966190338, 0.2784311771392822, 0.19552549719810486, 0.02549964003264904, 0.4350444972515106, -0.07172610610723495, 0.292699933052063, 0.2649235129356384, 0.24122245609760284, 0.33958300948143005, 0.1304047554731369, -0.029564831405878067, 0.26332125067710876, -0.05079741030931473, 0.1366869956254959, 0.28317123651504517, -0.03307163715362549, 0.1282949298620224, 0.3154487609863281, 0.019196247681975365, 0.18862205743789673, 0.451013445854187, 0.4985072910785675, 0.954025149345398, 0.03522423654794693, 0.4041161835193634, 0.4686208665370941, 0.675354540348053, 0.15971516072750092, 0.2202855944633484, 0.4418719410896301, 0.29513582587242126, 0.37700408697128296, 0.03073030523955822, 0.4060653746128082, 0.10194739699363708, 0.3615713119506836, 0.32609134912490845, 0.05325525254011154, 0.12360284477472305, 0.037758439779281616, 0.3676290214061737, 0.10487719625234604, 0.03574579954147339, 0.13352520763874054, 0.38781917095184326, 0.039858393371105194, -0.0320095419883728, 0.0067872097715735435, -0.009353888221085072, 0.16695290803909302, -0.008493992500007153, -0.0739058181643486, -0.00913260504603386, 0.3035878837108612, 0.481566458940506, 0.06419052928686142, -0.12081243097782135, 0.3288957476615906, 0.7684729099273682, 0.15215405821800232, 0.3313523530960083, 0.3856467604637146, 0.037044376134872437, 0.19061072170734406, 0.26438260078430176, 0.04201781377196312, 0.20608286559581757, 0.016589948907494545, 0.3812439739704132, -0.07047516852617264, 0.2860371768474579, 0.03447885811328888, 0.2112266570329666, -0.05231417715549469, 0.11944421380758286, 0.3249320685863495, -0.009892829693853855, -0.033180076628923416, 0.07803656905889511, 0.0657980665564537, 0.2616761326789856, 0.4311008155345917, 0.38678470253944397, 0.17476516962051392, 0.12110678106546402, 0.15571744740009308, 0.5918534994125366, 0.2171160727739334, 0.19054080545902252, 0.025687098503112793, -0.045378219336271286, -0.017354635521769524, 0.16661719977855682, 0.34717047214508057, -0.07334475964307785, 0.4757547378540039, -0.06468361616134644, -0.1085851863026619, 0.24324670433998108, 0.50855952501297, 0.36396491527557373, 0.5015668869018555, 0.6132017970085144, 0.4441727101802826, 0.48484525084495544, -0.010140937753021717, 0.09083478897809982, 0.19714273512363434, 0.16429734230041504, 0.687889814376831, 0.6626387238502502, 0.15924999117851257, 0.005827025510370731, 0.5588315725326538, 0.016240065917372704, 0.10489684343338013, 0.4632212519645691, 0.43868201971054077, 0.37869763374328613, 0.04588709771633148, -0.05869298055768013, -0.025482075288891792, 0.008785146288573742, -0.016611330211162567, 0.42203083634376526, 0.1331416219472885, 0.16990433633327484, 0.09876715391874313, -0.1782243400812149, 0.053801268339157104, 0.41142842173576355, -0.07334475964307785, 0.010077199898660183, 0.3501584827899933, 0.055057648569345474, 0.42029479146003723, -0.11948408931493759, 0.32737594842910767, 0.06478888541460037, 0.3124845325946808, 0.1951010674238205, 0.6001472473144531, 0.41376861929893494, 0.049417391419410706, -0.0011087424354627728, 0.08085530251264572, 0.014572403393685818, 0.38442927598953247, 0.1659357100725174, -0.04261079803109169, 0.0774306133389473, 0.15456052124500275, 0.1087065115571022, 0.06439278274774551, 0.686592698097229, 0.03983641415834427, 0.0032881442457437515, 0.3024340569972992, 0.43012794852256775, 0.32165372371673584, 0.2714340388774872, -0.11801896244287491, 0.2546182870864868, 0.17462663352489471, 0.7365505695343018, 0.017144395038485527, 0.13443300127983093, -0.06902468949556351, 0.013010329566895962, -0.011739627458155155, 0.23011180758476257, -0.013047670014202595, 0.09892532229423523, 0.2879689633846283, 0.3470911979675293, -0.05978618562221527, 0.0847499668598175, 0.6821467876434326, -0.03728003054857254, 0.0699521154165268, 0.46032533049583435, 0.27571985125541687, 0.23399339616298676, 0.28278595209121704, 0.592379093170166, -0.007366827689111233, 0.08853325992822647, 0.6121417880058289, 0.08692578971385956, 0.05304102227091789, 0.2886295020580292, 0.09346234053373337, 0.2533206045627594, 0.14298518002033234, 0.36909911036491394, 0.19710318744182587, 0.07278602570295334, -0.09918700903654099, 0.39856550097465515, 0.0938761904835701, 0.07965037226676941, 0.3981386721134186, -0.09599149972200394, 0.7830889821052551, 0.09483211487531662, 0.2635582387447357, 0.313428670167923, 0.09654553979635239, -0.03131606802344322, 0.09004927426576614, -0.08512183278799057, 0.4389054775238037, -0.030674481764435768, 0.18824875354766846, 0.09137984365224838, 0.37666618824005127, 0.046496544033288956, 0.041058000177145004, 0.35734930634498596, 0.47833073139190674, 0.09899622201919556, 0.10162679105997086, 0.330718994140625, 0.1739465445280075, 0.41328760981559753, -0.09246402233839035, 0.10910256952047348, -0.0505487285554409, 0.39895525574684143, 0.3082619905471802, 0.026236850768327713, 0.10980977863073349, 0.025219779461622238, 0.035521991550922394, 0.6196169853210449, 0.1325613409280777, 0.13660840690135956, 0.6053411364555359, 0.35794469714164734, -0.1157127097249031, -0.03116445057094097, 0.5538919568061829, 0.06460583955049515, 0.28414833545684814, -0.035332150757312775, 0.3306458294391632, 0.3983806073665619, -0.015747519209980965, 0.20295050740242004, 0.3227064907550812, 0.10258613526821136, 0.30576279759407043, -0.014840043149888515, 0.32930952310562134, 0.016523117199540138, 0.2015310376882553, 0.016797589138150215, -0.0930560976266861, 0.0025242334231734276, 0.07182765752077103, 0.34188777208328247, -0.03213706612586975, 0.04381226748228073, -0.05934152752161026, -0.004420108161866665, -0.08421806246042252, 0.2436942309141159, 0.37550994753837585, 0.22067633271217346, 0.05365138500928879, 0.2771858870983124, 0.5174953937530518, 0.49333685636520386, 0.15697644650936127, 0.34900811314582825, 0.5184233784675598, 0.09047070145606995, -0.025436747819185257, 0.11456497758626938, 0.06036527827382088, 0.15316788852214813, 0.7802850604057312, 0.12901465594768524, -0.011220998130738735, 0.10515180230140686, 0.09530111402273178, -0.011590634472668171, 0.20035822689533234, -0.050512928515672684, -0.036854665726423264, 0.313428670167923, 0.22097359597682953, 0.025676628574728966, -0.1941041797399521, 0.1996668130159378, 0.12107010930776596, 0.12129528075456619, 0.10536108165979385, 0.23717744648456573, 0.10997685045003891, -0.13196241855621338, 0.052963197231292725, 0.0034827841445803642, 0.42110931873321533, 0.15180516242980957, 0.6593383550643921, 0.3821108937263489, -0.027495834976434708, 0.6532375812530518, 0.08299247920513153, 0.3993229269981384, 0.28372374176979065, -0.10082300752401352, 0.36961400508880615, -0.060013748705387115, 0.06017626076936722, 0.027742978185415268, 0.06555371731519699, -0.009207413531839848, 0.0413876436650753, -0.07281406223773956, 0.1270480901002884, 0.3306458294391632, 0.23823143541812897, 0.17775064706802368, 0.20793893933296204, 0.012480565346777439, 0.3113887906074524, 0.38948822021484375, -0.08154229074716568, 0.17831775546073914, -0.00871062371879816, 0.25354209542274475, 0.4385344982147217, 0.28805258870124817, -0.041757021099328995, 0.11716359108686447, 0.4136601388454437], \"y\": [-0.02700064331293106, -0.6196759343147278, -0.3481173813343048, -0.7936646342277527, -0.6484288573265076, -0.45413702726364136, -0.2930898070335388, -0.2587529420852661, -0.19105885922908783, -0.25853386521339417, -0.34310397505760193, -0.1978774219751358, -0.07302222400903702, -0.12543532252311707, -0.1272876113653183, -0.05900152027606964, -0.644065260887146, -0.2974991500377655, -0.42261332273483276, -0.21520845592021942, -0.5047379732131958, -0.023676661774516106, -0.46006157994270325, -0.3335177004337311, -0.09766021370887756, -0.08385464549064636, -0.07851530611515045, -0.19901449978351593, -0.33824893832206726, -0.15467679500579834, -0.08550611138343811, -0.11972987651824951, -0.1345331370830536, -0.14451506733894348, -0.19404925405979156, -0.1262412816286087, -0.16360782086849213, -0.5535538196563721, -0.378537654876709, -0.1602688878774643, -0.3402892053127289, -0.15915422141551971, -0.13572277128696442, -0.6808426976203918, -0.11766223609447479, -0.23600001633167267, -0.4788627624511719, -0.12788601219654083, -0.10952334105968475, -0.1600027233362198, -0.15811972320079803, -0.3701021373271942, -0.08858472853899002, -0.3711985647678375, -0.24775607883930206, -0.009259778074920177, -0.20108367502689362, -0.13611076772212982, -0.26906052231788635, -0.4206124246120453, -0.08113620430231094, -0.2168027013540268, -0.19457995891571045, -0.13801057636737823, -0.22812330722808838, -0.44703221321105957, -0.21118231117725372, -0.40557169914245605, -0.23323357105255127, -0.04901454970240593, -0.15160658955574036, -0.44420185685157776, -0.4861569106578827, -0.32829657196998596, -0.340427428483963, -0.24226751923561096, -0.3738349378108978, -0.32832762598991394, -0.2753654718399048, -0.12031476944684982, -0.18308915197849274, -0.3111964762210846, -0.2806524932384491, -0.19480004906654358, -0.4168706238269806, -0.3266639709472656, -0.12324855476617813, -0.22859430313110352, -0.2833716571331024, -0.22773413360118866, -0.6868085861206055, -0.07030527293682098, -0.24963845312595367, -0.5056639313697815, -0.2281077355146408, -0.15691140294075012, -0.4164541959762573, -0.24801628291606903, -0.21426688134670258, -0.3215658366680145, -0.2156146615743637, -0.2962622046470642, -0.3839213252067566, -0.4696100056171417, -0.12575119733810425, -0.03248438984155655, -0.22693577408790588, -0.16749919950962067, -0.14740215241909027, -0.16857962310314178, -0.3347158133983612, -0.2460513859987259, -0.09798929840326309, -0.1449638456106186, -0.2700393497943878, -0.01021574903279543, -0.265570729970932, -0.18438883125782013, -0.33881863951683044, -0.34341561794281006, -0.3294958174228668, -0.41443589329719543, -0.25723281502723694, -0.06811773031949997, -0.16104163229465485, -0.47971734404563904, -0.13119570910930634, -0.11771664023399353, -0.3979211747646332, -0.2069438248872757, -0.23217299580574036, -0.15260078012943268, -0.10791380703449249, -0.17919056117534637, -0.17945413291454315, -0.09223449230194092, -0.3855283558368683, -0.6438139081001282, -0.14944620430469513, -0.14424026012420654, -0.46978995203971863, -0.17039117217063904, -0.15129336714744568, -0.14303620159626007, -0.06152044236660004, -0.38511767983436584, -0.7081124186515808, -0.2432543784379959, -0.09791690111160278, -0.27183207869529724, -0.26948949694633484, -0.25438693165779114, -0.23473812639713287, -0.223099485039711, -0.23512403666973114, -0.5187934637069702, -0.010890605859458447, -0.0939486175775528, -0.027957791462540627, -0.09345540404319763, -0.13982243835926056, -0.4423058331012726, -0.5694137811660767, -0.332768052816391, -0.181073859333992, -0.2914336323738098, -0.3906714618206024, -0.2419460117816925, -0.18970675766468048, -0.22460061311721802, -0.568743109703064, -0.23167718946933746, -0.24546244740486145, -0.16128714382648468, -0.4581727385520935, -0.3684328496456146, -0.04016309604048729, -0.5312768816947937, -0.20918695628643036, -0.03434595465660095, -0.22346223890781403, -0.25256165862083435, -0.21035641431808472, -0.373138427734375, -0.2373015582561493, -0.5537393689155579, -0.17285247147083282, -0.3799948990345001, -0.18824847042560577, -0.2432584911584854, -0.20709653198719025, -0.1599351316690445, -0.07322791963815689, -0.18124251067638397, -0.30625930428504944, -0.3786987364292145, -0.1571962833404541, -0.2495868057012558, -0.1375352293252945, -0.18517853319644928, -0.3060256838798523, -0.044532157480716705, -0.26465338468551636, -0.424922376871109, -0.29317814111709595, -0.3942115902900696, -0.07007956504821777, -0.19242414832115173, -0.33147767186164856, -0.33917978405952454, -0.2775938808917999, -0.5474551320075989, -0.339107483625412, -0.26545488834381104, -0.11718931794166565, -0.18831969797611237, -0.6329382658004761, -0.3012712001800537, -0.12624727189540863, -0.20633544027805328, -0.11965519189834595, -0.20596201717853546, -0.5521211624145508, -0.48595723509788513, -0.3077768087387085, -0.45710694789886475, -0.38242629170417786, -0.4002611041069031, -0.3070080578327179, -0.22317533195018768, -0.34559664130210876, -0.10540290176868439, -0.2599017024040222, -0.3736240863800049, -0.3110087811946869, -0.21966078877449036, -0.13015706837177277, -0.10835319757461548, -0.017901236191391945, -0.034915439784526825, -0.3863421380519867, -0.43097952008247375, -0.20628805458545685, -0.0966845452785492, -0.03635770455002785, -0.41596364974975586, -0.42047396302223206, -0.33623450994491577, -0.3059346079826355, -0.1527853012084961, -0.3256354033946991, -0.31588804721832275, -0.08182420581579208, -0.1155993789434433, -0.35447537899017334, -0.2408837378025055, -0.19112303853034973, -0.10507983714342117, -0.2844354212284088, -0.11397026479244232, -0.18064413964748383, -0.16477034986019135, -0.17430898547172546, -0.4723525941371918, -0.11467362940311432, -0.20750883221626282, -0.16883522272109985, -0.3738349378108978, -0.07508014887571335, -0.07846954464912415, -0.10125109553337097, -0.12166504561901093, -0.18127866089344025, -0.15388353168964386, -0.15357501804828644, -0.42642083764076233, -0.17563138902187347, -0.635734498500824, -0.30461928248405457, -0.2652956545352936, -0.11579598486423492, -0.09362567961215973, -0.1523648053407669, -0.30443134903907776, -0.32793328166007996, -0.32178565859794617, -0.0782322958111763, -0.25059446692466736, -0.22795416414737701, -0.37120017409324646, -0.4626118242740631, -0.15335236489772797, -0.13077425956726074, -0.15634603798389435, -0.13654178380966187, -0.11347679793834686, -0.2517971694469452, -0.6424023509025574, -0.5064197182655334, -0.1683121770620346, -0.2570306062698364, -0.5499948859214783, -0.4842652678489685, -0.3843996822834015, -0.4286099374294281, -0.29460248351097107, -0.19375652074813843, -0.5378926396369934, -0.2972196638584137, -0.11334250867366791, -0.16461949050426483, -0.39314010739326477, -0.5277068614959717, -0.25366854667663574, -0.2734437882900238, -0.38216447830200195, -0.3699023723602295, -0.15373440086841583, -0.3440099358558655, -0.4969976246356964, -0.3420599400997162, -0.1226356253027916, -0.3255995213985443, -0.6585713624954224, -0.36996278166770935, -0.28201916813850403, -0.36270320415496826, -0.15406504273414612, -0.13894112408161163, -0.37405169010162354, -0.22149302065372467, -0.5483847856521606, -0.17236901819705963, -0.05977862328290939, -0.13682721555233002, -0.19254443049430847, -0.670123815536499, -0.0813744068145752, -0.07496901601552963, -0.2618858218193054, -0.24275760352611542, -0.2780705988407135, -0.4149629473686218, -0.2290736734867096, -0.5627288222312927, -0.14451538026332855, -0.3901965022087097, -0.572344183921814, -0.20159077644348145, -0.16825540363788605, -0.15160658955574036, -0.17946532368659973, -0.5576698184013367, -0.5330397486686707, -0.24784258008003235, -0.30584996938705444, -0.2451159507036209, -0.2928626835346222, -0.4890196621417999, -0.21576422452926636, -0.28545305132865906, -0.29929661750793457, -0.1602431982755661, -0.4572608768939972, -0.20333227515220642, -0.2831513285636902, -0.259389728307724, -0.1858193427324295, -0.5955886244773865, -0.42135390639305115, -0.18295156955718994, -0.10267451405525208, -0.4658699333667755, -0.17746560275554657, -0.22142186760902405, -0.466222882270813, -0.04384028539061546, -0.2554793655872345, -0.3227331340312958, -0.3323589563369751, -0.15072910487651825, -0.7023665904998779, -0.1348782628774643, -0.3640306293964386, -0.20959751307964325, -0.14642061293125153, -0.36362460255622864, -0.14758837223052979, -0.1964920312166214, -0.2744775414466858, -0.3912917375564575, -0.3339715600013733, -0.21964113414287567, -0.3432345688343048, -0.17184346914291382, -0.2239852249622345, -0.1956327259540558, -0.3057270646095276, -0.2878284752368927, -0.19695809483528137, -0.2649323344230652, -0.13894112408161163, -0.5729600191116333, -0.19252143800258636, -0.08272621780633926, -0.0837165117263794, -0.12276894599199295, -0.14847634732723236, -0.07719600945711136, -0.13415315747261047, -0.18462221324443817, -0.2433866262435913, -0.5447538495063782, -0.17050249874591827, -0.1389445811510086, -0.41818463802337646, -0.26191362738609314, -0.2739628255367279, -0.6183884739875793, -0.6410815119743347, -0.1258740872144699, -0.21870094537734985, -0.12936918437480927, -0.042078156024217606, -0.17140762507915497, -0.2653329372406006, -0.3321044445037842, -0.40145769715309143, -0.05499567463994026, -0.27216973900794983, -0.2743278741836548, -0.3993014991283417, -0.2612569034099579, -0.4619252383708954, -0.2379084974527359, -0.29972949624061584, -0.1394677311182022, -0.16049626469612122, -0.6588734984397888, -0.27694275975227356, -0.18339605629444122, -0.5730051398277283, -0.23754307627677917, -0.20009712874889374, -0.22539691627025604, -0.06341195106506348, -0.4608941078186035, -0.16357548534870148, -0.13547135889530182, -0.22926165163516998, -0.22295889258384705, -0.3079632818698883, -0.1643359214067459, -0.1492951363325119, -0.07227686047554016, -0.5388534069061279, -0.38626471161842346, -0.23030006885528564, -0.15657930076122284, -0.4062485694885254, -0.23044559359550476, -0.582220196723938, -0.19561907649040222, -0.34019917249679565, -0.3923228681087494, -0.5905721783638, -0.41919535398483276, -0.206995889544487, -0.23439598083496094, -0.07962451875209808, -0.29724249243736267, -0.22132937610149384, -0.07092957198619843, -0.20592497289180756, -0.37567242980003357, -0.1235436424612999, -0.2069016993045807, -0.2690075933933258, -0.21053706109523773, -0.2234748750925064, -0.30768367648124695, -0.5545192360877991, -0.35985660552978516, -0.09171174466609955, -0.1853170543909073, -0.4712565541267395, -0.5674465298652649, -0.044087786227464676, -0.13210342824459076, -0.2887888252735138, -0.405169278383255, -0.623503565788269, -0.25750523805618286, -0.25357669591903687, -0.13459564745426178, -0.22759711742401123, -0.19122397899627686, -0.45860281586647034, -0.1412186622619629, -0.07855883240699768, -0.2332104593515396, -0.4276075065135956, -0.6162263751029968, -0.12703904509544373, -0.1085532158613205, -0.41272038221359253, -0.3943157494068146, -0.6283082365989685, -0.2448713183403015, -0.0390298031270504, -0.3657097816467285, -0.18195340037345886, -0.2236824333667755, -0.1570437252521515, -0.12102712690830231, -0.11419249325990677, -0.30735769867897034, -0.24715808033943176, -0.44731736183166504, -0.21662141382694244, -0.2510698139667511, -0.11495308578014374, -0.2110002487897873, -0.3547278046607971, -0.19769999384880066, -0.04941588267683983, -0.23168569803237915, -0.3076396882534027, -0.3794163465499878, -0.3089551329612732, -0.1016971543431282, -0.13583123683929443, -0.3157890737056732, -0.11163612455129623, -0.12067045271396637, -0.2552682161331177, -0.3190954029560089, -0.12818121910095215, -0.023849496617913246, -0.06803234666585922, -0.18470287322998047, -0.13580818474292755, -0.10256783664226532, -0.2616640627384186, -0.6598567366600037, -0.18875478208065033, -0.14480553567409515, -0.11440618336200714, -0.18563799560070038, -0.4543085992336273, -0.09812303632497787, -0.21764357388019562, -0.22965556383132935, -0.17043644189834595, -0.16540800034999847, -0.2598203718662262, -0.28328514099121094, -0.06972791999578476, -0.13720589876174927, -0.19432224333286285, -0.23304079473018646, -0.20419569313526154, -0.08987961709499359, -0.11482928693294525, -0.18477065861225128, -0.37902384996414185, -0.30833712220191956, -0.22676774859428406, -0.4709797203540802, -0.6551458239555359, -0.31484517455101013, -0.5690723657608032, -0.24847447872161865, -0.04920283332467079, -0.360729843378067, -0.0905585065484047, -0.2913968563079834, -0.31213822960853577, -0.44127511978149414, -0.10583905875682831, -0.44296640157699585, -0.14847679436206818, -0.4133983552455902, -0.18643885850906372, -0.14759688079357147, -0.18057963252067566, -0.15530328452587128, -0.09018924832344055, -0.3413871228694916, -0.29251396656036377, -0.06091766431927681, -0.2293645590543747, -0.03704870119690895, -0.1079707071185112, -0.4807163178920746, -0.5143583416938782, -0.2188473641872406, -0.42666295170783997, -0.007751966826617718, -0.4594947099685669, -0.08973075449466705, -0.1688612699508667, -0.19522108137607574, -0.1545092910528183, -0.30616116523742676, -0.6365212798118591, -0.1539432257413864, -0.2033243328332901, -0.15469788014888763, -0.10693496465682983, -0.4384017288684845, -0.294213205575943, -0.25006526708602905, -0.19401565194129944, -0.07413026690483093, -0.3757675588130951, -0.11969628185033798, -0.20653152465820312, -0.2859095335006714, -0.17617903649806976, -0.07551023364067078, -0.3174794018268585, -0.11757390946149826, -0.423074334859848, -0.15988434851169586, -0.21599572896957397, -0.4751187562942505, -0.2796113193035126, -0.1764286309480667, -0.4954054653644562, -0.12484605610370636, -0.2241445928812027, -0.16724206507205963, -0.10831250250339508, -0.6050277352333069, -0.6071339249610901, -0.1559269279241562, -0.3612847626209259, -0.2022468000650406, -0.11537343263626099, -0.3819877505302429, -0.1866949051618576, -0.6249679923057556, -0.43460583686828613, -0.2903796136379242, -0.350629985332489, -0.07286259531974792, -0.45424559712409973, -0.15199699997901917, -0.10589228570461273, -0.18662111461162567, -0.14826378226280212, -0.43607190251350403, -0.28949257731437683, -0.09201836585998535, -0.009268135763704777, -0.14712834358215332, -0.1370406299829483, -0.41797956824302673, -0.04218481853604317, -0.6499945521354675, -0.07260783016681671, -0.14740179479122162, 0.012713250704109669, -0.5186694860458374, -0.5384652614593506, -0.12715913355350494, -0.18884195387363434, -0.5694701075553894, -0.6744025945663452, -0.3446100950241089, -0.3532339632511139, -0.329127699136734, -0.18933553993701935, -0.40974608063697815, -0.05827749893069267, -0.5067521333694458, -0.2963196635246277, -0.0943882167339325, -0.14758837223052979, -0.2896959185600281, -0.34148702025413513, 0.012416505254805088, -0.10085386037826538, -0.34035739302635193, -0.3761948049068451, -0.20123933255672455, -0.21258118748664856, -0.42361000180244446, -0.16449318826198578, -0.19205936789512634, -0.20100568234920502, -0.33525633811950684, -0.317791223526001, -0.7081124186515808, -0.09050597250461578, -0.1834450364112854, -0.5530396103858948, -0.22345222532749176, -0.15453976392745972, -0.05917365103960037, -0.25142645835876465, -0.11378492414951324, -0.08553101122379303, -0.09606564044952393, -0.22502706944942474, -0.11863475292921066, -0.2841566503047943, -0.22741711139678955, -0.44496098160743713, -0.09629243612289429, -0.10907168686389923, -0.17525500059127808, -0.3313099443912506, -0.4607343375682831, -0.34384778141975403, -0.27123716473579407, -0.08113376796245575, -0.12054219841957092, -0.07526493817567825, -0.3651691973209381, -0.41826561093330383, -0.30592280626296997, -0.5702107548713684, -0.6038835048675537, -0.2528870403766632, -0.51047283411026, -0.3377021849155426, -0.16315585374832153, -0.13458697497844696, -0.10711100697517395, -0.4915252923965454, -0.11719077825546265, -0.23110507428646088, -0.06040608137845993, -0.2524808943271637, -0.14847110211849213, -0.34266397356987, -0.2842444181442261, -0.5580504536628723, -0.3226604759693146, -0.11352955549955368, -0.32343700528144836, -0.1686094105243683, -0.3549228310585022, -0.32342079281806946, -0.09084541350603104, -0.1969718188047409, -0.3733339309692383, -0.3508765399456024, -0.2911997139453888, -0.511667013168335, -0.13642582297325134, -0.265175998210907, -0.2760724425315857, -0.10208494961261749, -0.316800981760025, -0.1613779217004776, -0.5226268768310547, -0.03218724578619003, -0.27723315358161926, -0.2573939263820648, -0.16128554940223694, -0.24964915215969086, -0.35737431049346924, -0.19404295086860657, -0.1986309140920639, -0.14366388320922852, -0.1588125079870224, -0.1307811588048935, -0.19942857325077057, -0.32850366830825806, -0.3292713463306427, -0.24277465045452118, -0.10574670881032944, -0.11350631713867188, -0.11640286445617676, -0.10278981924057007, -0.2783868610858917, -0.16217617690563202, -0.16711096465587616, -0.03964788094162941, -0.111520916223526, -0.21251697838306427, -0.18093764781951904, -0.11977774649858475, -0.3096904754638672, -0.305033802986145, -0.407459557056427, -0.18165497481822968, -0.5631856322288513, -0.24816876649856567, -0.16937915980815887, -0.08725816011428833, -0.2378799319267273, -0.06532180309295654, -0.15408861637115479, -0.4117138385772705, -0.4138522148132324, -0.4372541010379791, -0.31355342268943787, -0.09885353595018387, -0.09298060834407806, -0.2651592791080475, -0.3852129280567169, -0.12256450951099396, -0.3541271388530731, -0.3409036695957184, -0.22687391936779022, -0.3473498523235321, -0.0934353917837143, -0.32919254899024963, -0.26690873503685, -0.3871217370033264, -0.3377009928226471, -0.16978386044502258, -0.10113615542650223, -0.3346649408340454, -0.051546111702919006, -0.3166100084781647, -0.2759934663772583, -0.11450929939746857, -0.13503485918045044, -0.51047283411026, -0.3422355055809021, -0.2421107292175293, -0.36478671431541443, -0.34176525473594666, -0.7579554915428162, -0.14966434240341187, -0.4547359049320221, -0.4456135928630829, -0.5689330697059631, -0.3136548101902008, -0.1229507252573967, -0.4884152412414551, -0.0897521898150444, -0.3286602795124054, -0.2373015582561493, -0.3521994352340698, -0.18927814066410065, -0.3423437774181366, -0.2870635390281677, -0.05249728262424469, -0.22827614843845367, -0.07991290092468262, -0.33514395356178284, -0.19233612716197968, -0.16504967212677002, -0.2349270135164261, -0.4684087336063385, -0.31188344955444336, -0.04753217473626137, -0.024661721661686897, -0.11172471940517426, -0.34851402044296265, -0.04999315366148949, -0.11850837618112564, -0.1566668301820755, -0.22656503319740295, -0.5804191827774048, -0.055429793894290924, -0.1566518098115921, -0.20282424986362457, -0.6324298977851868, -0.1901456117630005, -0.28872236609458923, -0.36472436785697937, -0.13759173452854156, -0.29344695806503296, -0.13864409923553467, -0.1640745997428894, -0.248085618019104, -0.08224916458129883, -0.3018045723438263, -0.07752218097448349, -0.2631789743900299, -0.0847901925444603, -0.18738213181495667, -0.32517534494400024, -0.15143026411533356, -0.434699684381485, -0.15638144314289093, -0.1105065867304802, -0.2143368124961853, -0.08083643764257431, -0.39195844531059265, -0.4212096333503723, -0.3114902377128601, -0.1240936890244484, -0.2085104137659073, -0.13386617600917816, -0.4253625273704529, -0.18594259023666382, -0.2715986967086792, -0.02619926817715168, -0.27627623081207275, -0.16642405092716217, -0.21392790973186493, -0.33154183626174927, -0.15735943615436554, -0.4883330464363098, -0.14801208674907684, -0.14159859716892242, -0.24923093616962433, -0.42836707830429077, -0.42735353112220764, -0.34329932928085327, -0.5582705140113831, -0.5775955319404602, -0.41194969415664673, -0.17720122635364532, -0.2705155611038208, -0.25500354170799255, -0.3496515154838562, -0.5056143403053284, -0.539306104183197, -0.1174006462097168, -0.06967289000749588, -0.4554135501384735, -0.18654192984104156, -0.25277817249298096, -0.4551056921482086, -0.4538368880748749, -0.1621158868074417, -0.14785492420196533, -0.08685730397701263, -0.24725155532360077, -0.1570185273885727, -0.20713694393634796, -0.4526037275791168, -0.34233346581459045, -0.17831985652446747, -0.24119822680950165, -0.21896205842494965, -0.2521760165691376, -0.5027145147323608, -0.15735943615436554, -0.1805848628282547, -0.2718808650970459, -0.09349730610847473, -0.44806626439094543, -0.14798060059547424, -0.4190857708454132, -0.26806506514549255, -0.3419122099876404, -0.189911887049675, -0.3189530074596405, -0.41037997603416443, -0.013379170559346676, -0.07746047526597977, -0.026938648894429207, -0.29167357087135315, -0.44502386450767517, -0.16793276369571686, -0.2317812144756317, 0.006176994182169437, -0.24501438438892365, -0.2079477459192276, -0.18368153274059296, -0.5145997405052185, -0.12233466655015945, -0.14571066200733185, -0.35662660002708435, -0.3511510491371155, -0.2970788776874542, -0.07727780938148499, -0.14515605568885803, -0.38716474175453186, -0.2078840285539627, -0.6869364380836487, -0.19855621457099915, -0.26655033230781555, -0.12870052456855774, -0.1317237764596939, -0.039067357778549194, -0.3124177157878876, -0.05855344980955124, -0.1577245593070984, -0.3060303032398224, -0.4103986322879791, -0.102379709482193, -0.22659353911876678, -0.642450213432312, -0.14043501019477844, -0.10884766280651093, -0.4421413838863373, -0.3281081020832062, -0.2541154623031616, -0.43732455372810364, -0.6122077703475952, -0.03592917323112488, -0.3778083324432373, -0.5638817548751831, -0.21336306631565094, -0.2936844229698181, -0.2996146082878113, -0.24332402646541595, -0.3511143624782562, -0.256693571805954, -0.35399922728538513, -0.20433197915554047, -0.13236673176288605, -0.12876151502132416, -0.4179670214653015, -0.20257973670959473, -0.19018438458442688, -0.24509216845035553, -0.1928316205739975, -0.5342888832092285, -0.30329927802085876, -0.33030927181243896, -0.2155843824148178, -0.17044566571712494, -0.1348860263824463, -0.30241653323173523, -0.10528383404016495, -0.36162030696868896, -0.1104823499917984, -0.3572671711444855, -0.31202906370162964, -0.2486838549375534, -0.17463108897209167, -0.07528875768184662, -0.28251558542251587, -0.4060783386230469, -0.22762484848499298, -0.24672560393810272, -0.2755344808101654, -0.3377494513988495, -0.4593103229999542, -0.19139112532138824, -0.3063010573387146, -0.11677109450101852, -0.48504677414894104, -0.37834489345550537, -0.21810393035411835, -0.1777815967798233, -0.21323885023593903, -0.11389653384685516, -0.3943157494068146, -0.06404521316289902, -0.300271213054657, -0.5150879621505737, -0.32551831007003784, -0.1551012098789215, -0.1698334813117981, -0.38511767983436584, -0.20227059721946716, -0.3854062855243683, -0.14384175837039948, -0.3395683467388153, -0.31486037373542786, -0.13849644362926483, -0.3448159396648407, -0.2770020067691803, -0.22900310158729553, -0.23452578485012054, -0.1598087102174759, -0.43726977705955505, -0.18964983522891998, -0.05212213844060898, -0.21646930277347565, -0.16028976440429688, -0.20136521756649017, -0.14747275412082672, -0.32246342301368713, -0.1202261745929718, -0.18303371965885162, -0.07408560812473297, -0.05418302118778229, -0.10047025233507156, -0.1606759876012802, -0.30934038758277893, -0.270913690328598, -0.20709653198719025, -0.4143354892730713, -0.46669119596481323, -0.5714792013168335, -0.14164955914020538, -0.47628623247146606, -0.4666638970375061, -0.25996530055999756, -0.08226175606250763, -0.1487877517938614, -0.186693012714386, -0.29416340589523315, -0.4289712905883789, -0.10406363010406494, -0.1073828786611557, -0.20291602611541748, -0.22482961416244507, -0.1676001250743866, -0.1003054529428482, -0.11232582479715347, -0.2652847468852997, -0.2155843824148178, -0.4363369643688202, -0.18967097997665405, -0.20783166587352753, -0.43027225136756897, -0.18873506784439087, -0.14618033170700073, -0.19392096996307373, -0.28765708208084106, -0.06130164489150047, -0.13305287063121796, -0.13094519078731537, -0.21132934093475342, -0.3451606333255768, -0.12640373408794403, -0.46565523743629456, -0.36952388286590576, -0.19603075087070465, -0.5842118263244629, -0.20714494585990906, -0.36777982115745544, -0.2766771614551544, -0.1110040694475174, -0.23545412719249725, -0.12757939100265503, -0.09471850097179413, -0.22825659811496735, -0.25161969661712646, -0.11696767807006836, -0.0849551111459732, -0.1161285936832428, -0.25957995653152466, -0.3395683467388153, -0.4025728702545166, -0.3166099190711975, -0.08826474845409393, -0.07855883240699768, -0.37575075030326843, -0.34232237935066223, -0.21619364619255066, -0.24210906028747559, -0.014312556944787502, -0.2262171506881714, -0.3115370273590088, -0.39024093747138977, -0.2060384452342987, -0.23759406805038452, -0.4157792329788208]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Likelihood of genders for true positive (female)\"}, \"xaxis\": {\"title\": {\"text\": \"Likelihood female\"}}, \"yaxis\": {\"title\": {\"text\": \"Likelihood male\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6c264dc4-2fca-4751-a082-d37b207faff4');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                \n",
       "            </script>\n",
       "        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
       "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
       "            <div id=\"899c1d4a-c4ab-4161-8b62-47e4f4ebf087\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                \n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"899c1d4a-c4ab-4161-8b62-47e4f4ebf087\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '899c1d4a-c4ab-4161-8b62-47e4f4ebf087',\n",
       "                        [{\"mode\": \"markers\", \"type\": \"scatter\", \"x\": [0.0676889568567276, -0.02201196737587452, 0.30775368213653564, 0.03748738393187523, 0.11817105114459991, 0.17130015790462494, 0.27258217334747314, 0.213948056101799, 0.15337976813316345, 0.20312519371509552, 0.3597312867641449, 0.12089637666940689, 0.453473836183548, 0.2477848380804062, 0.05778522416949272, 0.07062394917011261, 0.4033244550228119, 0.2818656861782074, 0.3458389341831207, 0.020683685317635536, 0.09848258644342422, 0.4232846796512604, -0.03678393363952637, 0.30761995911598206, -0.06383630633354187, 0.0596468560397625, -0.005398440174758434, 0.24770379066467285, -0.04069840535521507, 0.36968424916267395, -0.1425143927335739, 0.20675836503505707, 0.024969855323433876, 0.2797645032405853, -0.004783633165061474, 0.14569254219532013, 0.13081544637680054, -0.012014838866889477, 0.25991329550743103, 0.08238206058740616, 0.07650213688611984, 0.3580159842967987, 0.47962361574172974, 0.11990219354629517, 0.1467026323080063, 0.2530049681663513, 0.1893913894891739, 0.38641178607940674, 0.23731525242328644, 0.2583751082420349, 0.09068247675895691, 0.051395103335380554, 0.049045052379369736, 0.3433208465576172, 0.2419607937335968, 0.1833229511976242, -0.0170938391238451, -0.13267797231674194, 0.1511683464050293, 0.1154901459813118, -0.05473792180418968, 0.2623080909252167, 0.3936019539833069, -0.029345953837037086, 0.017410797998309135, 0.4338238835334778, -0.01946689747273922, 0.028691750019788742, 0.005768583156168461, 0.1348322331905365, -0.022570952773094177, -0.01280736830085516, 0.40988823771476746, 0.19656112790107727, 0.1104683205485344, 0.216495543718338, 0.324150413274765, 0.3657561242580414, -0.08819036185741425, 0.3545840084552765, 0.09920572489500046, -0.024995027109980583, 0.24482949078083038, 0.2511739432811737, 0.10363461822271347, 0.0042513282969594, 0.4389057755470276, 0.10322828590869904, 0.2012060135602951, -0.10279536247253418, 0.10111650079488754, 0.4889748692512512, 0.08411676436662674, 0.21302813291549683, 0.14123010635375977, -0.08730766922235489, -0.020552687346935272, 0.38303932547569275, 0.20318622887134552, 0.18596895039081573, 0.4340645968914032, -0.05264993757009506, 0.2716419994831085, 0.3529614508152008, -0.026270875707268715, 0.22942498326301575, 0.11742129176855087, -0.05889374390244484, 0.03566581755876541, 0.15339221060276031, 0.03165654093027115, 0.2947678864002228, 0.27483198046684265, 0.14232531189918518, 0.18519000709056854, -0.1766958385705948, 0.2471838742494583, 0.3293423652648926, 0.05434444174170494, 0.05298371613025665, 0.0367492251098156, -0.06917224824428558, 0.16353057324886322, 0.18202616274356842, 0.06789910793304443, 0.07151047885417938, -0.05057550594210625, 0.38559165596961975, -0.025600263848900795, 0.3931709825992584, 0.0913219153881073, 0.45868977904319763, 0.14462070167064667, -0.11312833428382874, 0.15178434550762177, 0.10029776394367218, 0.267728716135025, 0.26117607951164246, -0.07991360872983932, 0.11675573885440826, 0.33991825580596924, 0.34281083941459656, -0.05706612393260002, 0.07953432202339172, 0.3040715456008911, 0.3288501799106598, 0.02069440297782421, 0.08722566813230515, 0.16776669025421143, 0.042692866176366806, 0.05903537943959236, -0.14052771031856537, 0.1981026977300644, 0.004201649688184261, 0.11741317808628082, 0.05578189343214035, 0.2563150227069855, -0.044217173010110855, -0.023657992482185364, 0.17784087359905243, 0.4123229384422302, 0.08320725709199905, 0.014718656428158283, 0.5310402512550354, 0.1394720822572708, 0.4237467348575592, 0.2655557096004486, 0.3986721336841583, 0.07277429103851318, 0.05705771967768669, -0.0005960080889053643, 0.02388804964721203, 0.13359420001506805, 0.01834421418607235, -0.02927415259182453, 0.3738895356655121, 0.13998983800411224, 0.2329878956079483, 0.19177353382110596, 0.3351716995239258, 0.3856532573699951, 0.22244825959205627, 0.21983757615089417, 0.2248174399137497, 0.014166665263473988, 0.3529983162879944, 0.20317070186138153, 0.05991347134113312, 0.06044178083539009, 0.5628249049186707, -0.029715243726968765, 0.1045185774564743, 0.017589213326573372, 0.030419891700148582, 0.16719534993171692, 0.12133698910474777, 0.09679590165615082, 0.1004885882139206, -0.03825748711824417, 0.25297537446022034, 0.2909303605556488, -0.011458124034106731, -0.059691160917282104, 0.15561756491661072, 0.203672394156456, 0.047957394272089005, 0.08326919376850128, -0.016987713053822517, 0.15140630304813385, 0.0846879780292511, 0.030701836571097374, 0.09626723825931549, 0.2804514169692993, -0.08376175165176392, 0.2975459396839142, 0.11360792070627213, 0.054127808660268784, 0.4164954423904419, -0.0006301112589426339, -0.0972452312707901, 0.03354630991816521, 0.14277301728725433, 0.006283287890255451, 0.1277204006910324, 0.21222424507141113, 0.0024697270710021257, 0.12479174137115479, 0.254983127117157, 0.07480715960264206, 0.2232259064912796, -0.03842499479651451, 0.2793283462524414, 0.08765298128128052, -0.028993766754865646, 0.10367663204669952, -0.0355616919696331, 0.24803273379802704, 0.40572768449783325, 0.6406671404838562, 0.30660632252693176, -0.00847217720001936, 0.17099429666996002, 0.14325708150863647, 0.16315485537052155, -0.017770742997527122, -0.04748476296663284, 0.17411869764328003, 0.06377996504306793, 0.16147460043430328, 0.34285762906074524, 0.20005463063716888, 0.4462713301181793, 0.14801016449928284, 0.1178436130285263, 0.1009809821844101, 0.16540096700191498, 0.32635679841041565, 0.1946810632944107, 0.2693204879760742, 0.23954929411411285, 0.20704634487628937, 0.2903308570384979, 0.3741348683834076, 0.07308979332447052, 0.29785314202308655, 0.06773659586906433, 0.27063360810279846, -0.05077439174056053, 0.3186870515346527, 0.23271960020065308, 0.24867676198482513, 0.4485463798046112, 0.3500209152698517, -0.06458410620689392, 0.041095707565546036, 0.34011921286582947, 0.12696699798107147, 0.03288869187235832, 0.15040984749794006, 0.13600687682628632, -0.020135516300797462, 0.10179629921913147, 0.5634042024612427, 0.13861183822155, 0.05084559693932533, -0.016048302873969078, 0.011628410778939724, 0.38428282737731934, 0.2803841531276703, 0.14303460717201233, -0.022922081872820854, 0.01213847566395998, 0.07717253267765045, 0.06987936049699783, 0.13445250689983368, 0.03288780897855759, -0.02309284545481205, -0.013699362985789776, 0.13695760071277618, 0.3312298655509949, -0.18408681452274323, 0.13016444444656372, 0.4366580545902252, 0.21497516334056854, -0.013701668940484524, -0.057162050157785416, -0.04870179668068886, 0.5203342437744141, 0.05032103508710861, -0.04283589869737625, 0.27732884883880615, 0.023866133764386177, 0.18539033830165863, 0.0707760602235794, 0.06596828252077103, 0.03333006799221039, 0.3406442403793335, 0.11159902065992355, 0.365592360496521, 0.20891064405441284, 0.22137780487537384, 0.08765298128128052, 0.32198566198349, 0.28388869762420654, -0.05218926817178726, 0.07022079825401306, 0.44454464316368103, 0.21927042305469513, 0.40187424421310425, 0.3820999562740326, -0.10663758218288422, 0.14348812401294708, 0.2300884872674942, 0.3844829797744751, 0.08642873913049698, 0.18825982511043549, 0.14980003237724304, 0.5460476279258728, 0.22364191710948944, 0.034219324588775635, 0.1205885261297226, 0.19566859304904938, -0.009556648321449757, -0.01431939098984003, 0.2059842348098755, 0.18447014689445496, 0.47536295652389526, 0.5061800479888916, 0.14637191593647003, -0.10372196137905121, -0.0312472153455019, -0.08245999366044998, 0.0027326710987836123, -0.031829364597797394, 0.05094290152192116, 0.23385168612003326, 0.3407709300518036, 0.06034334376454353, 0.02107730321586132, 0.07952473312616348, 0.24393230676651, 0.5188155770301819, -0.026350075379014015, 0.03683732450008392, 0.020314259454607964, 0.1254136711359024, -0.02011825703084469, 0.121522456407547, -0.11661376059055328, 0.30618762969970703, 0.3234400749206543, 0.05016631260514259, 0.25585731863975525, 0.07905282825231552, 0.05620984360575676, 0.23293039202690125, 0.33742982149124146, 0.07301051914691925, 0.19684068858623505, 0.42984744906425476, 0.1273452192544937, 0.020862452685832977, 0.09848258644342422, 0.38078901171684265, 0.21004818379878998, -0.07515792548656464, 0.020273787900805473, 0.2973288893699646, 0.07903266698122025, -0.0677662342786789, 0.24603073298931122, 0.20848987996578217, 0.3469584584236145, 0.1830213963985443, -0.06958147883415222, -0.01122331339865923, 0.20026172697544098, 0.09224305301904678, 0.4563019573688507, -0.020515387877821922, 0.15346452593803406, 0.13593046367168427, 0.19782426953315735, 0.1895892322063446, 0.34807589650154114, 0.1030159741640091, 0.05687596648931503, 0.2513703405857086, -0.008467872627079487, 0.233219712972641, 0.5013059377670288, -0.03595815971493721, 0.23312343657016754, 0.3641764223575592, -0.061938636004924774, 0.1904073804616928, 0.18406939506530762, 0.17115072906017303, 0.39837703108787537, 0.04298241436481476, 0.18083778023719788, 0.030352214351296425, -0.017335398122668266, 0.2514199912548065, 0.5628249049186707, 0.177140474319458, 0.10493971407413483, 0.1482967883348465, -0.10520224273204803, 0.19806741178035736, -0.0056039514020085335, 0.07329007238149643, 0.041588179767131805, -0.026246028020977974, 0.11901784688234329, -0.028103986755013466, 0.20386521518230438, -0.0300655085593462, -0.03624064475297928, 0.23642219603061676, 0.581486701965332, 0.1772715151309967, 0.2299378663301468, 0.06676444411277771, 0.4692060053348541, 0.2060769647359848, 0.29668277502059937, 0.3709854185581207, 0.107603058218956, -0.018750866875052452, 0.3335702419281006, 0.16351312398910522, 0.23775090277194977, -0.08544722199440002, 0.11735373735427856, 0.220753014087677, 0.31379953026771545, -0.14937971532344818, 0.02917318232357502, -0.015944158658385277, 0.296234130859375, 0.012854569591581821, 0.07154173403978348, 0.09552250802516937, 0.21096722781658173, 0.10816469043493271, 0.15041591227054596, 0.15203212201595306, -0.10744138062000275, 0.07735685259103775, 0.2637300193309784, 0.5232351422309875, 0.14238876104354858, 0.29622435569763184, 0.12351834774017334, 0.27541011571884155, -0.12608902156352997, 0.19743219017982483, 0.1692555546760559, 0.444112092256546, 0.04733522981405258, -0.06477665156126022, 0.32548052072525024, -0.07151417434215546, 0.1259673684835434, -0.06097709760069847, 0.2040543407201767, 0.44577834010124207, 0.1827024519443512, 0.04072358086705208, 0.37657463550567627, -0.04126441851258278, 0.1891321986913681, 0.08720420300960541, 0.014950898475944996, 0.24034647643566132, 0.4201149642467499, 0.39379188418388367, 0.16680249571800232, 0.4298225939273834, 0.10243072360754013, -0.09133817255496979, 0.22375179827213287, -0.026917636394500732, 0.022825738415122032, 0.025524931028485298, 0.2354278415441513, 0.095463827252388, -0.030110156163573265, 0.2546754777431488, 0.1207808181643486, 0.21382105350494385, 0.47382164001464844, -0.09614020586013794, 0.01752905361354351, 0.15932254493236542, 0.13032780587673187, -0.032522667199373245, 0.08172684907913208, 0.2681054174900055, 0.2413058876991272, -0.037864480167627335, 0.574213445186615, 0.20022112131118774, 0.07495873421430588, 0.08508563041687012, 0.35003164410591125, 0.2575550675392151, 0.285628080368042, 0.22838903963565826, 0.07473006099462509, 0.04168478772044182, 0.28227469325065613, -0.01301585789769888, 0.4225846827030182, -0.03409061208367348, 0.1554102599620819, -0.0015532958786934614, 0.06723212450742722, 0.21554015576839447, 0.08692191541194916, 0.18099914491176605, -0.018492819741368294, 0.08600426465272903, -0.07565690577030182, 0.011074417270720005, 0.1502155065536499, 0.14979298412799835, -0.008036128245294094, 0.2851395905017853, 0.16970784962177277, 0.3503473103046417, 0.09436124563217163, 0.10025457292795181, 0.28802549839019775, 0.11154608428478241, 0.387300044298172, 0.27761009335517883, 0.006924022920429707, 0.06673678010702133, 0.16112001240253448, -0.011049431748688221, 0.30089664459228516, -0.09913063049316406, 0.003553328337147832, 0.00771715585142374, -0.016604119911789894, 0.13943175971508026, 0.09013491868972778, 0.13058684766292572, 0.3025272786617279, 0.08810397982597351, -0.00694809015840292, 0.1827024519443512, -0.10632587969303131, 0.40794283151626587, 0.10429427772760391, -0.09720553457736969, 0.14142455160617828, 0.4767979085445404, 0.13806745409965515, 0.16101212799549103, 0.11526321619749069, -0.07923075556755066, -0.00680993078276515, 0.19754482805728912, 0.0005176972481422126, -0.021775711327791214, 0.1559717208147049, 0.2498113065958023, 0.0704234391450882, 0.2015860229730606, 0.09403932094573975, 0.4650798439979553, -0.028488466516137123, 0.21869361400604248, 0.22041894495487213, 0.115027517080307, 0.1745835244655609, 0.0027037833351641893, 0.16265474259853363, 0.021348964422941208, 0.0017921358812600374, 0.21608899533748627, 0.022369571030139923, 0.61063152551651, 0.3969133794307709, 0.22428178787231445, 0.1414330154657364, 0.015273195691406727, -0.01768488436937332, 0.30153316259384155, -0.02238490618765354, 0.06608669459819794, 0.21290825307369232, 0.2214014083147049, 0.31682804226875305, 0.16134454309940338, 0.014064670540392399, 0.2248547226190567, 0.135385200381279, -0.09337867796421051, -0.04437171295285225, -0.0028112127911299467, -0.020515387877821922, 0.004022304899990559, -0.030877022072672844, 0.006800890900194645, 0.12968093156814575, 0.021725153550505638, 0.22354526817798615, 0.236580953001976, 0.29914629459381104, 0.259126752614975, 0.32056713104248047, 0.11389531195163727, 0.0735929012298584, 0.1656120866537094, 0.024575388059020042, 0.39298015832901, 0.30315524339675903, -0.12597410380840302, -0.09268257021903992, 0.23185382783412933, -0.09197080135345459, 0.03397781401872635, -0.054439809173345566, 0.21906085312366486, 0.204303577542305, 0.3838203251361847, -0.0019695672672241926, 0.02623605541884899, 0.33826297521591187, 0.06318970769643784, 0.11789172887802124, 0.21071740984916687, 0.19484341144561768, 0.22537820041179657, -0.054053857922554016, 0.02875295840203762, 0.20529381930828094, 0.17525067925453186, 0.21660378575325012, -0.03692144155502319, -0.08300640434026718, 0.4188413918018341, 0.30737635493278503, 0.39244890213012695, 0.12331598252058029, 0.48505961894989014, 0.3387768268585205, -0.06616078317165375, 0.10106649249792099, 0.09465868771076202, 0.014370587654411793, 0.22286109626293182, -0.08267860859632492, -0.12808459997177124, 0.3836374878883362, 0.039538487792015076, 0.20350344479084015, 0.19330953061580658, 0.5147979259490967, 0.6345842480659485, 0.058405552059412, 0.05295591428875923, -0.032114218920469284, 0.15252308547496796, 0.08510614931583405, 0.09775681793689728, 0.6362336277961731, 0.06732361018657684, 0.26119425892829895, 0.05846286192536354, 0.3042106628417969, 0.02192673273384571, 0.030330615118145943, 0.30492061376571655, 0.12837685644626617, -0.04172055795788765, 0.1271268129348755, -0.05259965732693672, -0.06180896237492561, 0.2954576015472412, 0.19638139009475708, -0.052178047597408295, -0.12070602923631668, 0.03222868964076042, 0.16677427291870117, 0.0862240195274353, 0.280680775642395, 0.21126864850521088, -0.1824232041835785, -0.029911769554018974, -0.019861780107021332, 0.37701672315597534, 0.3725293278694153, 0.17656393349170685, 0.02925419993698597, 0.06258790194988251, 0.3944186866283417, 0.40752631425857544, 0.14974656701087952, -0.015413238666951656, 0.18770717084407806, -0.11589979380369186, 0.26427626609802246, 0.3271896541118622, 0.231223925948143, 0.157400444149971, -0.05196965113282204, 0.06597181409597397, -0.06891278177499771, -0.06106449291110039, 0.04332059994339943, 0.20546074211597443, 0.21189351379871368, 0.1025710478425026, 0.03863859549164772, 0.17038001120090485, 0.06166410446166992, -0.08742817491292953, 0.07823412120342255, 0.06106557324528694, -0.0006301112589426339, 0.1581684798002243, 0.23853544890880585, -0.03600718080997467, 0.3261963129043579, 0.06758905202150345, 0.44117170572280884, 0.258600652217865, 0.03556181862950325, 0.1866195797920227, 0.29039087891578674, -0.07927915453910828, 0.5653183460235596, 0.28189602494239807, 0.10152936726808548, 0.06166410446166992, 0.008928180672228336, 0.05697769299149513, -0.004650524817407131, 0.12832115590572357, 0.18009160459041595, 0.3015975058078766, 0.14670276641845703, 0.2585863769054413, -0.040344543755054474, 0.1495862603187561, 0.05846191197633743, 0.1267191618680954, 0.41003748774528503, 0.10861390829086304, -0.00639969389885664, 0.02734237350523472, 0.06953184306621552, 0.5329237580299377, 0.09475956112146378, 0.03067735768854618, 0.14074201881885529, -0.0649932473897934, 0.2110229730606079, 0.24795567989349365, -0.01504244189709425, 0.12420015037059784, 0.03496434912085533, 0.3571665585041046, 0.27126431465148926, 0.4548647105693817, 0.1683756709098816, 0.0626613050699234, 0.05777020379900932, 0.20847375690937042, 0.18582649528980255, 0.3186584413051605, 0.3205534517765045, 0.16353236138820648, 0.15747489035129547, -0.06710419803857803, 0.1669686883687973, 0.30748048424720764, -0.12564988434314728, 0.10818026959896088, -0.004114422015845776, 0.1420617252588272, 0.41002726554870605, -0.03208625689148903, 0.044050704687833786, 0.0662558525800705, 0.13715507090091705, 0.3520241975784302, -0.07166939228773117, 0.22428178787231445, 0.3184248208999634, 0.17943081259727478, 0.059217777103185654, 0.39357128739356995, -0.05677734315395355, 0.45178472995758057, -0.028139978647232056, -0.013800560496747494, 0.1389351785182953, 0.0058800457045435905, 0.45416921377182007, 0.19784867763519287, 0.05635717138648033, 0.183385968208313, 0.05815276503562927, -0.06685328483581543, 0.0029656675178557634, -0.008886526338756084, 0.13889525830745697, 0.08176443725824356, 0.05342075973749161, 0.3386988341808319, 0.5955538153648376, 0.45919835567474365, -0.019574014469981194, -0.012323238886892796, 0.47173234820365906, 0.19743438065052032, 0.1547224074602127, 0.16414295136928558, 0.11102089285850525, 0.05763508379459381, 0.11627253890037537, -0.04811182990670204, 0.2471606731414795, 0.15920759737491608, 0.029998069629073143, 0.1291159689426422, 0.19146855175495148, -0.05179827660322189, 0.3159136474132538, -0.008016551844775677, 0.18188272416591644, 0.6160924434661865, 0.008106389082968235, 0.10521364212036133, 0.27452734112739563, 0.04072358086705208, 0.11526321619749069, -0.030061082914471626, 0.16577066481113434, -0.027270160615444183, -0.05971668288111687, 0.243735671043396, 0.12671582400798798, 0.026042984798550606, 0.14203110337257385, 0.25951075553894043, 0.3766646981239319, 0.15219561755657196, 0.03996235877275467, -0.10099140554666519, 0.011074134148657322, 0.08576680719852448, 0.07947645336389542, 0.06154516339302063, 0.12415660917758942, 0.20601461827754974, 0.1946782022714615, -0.04094509407877922, 0.4928368926048279, 0.11414410173892975, 0.1808909922838211, 0.3049491047859192, 0.12863023579120636, 0.21071740984916687, -0.08484230190515518, 0.09387186169624329, 0.1127857193350792, 0.10375352203845978, 0.013610296882689, 0.387300044298172, 0.2580057382583618, 0.23646584153175354, 0.1413397639989853, 0.22299301624298096, 0.6992068886756897, 0.01193970162421465, 0.31706205010414124, 0.2798777222633362, -0.012403829954564571, 0.2045973688364029, 0.08950118720531464, 0.011709398590028286, 0.08118683099746704, 0.027194837108254433, 0.05835403874516487, -0.055914126336574554, 0.12398197501897812, 0.07657278329133987, 0.055172860622406006, 0.15234439074993134, 0.1751452386379242, 0.4140048623085022, 0.10684601217508316, 0.17357592284679413, -0.003920611925423145, 0.41434523463249207, 0.15900884568691254, 0.20924699306488037, 0.3195231258869171, -0.044114455580711365, 0.23886951804161072, 0.1626330018043518, 0.12163472920656204, 0.4357706606388092, -0.045911576598882675, 0.24245376884937286, 0.22332489490509033, 0.1534826010465622, 0.31465256214141846, 0.14315958321094513, -0.029831456020474434, 0.17099429666996002, 0.14528779685497284, 0.19016657769680023, 0.18486464023590088, 0.1642523556947708, -0.07191142439842224, -0.040095310658216476, 0.02140877954661846, 0.2472609132528305, 0.04513898119330406, 0.1540970802307129, 0.39019474387168884, 0.4138626158237457, 0.13083776831626892, 0.13031060993671417, 0.4366580545902252, -0.18408681452274323, 0.08813556283712387, 0.2626465857028961, -0.0043606264516711235, -0.06769132614135742, 0.26895272731781006, 0.26692819595336914, 0.27069777250289917, 0.1529545783996582, 0.14685699343681335, 0.014021077193319798, -0.07282339036464691, 0.3457774519920349, 0.11171730607748032, 0.3666398227214813, 0.06728163361549377, 0.15837320685386658, 0.2777876555919647, 0.012018308974802494, 0.06493612378835678, -0.07876620441675186, -0.032504383474588394, 0.041789185255765915, 0.3861073851585388, 0.46061041951179504, 0.2909303605556488, 0.4423171579837799, 0.040662746876478195, 0.23031587898731232, 0.08844368159770966, -0.0671578049659729, 0.06953222304582596, 0.3619442582130432, -0.08521074056625366, 0.24544991552829742, 0.24183553457260132, -0.09375177323818207, 0.04694681242108345, 0.13396920263767242, 0.02264503575861454, 0.18807627260684967, 0.18522866070270538, 0.011381412856280804, 0.147050678730011, 0.2579617202281952, 0.09173761308193207, 0.30466485023498535, 0.40455833077430725, 0.25029581785202026, 0.2803179621696472, 0.06410810351371765, 0.22475992143154144, -0.04621541127562523, -0.013369432650506496, 0.3538384735584259, -0.06406562775373459, 0.09944090247154236, -0.08081359416246414, 0.2875097990036011, -0.10683660209178925, -0.03418903425335884, 0.14166121184825897, 0.2668444514274597, 0.23998863995075226, 0.32029998302459717, 0.020949551835656166, 0.18235713243484497, -0.004288746975362301, 0.26569032669067383, 0.128373920917511, 0.17563968896865845, 0.11809725314378738, 0.12359920144081116, 0.15998277068138123, 0.21215981245040894, 0.21717023849487305, 0.06095798313617706, -0.009664659388363361, 0.3273239731788635, 0.32735052704811096, -0.09256653487682343, -0.14575842022895813, 0.2661307156085968, 0.17367564141750336, 0.15075275301933289, 0.22312647104263306, 0.1917957216501236, 0.0946676954627037, -0.03274834528565407, 0.4878978729248047, 0.1294008195400238, 0.30064019560813904, -0.06718065589666367, 0.2247321754693985, 0.1735953837633133, 0.16008858382701874, 0.1014539897441864, 0.4870687425136566, 0.2741689383983612, 0.15161509811878204, 0.21199682354927063, 0.553358793258667, -0.04448983073234558, 0.2003999948501587, 0.17199142277240753, 0.1893366426229477, 0.36376887559890747, 0.00847670529037714, -0.00875148456543684, 0.010743310675024986, 0.0075647542253136635, 0.13777892291545868, 0.0035494614858180285, 0.022198067978024483, -0.04859801009297371, 0.1366337686777115, -0.06717289239168167, 0.27044418454170227, 0.2703269422054291, 0.4031636416912079, 0.0069567011669278145, 0.2049572616815567, 0.07800829410552979, -0.0013554214965552092, 0.20576322078704834, 0.08123750984668732, 0.46976423263549805, 0.27959129214286804, 0.23545989394187927, 0.1626330018043518, 0.5620593428611755, 0.24810807406902313, -0.0034128136467188597, 0.6497325897216797, 0.16894938051700592, 0.41102686524391174, 0.12244895100593567, 0.2868502140045166, -0.04661194607615471, 0.18838980793952942, 0.2321481555700302, 0.10947031527757645, 0.26770034432411194, 0.27847447991371155, 0.22346505522727966, 0.2528698742389679, 0.30823424458503723, 0.2580747604370117, 0.0342160202562809, 0.06145387887954712, 0.2159222811460495, 0.05762387812137604, 0.00235203024931252, -0.0270686037838459, 0.37685427069664, 0.27454712986946106, 0.0687187910079956, 0.038774505257606506, 0.21855562925338745, -0.017403943464159966, 0.5169426202774048, 0.10744178295135498, 0.46388164162635803, 0.04270344600081444, 0.21318510174751282, 0.570572018623352, 0.4772716760635376, 0.14931614696979523, 0.050718922168016434, -0.06583870202302933, 0.34716519713401794, 0.28346481919288635, 0.11036550998687744, 0.36698609590530396, -0.011674541048705578, 0.40997064113616943, 0.1687450110912323, 0.30615776777267456, 0.2635286748409271, 0.3174164295196533, -0.04129787161946297, 0.21804703772068024, -0.028932997956871986, 0.2102862447500229, -0.02948562055826187, 0.03581377491354942, 0.020957669243216515, 0.12989425659179688, -0.013974661938846111, 0.16744159162044525, 0.07039959728717804, 0.03397316113114357, 0.004170597530901432, 0.34419336915016174, 0.22165875136852264, 0.03638780862092972, -0.014164994470775127, 0.4406886100769043, 0.25524619221687317, -0.014501550234854221, -0.0034940175246447325, -0.0772496834397316, 0.02600543014705181, -0.0624103881418705, 0.047690752893686295, 0.14794650673866272, 0.3398025929927826, -0.19361364841461182, 0.0069567011669278145, 0.00648865569382906, 0.37693294882774353, 0.2876748740673065, 0.06873290240764618, 0.09196458756923676, -0.0013861607294529676, 0.18399959802627563, -0.0012738693039864302, 0.10025309771299362, 0.5161744356155396, -0.0124304024502635, 0.5528501868247986, 0.06486162543296814, 0.43830665946006775, 0.3485492169857025, 0.24460285902023315, 0.17767150700092316, 0.10389826446771622, 0.4306848347187042, 0.02593977376818657, 0.08118665218353271, -0.009599323384463787, 0.26432451605796814, 0.39444026350975037, -0.11959889531135559, 0.3778776228427887, 0.4065493047237396, 0.04962650686502457, -0.027030250057578087, 0.3615896999835968, 0.3699224591255188, 0.17961826920509338, -0.025250952690839767, 0.030772997066378593, 0.3304455578327179, 0.007818472571671009, 0.09533090144395828, 0.2443951964378357, 0.19530631601810455, -0.056352969259023666, -0.04726279899477959, 0.05212882161140442, 0.2198561578989029, 0.19997264444828033, 0.09555938839912415, 0.19009026885032654, 0.37148600816726685, 0.16241124272346497, 0.3407149314880371, -0.0972452312707901, 0.019267069175839424, 0.2911306619644165, -0.06073863431811333, -0.011658758856356144, 0.2786085307598114, -0.0021225500386208296, 0.027486806735396385, 0.41710785031318665, -0.12634576857089996, 0.17722581326961517, 0.21182133257389069, 0.11845430731773376, 0.3400281071662903, -0.05631326138973236, 0.19814537465572357, 0.1223117932677269, 0.18016517162322998, -0.01656980998814106, -0.07246936857700348, 0.0332474559545517, 0.434120774269104, 0.47284579277038574, 0.19834814965724945, 0.22118958830833435, 0.15307210385799408, 0.28385090827941895, -0.01859412156045437, 0.11970777809619904, 0.13515593111515045, 0.19832159578800201, 0.02460584230720997, 0.13758550584316254, 0.3002566993236542, 0.10611888021230698, 0.20086924731731415, 0.12653683125972748, 0.497124046087265, 0.07713137567043304, 0.09470310062170029, -0.12114633619785309, 0.5102929472923279, -0.041704077273607254, 0.12358145415782928, 0.3201492428779602, 0.01987503655254841, -0.08838354051113129, 0.02989923022687435, -0.035539258271455765, 0.39251241087913513, 0.11437662690877914, -0.010127670131623745, 0.06016836315393448, 0.11344823986291885, 0.5561308860778809, 0.04390871152281761, 0.4164954423904419, -0.04227038100361824, -0.042579203844070435, 0.04430294409394264, 0.15732990205287933, -0.07532960921525955, -0.04479581117630005, 0.20043796300888062, 0.2562336027622223, 0.13579127192497253, 0.25081366300582886, 0.07289184629917145, -0.09136881679296494, 0.33117982745170593, 0.3368646800518036, 0.3641561269760132, 0.21717023849487305, 0.05766844376921654, 0.00934309046715498, 0.3386988341808319, -0.0037775698583573103, -0.06020987033843994, -0.07753463834524155, 0.31627440452575684, 0.2482355684041977, 0.030503271147608757, -0.012881125323474407, 0.021596500650048256, 0.06470838189125061, -0.0681828111410141, 0.11300817131996155, 0.39352598786354065, 0.05735943093895912, 0.2944718897342682, 0.11263016611337662, -0.05204799771308899, -0.15679040551185608, 0.3399745523929596, 0.3754355013370514, 0.23465581238269806, -0.02804633602499962, 0.15689286589622498, 0.36217522621154785, -0.06354933232069016, -0.013473276980221272, 0.2411590814590454, -0.028450658544898033, 0.4478437006473541, 0.1914457231760025, 0.03855009377002716, 0.19344308972358704, 0.20233815908432007, -0.1209094226360321, 0.019007230177521706, 0.06211128830909729, 0.03948831185698509, 0.19587451219558716, 0.27354246377944946, 0.24526597559452057, 0.022618159651756287, 0.3511498272418976, -0.04181266948580742, 0.2613683044910431, 0.0874001532793045, 0.20215646922588348, 0.2573181092739105, 0.22612246870994568, 0.05793628469109535, -0.034155022352933884, -0.028137849643826485, 0.13526156544685364, 0.30897873640060425, 0.2537621259689331, 0.019124647602438927, 0.17525067925453186, -0.04564325883984566, 0.18542444705963135, 0.29892411828041077, -0.04809870198369026, 0.23469430208206177, 0.019301285967230797, 0.08736123889684677, 0.17278799414634705, 0.09060348570346832, -0.00875148456543684, 0.00653474498540163, 0.04975968971848488, 0.2846936583518982, 0.16903257369995117, 0.09393899142742157, 0.1050940454006195, -0.0007833305862732232, 0.5373604893684387, 0.010056988336145878, 0.4619038999080658, 0.20653696358203888, 0.3000198006629944, 0.29902225732803345, -0.060305479913949966, 0.4172600209712982, 0.0646992027759552, 0.30242785811424255, 0.32383909821510315, 0.12242726981639862, -0.10524113476276398, 0.1868671029806137, 0.09935237467288971, -0.10773099958896637, 0.17499832808971405, 0.34284865856170654, -0.015841929242014885, 0.2862115502357483, 0.008839246816933155, 0.00951369572430849, 0.10777007043361664, 0.07572376728057861, 0.015273195691406727, 0.14709077775478363, 0.32435041666030884, -0.013608439825475216, -0.03575335815548897, -0.011101418174803257, 0.24461151659488678, 0.48914235830307007, 0.29516908526420593, -0.05814744159579277, 0.23055370151996613, -0.1081613078713417, 0.1619722843170166, 0.055172860622406006, 0.06260181218385696, -0.02699493058025837, 0.11022786796092987, 0.2909172773361206, -0.01223811972886324, 0.2726878225803375, 0.030736522749066353, 0.1381560117006302, 0.35369354486465454, 0.03513360396027565, 0.2298652082681656, 0.512184739112854, 0.3840629458427429, 0.33770230412483215, 0.4397556185722351, 0.656007707118988, 0.15681150555610657, 0.3279634416103363, 0.2177124172449112, 0.14037230610847473, 0.004266311414539814, 0.2427760511636734, 0.22863954305648804, 0.16694305837154388, 0.09474969655275345, -0.0067666685208678246, -0.012707679532468319, -0.025207359343767166, 0.01209354493767023, 0.19364182651042938, 0.058011747896671295, 0.18462060391902924, 0.17721019685268402, 0.013292447663843632, 0.06935139745473862, 0.21098946034908295, -0.021706758067011833, 0.0027691510040313005, 0.3589943051338196, 0.12396732717752457, 0.048526156693696976, 0.41587913036346436, 0.11641652137041092, 0.09877882152795792, -0.016884902492165565, -0.013629256747663021, 0.24381288886070251, 0.130729541182518, 0.1982281357049942, -0.024397989735007286, 0.14454308152198792, 0.537999153137207, -0.06330522894859314, 0.09075899422168732, 0.19664715230464935, 0.3995799422264099, 0.15787412226200104, 0.17195920646190643, 0.41766244173049927, -0.03346700593829155, 0.31097641587257385, 0.3781397342681885, 0.335286945104599, 0.1702195554971695, 0.25057417154312134, 0.12689310312271118, 0.4474794566631317, -0.03472381457686424, -0.08225231617689133, 0.5096474289894104, 0.19146855175495148, 0.13359420001506805, 0.3000189960002899, 0.009273574687540531, 0.17672188580036163, 0.12616446614265442, 0.13035309314727783, -0.04791172221302986, -0.03495480492711067, 0.1657673418521881, 0.006872032769024372, 0.17165035009384155, 0.5712032914161682, 0.5127791166305542, 0.25436991453170776, 0.1973133534193039, 0.02249118499457836, 0.026928333565592766, 0.2131780982017517, 0.161951944231987, 0.22444671392440796, -0.02969006635248661, 0.5814366936683655, 0.2644091248512268, 0.20163488388061523, 0.23720116913318634, 0.2956264913082123, 0.03268633410334587, -0.0016075733583420515, 0.015614827163517475, -0.08691233396530151, 0.026722395792603493, 0.29631173610687256, -0.05153839662671089, 0.15297944843769073, 0.060558196157217026, 0.00572455208748579, 0.10800906270742416, 0.2589518129825592, 0.46433672308921814, 0.2228817492723465, 0.25346672534942627, -0.0988556444644928, 0.2675686478614807, -0.04026447981595993, 0.0548524409532547, 0.3919079899787903, 0.12258940190076828, 0.07608377933502197], \"y\": [-0.13555872440338135, -0.16460849344730377, -0.4387456178665161, -0.2708413004875183, -0.20032112300395966, -0.127985417842865, -0.31267768144607544, -0.3602776527404785, -0.24520747363567352, -0.16772906482219696, -0.35649073123931885, -0.2538887560367584, -0.6820013523101807, -0.4416227638721466, -0.09013616293668747, -0.37045368552207947, -0.39718741178512573, -0.5197680592536926, -0.3322076201438904, -0.13240088522434235, -0.18941958248615265, -0.38442787528038025, -0.13184179365634918, -0.26138564944267273, -0.19820088148117065, -0.26566794514656067, -0.22359423339366913, -0.43599867820739746, -0.09782727807760239, -0.4037250280380249, -0.2020728439092636, -0.24889907240867615, -0.2653522491455078, -0.34109893441200256, -0.3487671911716461, -0.23397310078144073, -0.2789393663406372, -0.2831267714500427, -0.4875226318836212, -0.14319394528865814, -0.33429861068725586, -0.38769057393074036, -0.6434528827667236, -0.32132741808891296, -0.3463880121707916, -0.40228089690208435, -0.12931500375270844, -0.3659152090549469, -0.20697472989559174, -0.568801760673523, -0.18135228753089905, -0.32584938406944275, -0.3218306303024292, -0.293613463640213, -0.22374488413333893, -0.4625185430049896, -0.10425934940576553, -0.2186390459537506, -0.32945504784584045, -0.12004248052835464, -0.2125558704137802, -0.3160019516944885, -0.4198634624481201, -0.32746049761772156, -0.2560771405696869, -0.4511919319629669, -0.3032580614089966, -0.2366427183151245, -0.39551350474357605, -0.44199904799461365, -0.17155048251152039, -0.12399440258741379, -0.7306110262870789, -0.41351014375686646, -0.1843176931142807, -0.2940886318683624, -0.3847188353538513, -0.3190244436264038, -0.08936517685651779, -0.39455446600914, -0.3457656800746918, -0.25134339928627014, -0.2860708236694336, -0.351596862077713, -0.4119337797164917, -0.19358552992343903, -0.48578086495399475, -0.25107836723327637, -0.11930645257234573, -0.3169257938861847, -0.06982170790433884, -0.4886806309223175, -0.057375188916921616, -0.36313244700431824, -0.2635202705860138, -0.11108208447694778, -0.13000120222568512, -0.5285422801971436, -0.3654457628726959, -0.32403063774108887, -0.5384564399719238, -0.15185563266277313, -0.4431363046169281, -0.43183621764183044, -0.11886295676231384, -0.4093863368034363, -0.13602666556835175, -0.40286514163017273, -0.15042893588542938, -0.2820225954055786, -0.3403381407260895, -0.43909358978271484, -0.6069523096084595, -0.2877773940563202, -0.2858666479587555, -0.1895347386598587, -0.39383628964424133, -0.5116007328033447, -0.14150160551071167, -0.19293810427188873, -0.21315647661685944, -0.18900029361248016, -0.3622813820838928, -0.4419907033443451, -0.2243846356868744, -0.3566150367259979, -0.20113065838813782, -0.45482683181762695, -0.0501951240003109, -0.48538675904273987, -0.3297763764858246, -0.4476933181285858, -0.6660054922103882, -0.12068839371204376, -0.2402353584766388, -0.07033634185791016, -0.4437576234340668, -0.447611004114151, -0.3027299642562866, -0.2999507486820221, -0.26741823554039, -0.4089576303958893, -0.09160035848617554, -0.2733706831932068, -0.37500208616256714, -0.38719722628593445, -0.14478597044944763, -0.26047757267951965, -0.38701459765434265, -0.28240513801574707, -0.2535472810268402, -0.2185007929801941, -0.17572243511676788, -0.3013959527015686, -0.38118329644203186, -0.42708051204681396, -0.22792136669158936, -0.16271188855171204, -0.1494845449924469, -0.3668869137763977, -0.3602275550365448, -0.08625319600105286, -0.20100148022174835, -0.5140657424926758, -0.07338057458400726, -0.4142697751522064, -0.3182533383369446, -0.630072295665741, -0.285228431224823, -0.22126348316669464, -0.09347845613956451, -0.3680447041988373, -0.2846354842185974, -0.26509788632392883, -0.20997297763824463, -0.5143136978149414, -0.31821373105049133, -0.3732653558254242, -0.5244256258010864, -0.3948839008808136, -0.5387363433837891, -0.2999017536640167, -0.3008261024951935, -0.2221941202878952, -0.3296928405761719, -0.35624805092811584, -0.1672000139951706, -0.2717720568180084, -0.2948037087917328, -0.7736576199531555, -0.13512438535690308, -0.47648823261260986, -0.28178441524505615, -0.2330009788274765, -0.3938004672527313, -0.291410356760025, -0.24063335359096527, -0.13514184951782227, -0.2625235617160797, -0.44799500703811646, -0.2949199378490448, -0.35962462425231934, -0.16160525381565094, -0.4024066925048828, -0.42077550292015076, -0.2761647403240204, -0.16976040601730347, -0.13005034625530243, -0.2227681279182434, -0.23317579925060272, 0.007635650224983692, -0.36074399948120117, -0.33824828267097473, -0.1841900795698166, -0.26516687870025635, -0.5278469324111938, -0.44288042187690735, -0.40980860590934753, -0.34073591232299805, -0.12334298342466354, -0.089169442653656, -0.20029397308826447, -0.17203210294246674, -0.44219544529914856, -0.4729381501674652, -0.29521045088768005, -0.3593864440917969, -0.23447054624557495, -0.0689411461353302, -0.1554211676120758, -0.05671999230980873, -0.3476495146751404, -0.3717961609363556, -0.06032037362456322, -0.3223188817501068, -0.090638168156147, -0.2768716514110565, -0.49000096321105957, -0.7791847586631775, -0.21997155249118805, -0.17393220961093903, -0.42153000831604004, -0.20873388648033142, -0.3989849090576172, -0.21196483075618744, -0.20391498506069183, -0.32508257031440735, -0.14954127371311188, -0.35744792222976685, -0.38002899289131165, -0.17080837488174438, -0.41829177737236023, -0.2954437732696533, -0.36432743072509766, -0.3443823456764221, -0.4091251790523529, -0.3678026497364044, -0.11957761645317078, -0.27826762199401855, -0.10846079885959625, -0.40677645802497864, -0.2856324315071106, -0.4065731465816498, -0.19230671226978302, -0.386686772108078, -0.004308697767555714, -0.18442519009113312, -0.19498272240161896, -0.32641470432281494, -0.40698686242103577, -0.32848453521728516, -0.49526944756507874, -0.359997034072876, -0.15918150544166565, -0.25451356172561646, -0.5294023156166077, -0.2813102900981903, -0.1680135726928711, -0.355597585439682, -0.3033260107040405, -0.06107903644442558, -0.27011606097221375, -0.6390222311019897, -0.17105630040168762, -0.2958018183708191, -0.15693402290344238, -0.39229992032051086, -0.3682458996772766, -0.30142080783843994, -0.3001677691936493, -0.2515554428100586, -0.2626305818557739, -0.13393668830394745, -0.18133525550365448, -0.28437772393226624, -0.23565512895584106, -0.15543416142463684, -0.16026422381401062, -0.06802021712064743, -0.5594712495803833, -0.19772888720035553, -0.24033786356449127, -0.6949763894081116, -0.3420291244983673, -0.06304125487804413, -0.2710268199443817, -0.07683291286230087, -0.5221368670463562, -0.10619925707578659, -0.11589936167001724, -0.43593576550483704, -0.205546036362648, -0.3544815182685852, -0.34508389234542847, -0.25985315442085266, -0.008155723102390766, -0.6139355301856995, -0.45843324065208435, -0.3487086296081543, -0.3354201316833496, -0.5125073790550232, -0.3717961609363556, -0.4089158773422241, -0.353167325258255, -0.11793448776006699, -0.14935709536075592, -0.4841642379760742, -0.1986614167690277, -0.4076456129550934, -0.37960898876190186, -0.1314430832862854, -0.4397728443145752, -0.4289187788963318, -0.37205758690834045, -0.2105715274810791, -0.2598234713077545, -0.2987419664859772, -0.5488123297691345, -0.2113437056541443, -0.2684134244918823, -0.22984579205513, -0.23763549327850342, -0.2465621531009674, -0.05670055001974106, -0.4530164897441864, -0.3681073486804962, -0.4807675778865814, -0.5673913955688477, -0.24577052891254425, -0.12401702255010605, -0.10194871574640274, -0.09082546085119247, -0.04958490654826164, -0.11934726685285568, -0.3251148462295532, -0.4910139739513397, -0.33214861154556274, -0.19983777403831482, -0.15862520039081573, -0.3358376920223236, -0.37625131011009216, -0.6389707922935486, -0.08178389817476273, -0.24153392016887665, -0.24375106394290924, -0.3851512670516968, -0.22700543701648712, -0.2463562935590744, -0.2485472708940506, -0.3226369023323059, -0.38089561462402344, -0.21757949888706207, -0.2081211358308792, -0.15001468360424042, -0.02430625446140766, -0.5100175142288208, -0.5479738712310791, -0.11498374491930008, -0.547880232334137, -0.5730809569358826, -0.21698430180549622, -0.25987759232521057, -0.18941958248615265, -0.2984413504600525, -0.22427275776863098, -0.28649917244911194, -0.21881230175495148, -0.35324057936668396, -0.22548753023147583, -0.27732834219932556, -0.1850566565990448, -0.25365567207336426, -0.4552552103996277, -0.0837368369102478, -0.10451206564903259, -0.23309601843357086, -0.32040920853614807, -0.23696260154247284, -0.4821418225765228, -0.09482373297214508, -0.3214893639087677, -0.32070523500442505, -0.30295154452323914, -0.33800169825553894, -0.4596632421016693, -0.3997195363044739, -0.19788296520709991, -0.16677743196487427, -0.23915225267410278, -0.30417412519454956, -0.5421375632286072, -0.11811166256666183, -0.4221729040145874, -0.5045399069786072, -0.25256869196891785, -0.1184273436665535, -0.17176230251789093, -0.4168478846549988, -0.5292805433273315, -0.07952383905649185, -0.3213779926300049, -0.2765962481498718, -0.14851677417755127, -0.4456880986690521, -0.7736576199531555, -0.2569178640842438, -0.2119440883398056, -0.2137743979692459, -0.2990131974220276, -0.18119263648986816, -0.1514320820569992, -0.13902567327022552, -0.28619176149368286, -0.20802699029445648, -0.2377425581216812, -0.16374483704566956, -0.19982147216796875, -0.170880526304245, -0.11898447573184967, -0.4081295132637024, -0.5951874256134033, -0.3377487361431122, -0.4303501546382904, -0.17704100906848907, -0.450448215007782, -0.2760365307331085, -0.4356713891029358, -0.365255743265152, -0.3638289272785187, -0.1876082420349121, -0.3764042258262634, -0.3053194284439087, -0.15365363657474518, -0.1214977279305458, -0.32819852232933044, -0.3077719509601593, -0.297330766916275, -0.18993239104747772, -0.18717128038406372, -0.1610620766878128, -0.27013009786605835, -0.2878781259059906, -0.36966776847839355, -0.3797118067741394, -0.22314243018627167, -0.337265282869339, -0.3810267150402069, -0.27837949991226196, -0.11352855712175369, -0.3408026397228241, -0.2606746554374695, -0.5647925138473511, -0.3542684018611908, -0.5849948525428772, -0.28321120142936707, -0.38929563760757446, -0.2455352395772934, -0.29877278208732605, -0.29706549644470215, -0.4530460834503174, -0.29251420497894287, -0.08650919049978256, -0.32126981019973755, -0.22610785067081451, -0.12077683955430984, -0.10274334996938705, -0.31372731924057007, -0.5150797367095947, -0.3045900762081146, -0.1344723254442215, -0.5907797813415527, -0.2223367989063263, -0.3673926591873169, -0.3497176468372345, -0.13338319957256317, -0.21714216470718384, -0.46424195170402527, -0.4689409136772156, -0.19242127239704132, -0.548356831073761, -0.05798310041427612, -0.1396997570991516, -0.3137511610984802, -0.2493961900472641, -0.17546416819095612, -0.11849454045295715, -0.23608525097370148, -0.45134931802749634, -0.19032622873783112, -0.17471098899841309, -0.29529452323913574, -0.2321154922246933, -0.5577213764190674, -0.15328054130077362, -0.12405121326446533, -0.4143247902393341, -0.34837624430656433, -0.2624695599079132, -0.31419578194618225, -0.45040425658226013, -0.5422775149345398, -0.19035100936889648, -0.5655550360679626, -0.07659222185611725, -0.12295389175415039, -0.4922087788581848, -0.35985803604125977, -0.3155856132507324, -0.30438289046287537, -0.3979165256023407, -0.1328294575214386, -0.26468417048454285, -0.27857792377471924, -0.0743018090724945, -0.5083912014961243, -0.14046210050582886, -0.3175818622112274, -0.2904476821422577, -0.1115342453122139, -0.37691769003868103, -0.27032819390296936, -0.3272079527378082, -0.22715918719768524, -0.30148059129714966, -0.10932067036628723, -0.15690059959888458, -0.37018048763275146, -0.13668417930603027, -0.041547320783138275, -0.49009081721305847, -0.24511171877384186, -0.3238765597343445, -0.16055265069007874, -0.33420199155807495, -0.5239661931991577, -0.4094611704349518, -0.40038079023361206, -0.2554886043071747, -0.11384502798318863, -0.28934767842292786, -0.2923034131526947, -0.11719320714473724, -0.2900538742542267, -0.2122790515422821, -0.22178326547145844, -0.03328048437833786, -0.24636194109916687, -0.015027419663965702, -0.1517641544342041, -0.24830381572246552, -0.27726051211357117, -0.14649847149848938, -0.18566171824932098, -0.3045900762081146, -0.16507764160633087, -0.3834441006183624, -0.3263738453388214, -0.15892566740512848, -0.2558377981185913, -0.5028784275054932, -0.3609202206134796, -0.2917487621307373, -0.39903733134269714, -0.10023782402276993, -0.21511368453502655, -0.36554276943206787, -0.2503304183483124, -0.17109298706054688, -0.28841710090637207, -0.34286394715309143, -0.04580176994204521, -0.10879896581172943, -0.33351799845695496, -0.5073750615119934, -0.10883405059576035, -0.489471435546875, -0.5069261193275452, -0.36822444200515747, -0.3840983211994171, -0.0781804770231247, -0.45168453454971313, -0.359306275844574, -0.22237099707126617, -0.6561854481697083, -0.11523628979921341, -0.884572446346283, -0.5545159578323364, -0.5438929200172424, -0.13250616192817688, -0.4335140287876129, -0.12014966458082199, -0.27828481793403625, -0.052875667810440063, -0.09592480212450027, -0.2047683447599411, -0.4128200113773346, -0.49638667702674866, -0.43779370188713074, -0.3030907213687897, -0.3954257369041443, -0.25637492537498474, -0.11474809795618057, -0.12559683620929718, -0.06204840540885925, -0.09482373297214508, -0.18730242550373077, -0.19572864472866058, -0.2299051433801651, -0.39022064208984375, -0.20562398433685303, -0.19460713863372803, -0.5024399757385254, -0.3180210590362549, -0.42804136872291565, -0.5826436877250671, -0.28620970249176025, -0.30782780051231384, -0.2954127788543701, -0.1949033886194229, -0.45107075572013855, -0.5634232759475708, -0.14673227071762085, -0.11892642825841904, -0.311738520860672, -0.13321353495121002, -0.2543566823005676, -0.06779038161039352, -0.12967002391815186, -0.23681457340717316, -0.5281640291213989, -0.09678690880537033, 0.014676245860755444, -0.44308656454086304, -0.27021336555480957, -0.1751062422990799, -0.2916944622993469, -0.48846200108528137, -0.49902763962745667, -0.13113394379615784, -0.16890797019004822, -0.3588855266571045, -0.2954578697681427, -0.2786807119846344, -0.17671653628349304, -0.08643928915262222, -0.38652676343917847, -0.29703837633132935, -0.4565732777118683, -0.3618124723434448, -0.5330637693405151, -0.2853432297706604, -0.1260307878255844, -0.21171961724758148, -0.5119539499282837, -0.30006009340286255, -0.339411199092865, -0.13386200368404388, -0.1890024095773697, -0.4132870137691498, -0.21762871742248535, -0.19950410723686218, -0.4411347210407257, -0.6090646386146545, -0.6571049094200134, -0.15595392882823944, -0.2592278718948364, -0.10221005231142044, -0.5748785734176636, -0.21969512104988098, -0.3105451762676239, -0.7124921679496765, -0.14825847744941711, -0.5236537456512451, -0.1067328229546547, -0.3121514320373535, -0.194188192486763, -0.157143697142601, -0.2955244481563568, -0.14379382133483887, -0.284650981426239, -0.4662612974643707, -0.16816942393779755, -0.2017456442117691, -0.31277698278427124, -0.09109601378440857, -0.06200007349252701, -0.12457280606031418, -0.15445956587791443, -0.1190294548869133, -0.12356489151716232, -0.39259210228919983, -0.19415296614170074, -0.2696395814418793, -0.05892624333500862, -0.2260429710149765, -0.3936821520328522, -0.4094262421131134, -0.2405313402414322, -0.17496830224990845, -0.023186983540654182, -0.5258939266204834, -0.7072973847389221, -0.3621514141559601, -0.23424862325191498, -0.33782055974006653, -0.12472651153802872, -0.3530779480934143, -0.6941769123077393, -0.5261125564575195, -0.5199552178382874, -0.16645082831382751, -0.2320510596036911, -0.14780884981155396, -0.07521169632673264, -0.23728235065937042, -0.1973515897989273, -0.34641218185424805, -0.2855307161808014, -0.3606238067150116, -0.17735333740711212, -0.1881776750087738, -0.1894773244857788, -0.2210974097251892, -0.13649357855319977, -0.34073591232299805, -0.25842612981796265, -0.2258753925561905, -0.10004540532827377, -0.4156492352485657, -0.19082431495189667, -0.5945163369178772, -0.26873815059661865, -0.2588648200035095, -0.27842602133750916, -0.48222073912620544, -0.2941133379936218, -0.61656254529953, -0.5113627314567566, -0.4560915529727936, -0.1881776750087738, -0.1862749308347702, -0.45553264021873474, -0.23066435754299164, -0.2839468717575073, -0.3253372609615326, -0.42597827315330505, -0.14794163405895233, -0.25582876801490784, -0.10441160947084427, -0.5931625366210938, -0.21112024784088135, -0.10473760217428207, -0.4759952127933502, -0.10685992240905762, -0.21243178844451904, -0.14695985615253448, -0.2723183333873749, -0.525006115436554, -0.18153312802314758, -0.33506038784980774, -0.29098501801490784, -0.08403254300355911, -0.2492956966161728, -0.5821678042411804, -0.04444926977157593, -0.2751156985759735, -0.09248708933591843, -0.3860015273094177, -0.21992705762386322, -0.4577704966068268, -0.38086971640586853, -0.13529711961746216, -0.20533664524555206, -0.2352975457906723, -0.16218967735767365, -0.403268963098526, -0.31897398829460144, -0.3742089867591858, -0.49793097376823425, -0.23917411267757416, -0.24538670480251312, -0.43576517701148987, -0.18447239696979523, -0.3629594147205353, -0.15469837188720703, -0.21193869411945343, -0.5504631400108337, -0.19838781654834747, -0.20425726473331451, -0.20453865826129913, -0.32902875542640686, -0.5612558126449585, -0.16620269417762756, -0.5438929200172424, -0.3299592435359955, -0.15876004099845886, -0.14897730946540833, -0.4664452373981476, -0.17119991779327393, -0.6108582019805908, -0.14570514857769012, -0.11690586805343628, -0.4546150267124176, -0.2334151566028595, -0.4874511659145355, -0.41149309277534485, -0.2121056169271469, -0.22819839417934418, -0.19158880412578583, -0.1505073606967926, -0.026569467037916183, -0.2082875669002533, -0.1481630951166153, -0.26603060960769653, -0.151823028922081, -0.4873996675014496, -0.662601888179779, -0.5036657452583313, -0.039471544325351715, -0.200996533036232, -0.5364602208137512, -0.23383073508739471, -0.4651283919811249, -0.4392070472240448, -0.2770305275917053, -0.10038910061120987, -0.3410351276397705, -0.10952309519052505, -0.3190467357635498, -0.37184014916419983, -0.0051592374220490456, -0.2355065494775772, -0.4481236934661865, -0.19816744327545166, -0.35824209451675415, -0.26175588369369507, -0.247431218624115, -0.6608948707580566, -0.0923541858792305, -0.1707615703344345, -0.33125653862953186, -0.1344723254442215, -0.39903733134269714, -0.05812949687242508, -0.20526517927646637, -0.10881052911281586, -0.10919558256864548, -0.42292845249176025, -0.2705186903476715, -0.059280432760715485, -0.38854101300239563, -0.25557267665863037, -0.3142094314098358, -0.24062667787075043, -0.14197970926761627, -0.2598121464252472, -0.2002306580543518, -0.3044961988925934, -0.29168564081192017, -0.25507551431655884, -0.228875994682312, -0.24446244537830353, -0.5020482540130615, -0.04706401377916336, -0.6603101491928101, -0.41726651787757874, -0.32442986965179443, -0.25449809432029724, -0.23637638986110687, -0.2916944622993469, -0.3290908634662628, -0.27456799149513245, -0.404869943857193, -0.09874896705150604, -0.1535394787788391, -0.40038079023361206, -0.2968856394290924, -0.4356328547000885, -0.27073147892951965, -0.18180526793003082, -0.7838432192802429, -0.10204699635505676, -0.28515854477882385, -0.5200400352478027, -0.30239102244377136, -0.32590439915657043, -0.2541959583759308, -0.10390123724937439, -0.1364535242319107, -0.47718754410743713, 0.04199185594916344, -0.15189869701862335, -0.19050787389278412, -0.41223061084747314, -0.09157000482082367, -0.16587106883525848, -0.2243957817554474, -0.4322133958339691, -0.28826093673706055, -0.2308625727891922, -0.11105576902627945, -0.5802227258682251, -0.1114608496427536, -0.5033330917358398, -0.3080404996871948, -0.07220546156167984, -0.2521064579486847, -0.3847913146018982, -0.3134659230709076, -0.7769603133201599, -0.12651878595352173, -0.4310305714607239, -0.35842183232307434, -0.3504457473754883, -0.6529924869537354, -0.271272212266922, -0.1421194076538086, -0.42153000831604004, -0.30127644538879395, -0.17197281122207642, -0.3671092987060547, -0.32021042704582214, -0.1560124009847641, -0.0984436646103859, -0.05312405526638031, -0.5438042283058167, -0.1755959540605545, -0.4775424003601074, -0.46651899814605713, -0.5643424391746521, -0.02478746511042118, -0.18675433099269867, -0.6949763894081116, -0.19772888720035553, -0.1981448233127594, -0.3856672942638397, -0.1935371607542038, -0.17128242552280426, -0.5293371081352234, -0.33940115571022034, -0.3896148204803467, -0.44647452235221863, -0.40952810645103455, -0.2900928258895874, -0.16988645493984222, -0.3860127329826355, -0.09175483137369156, -0.4687201678752899, -0.2156081646680832, -0.522521436214447, -0.26416558027267456, -0.0752154216170311, -0.240372434258461, -0.09243762493133545, -0.11426994949579239, -0.20299431681632996, -0.640648603439331, -0.49062177538871765, -0.2949199378490448, -0.5699411034584045, -0.27951544523239136, -0.2514835596084595, -0.3062882423400879, -0.1228581964969635, -0.23540282249450684, -0.3410044014453888, -0.3175732493400574, -0.17957313358783722, -0.32200127840042114, -0.13351818919181824, -0.1037643626332283, -0.3567386567592621, -0.2013883739709854, -0.4487893283367157, -0.40543875098228455, -0.2462640106678009, -0.2527736723423004, -0.20345892012119293, -0.25178757309913635, -0.4073351323604584, -0.4559020400047302, -0.1832348257303238, -0.5023157596588135, -0.2541162371635437, -0.34586063027381897, -0.10389428585767746, -0.14392316341400146, -0.3610433340072632, -0.3124179244041443, -0.15357421338558197, -0.10669475793838501, -0.32944634556770325, -0.12686820328235626, -0.12144021689891815, -0.3763507604598999, -0.2756882607936859, -0.29314398765563965, -0.42105409502983093, -0.26426181197166443, -0.2612367868423462, -0.10923580825328827, -0.350453644990921, -0.37110698223114014, -0.20066027343273163, -0.3135397434234619, -0.2530156970024109, -0.33954206109046936, -0.10437756031751633, -0.2873910367488861, -0.43520137667655945, -0.23923498392105103, -0.4436149001121521, -0.30247247219085693, -0.31966498494148254, -0.15866713225841522, -0.4600814878940582, -0.48649564385414124, -0.1866665929555893, -0.42711642384529114, -0.09139133244752884, -0.3477674722671509, -0.09333744645118713, -0.5643042922019958, -0.19465342164039612, -0.29845044016838074, -0.21554085612297058, -0.4348796606063843, -0.2796105444431305, -0.25005877017974854, -0.16294105350971222, -0.5516743063926697, -0.24942295253276825, -0.35733944177627563, -0.41192176938056946, -0.6008288860321045, -0.09626693278551102, -0.3479450047016144, -0.5625142455101013, -0.2573682963848114, -0.42545029520988464, -0.2338964343070984, -0.1087726280093193, -0.17943339049816132, -0.32713013887405396, -0.27646809816360474, -0.17632879316806793, -0.05867312103509903, -0.07251857966184616, -0.3263077139854431, -0.17115387320518494, -0.2728143632411957, -0.30984535813331604, -0.591694176197052, -0.05864178016781807, -0.4126166105270386, -0.23044542968273163, -0.21742387115955353, -0.18297959864139557, -0.18943805992603302, -0.4943239986896515, -0.2083350569009781, -0.3095979690551758, -0.3847913146018982, -0.6917011141777039, -0.5158255696296692, -0.2412749081850052, -0.6567152738571167, -0.2549509108066559, -0.4277105927467346, -0.28535494208335876, -0.2082952857017517, -0.1365816742181778, -0.5592806935310364, -0.33888423442840576, -0.1720341295003891, -0.36172619462013245, -0.2860623598098755, -0.4567532241344452, -0.2747001349925995, -0.351241797208786, -0.36684316396713257, -0.2853240966796875, -0.3004057705402374, -0.4480762779712677, -0.11912860721349716, -0.09045084565877914, -0.09426426887512207, -0.4386257529258728, -0.34108108282089233, -0.23598791658878326, -0.027354786172509193, -0.4226105511188507, -0.111402727663517, -0.46674445271492004, -0.07589162141084671, -0.5355353951454163, -0.09771592915058136, -0.2882349193096161, -0.5700971484184265, -0.5379722118377686, -0.3491245210170746, -0.21768522262573242, -0.21781879663467407, -0.5787936449050903, -0.2306102216243744, -0.3489379286766052, -0.5007964372634888, -0.15462704002857208, -0.574622631072998, -0.10934571176767349, -0.41485288739204407, -0.261626273393631, -0.27523455023765564, -0.23598341643810272, -0.22051246464252472, -0.09273643046617508, -0.4556129276752472, -0.12702281773090363, -0.23106731474399567, -0.22411921620368958, -0.2950083017349243, -0.21172146499156952, -0.30891165137290955, -0.31537261605262756, -0.07315129786729813, -0.2732447385787964, -0.5053747892379761, -0.2748013138771057, -0.12064392119646072, -0.37312939763069153, -0.62138831615448, -0.4050537645816803, -0.2668707072734833, -0.09374866634607315, -0.10818620771169662, -0.1661849170923233, -0.18424278497695923, -0.27324479818344116, -0.141999289393425, -0.3844720721244812, -0.21555383503437042, -0.05864178016781807, -0.1512632519006729, -0.297696053981781, -0.3951171338558197, -0.22441276907920837, -0.09083777666091919, -0.0920165404677391, -0.5251045227050781, -0.08973155915737152, -0.20489655435085297, -0.618922770023346, -0.14573588967323303, -0.583617091178894, -0.26188820600509644, -0.40276315808296204, -0.30102968215942383, -0.32730889320373535, -0.3212057650089264, -0.21201229095458984, -0.5986886024475098, -0.26795151829719543, -0.04341524466872215, -0.22932550311088562, -0.31027764081954956, -0.4125196933746338, -0.16587458550930023, -0.38845136761665344, -0.40330544114112854, -0.20459552109241486, -0.14364276826381683, -0.3350440561771393, -0.5584474205970764, -0.25518789887428284, -0.1879281848669052, -0.13068920373916626, -0.3395882546901703, -0.03201255202293396, -0.35546764731407166, -0.4955858886241913, -0.37754520773887634, -0.16091914474964142, -0.3600274920463562, -0.13284428417682648, -0.13154049217700958, -0.2191871553659439, -0.46482858061790466, -0.4299086928367615, -0.42381325364112854, -0.1676328182220459, -0.29201313853263855, -0.12334298342466354, -0.2843151390552521, -0.3200421631336212, -0.12772336602210999, -0.18834728002548218, -0.5596186518669128, -0.2047635018825531, -0.07555722445249557, -0.4099513590335846, -0.19359053671360016, -0.42157575488090515, -0.46597182750701904, -0.31374162435531616, -0.34490981698036194, -0.07729906588792801, -0.35889479517936707, -0.1638193577528, -0.3683236539363861, -0.1580837368965149, -0.15654541552066803, -0.07236594706773758, -0.48685672879219055, -0.5783787965774536, -0.22152110934257507, -0.4684169292449951, -0.43904533982276917, -0.4030938446521759, -0.2640971541404724, -0.26397719979286194, -0.38146474957466125, -0.23548884689807892, -0.12772615253925323, -0.47951969504356384, -0.372085303068161, -0.15787780284881592, -0.1572483628988266, -0.09722879528999329, -0.5986151099205017, -0.47279927134513855, -0.10187090933322906, -0.16231215000152588, -0.5274865627288818, -0.05607984960079193, -0.2110712081193924, -0.3037901818752289, 0.0004915492027066648, -0.13894428312778473, -0.24262139201164246, -0.09590936452150345, -0.43280893564224243, -0.30280759930610657, -0.25232967734336853, -0.28395870327949524, -0.04288407042622566, -0.5553461313247681, -0.21369408071041107, -0.40980860590934753, -0.07712509483098984, -0.21357712149620056, -0.12420227378606796, -0.2137386053800583, -0.1940130740404129, -0.13670697808265686, -0.16757531464099884, -0.30866143107414246, -0.3014705777168274, -0.42399218678474426, -0.3629295527935028, -0.10288364440202713, -0.36233749985694885, -0.32423460483551025, -0.3470100462436676, -0.2873910367488861, -0.1618024855852127, -0.2574022114276886, -0.4873996675014496, -0.13369175791740417, -0.09880180656909943, -0.11368651688098907, -0.46057945489883423, -0.19236543774604797, -0.10669256746768951, -0.08515115827322006, -0.026748737320303917, -0.3968207538127899, -0.29657450318336487, -0.34851521253585815, -0.34229621291160583, -0.18300983309745789, -0.3789074122905731, -0.13333456218242645, -0.06632860004901886, -0.19027917087078094, -0.34638920426368713, -0.37019848823547363, -0.45593196153640747, -0.20796136558055878, -0.2741277515888214, -0.3964575529098511, -0.1240588054060936, -0.15556572377681732, -0.2653849422931671, -0.12280719727277756, -0.4880293309688568, -0.25189390778541565, -0.10712220519781113, -0.33628377318382263, -0.3631514608860016, -0.20995298027992249, -0.06092346832156181, -0.2401985377073288, -0.21868568658828735, -0.21818788349628448, -0.2689771354198456, -0.24386656284332275, -0.13479100167751312, -0.46533581614494324, -0.19371135532855988, -0.4103698134422302, -0.4207168519496918, -0.3834379315376282, -0.18917886912822723, -0.18553897738456726, -0.2826387286186218, -0.06361126154661179, -0.0845101922750473, -0.29419615864753723, -0.4932284951210022, -0.48698773980140686, -0.36626410484313965, -0.2954578697681427, -0.22234781086444855, -0.15700548887252808, -0.7660163044929504, -0.09734158217906952, -0.31504571437835693, -0.11625499278306961, -0.25196194648742676, -0.14351676404476166, -0.18834035098552704, -0.1087726280093193, -0.06800014525651932, -0.09035255759954453, -0.3995817005634308, -0.32643046975135803, -0.26744070649147034, -0.2405322641134262, -0.24793262779712677, -0.5773745179176331, -0.10424917936325073, -0.4463389813899994, -0.46249738335609436, -0.4195092022418976, -0.3854192793369293, -0.08950690180063248, -0.4378141760826111, -0.33708205819129944, -0.6281256675720215, -0.42151519656181335, -0.22168652713298798, -0.10751452296972275, -0.5110952854156494, -0.05177907273173332, -0.33532482385635376, -0.45243486762046814, -0.3684060871601105, -0.24077638983726501, -0.22256115078926086, -0.2895268499851227, -0.012869167141616344, -0.2554187774658203, -0.13926814496517181, -0.4335140287876129, -0.17766359448432922, -0.4285429120063782, -0.0254806000739336, -0.1102578267455101, -0.20589537918567657, -0.5002022981643677, -0.5177134871482849, -0.6958300471305847, -0.1123133972287178, -0.2984136939048767, -0.14000819623470306, -0.17729602754116058, -0.09157000482082367, -0.15526986122131348, -0.0765419527888298, -0.2867271602153778, -0.571912407875061, -0.23270678520202637, -0.25123193860054016, -0.1736152172088623, -0.1851673126220703, -0.5694004893302917, -0.20308804512023926, -0.2752847671508789, -0.48088982701301575, -0.5320216417312622, -0.35028359293937683, -0.5156313180923462, -0.723089337348938, -0.5297906994819641, -0.40780603885650635, -0.4553341567516327, -0.490093857049942, -0.17505723237991333, -0.2782019078731537, -0.16224659979343414, -0.3222600817680359, -0.14990796148777008, -0.10239354521036148, -0.19494011998176575, -0.2423163801431656, 0.0015003931475803256, -0.2508096396923065, -0.4325559735298157, -0.17735405266284943, -0.38433754444122314, -0.1275928020477295, -0.15339542925357819, -0.2591482996940613, -0.36111241579055786, -0.1817740499973297, -0.5744927525520325, -0.18520250916481018, -0.27274125814437866, -0.4120021164417267, -0.27985259890556335, -0.32416221499443054, -0.3153837025165558, -0.275468111038208, -0.17021718621253967, -0.46740564703941345, -0.2519961893558502, -0.26633405685424805, -0.3374108076095581, -0.5746136903762817, -0.14650005102157593, -0.31941020488739014, -0.315434992313385, -0.4086907207965851, -0.2610967457294464, -0.33345827460289, -0.5244238972663879, -0.2406122386455536, -0.3209782838821411, -0.4905446469783783, -0.4756470322608948, -0.31661951541900635, -0.46038541197776794, -0.3377169668674469, -0.42131856083869934, -0.18904419243335724, -0.0873900055885315, -0.49799206852912903, -0.4481236934661865, -0.2846354842185974, -0.3445031940937042, -0.18576495349407196, -0.26595792174339294, -0.22084805369377136, -0.31774353981018066, -0.18082916736602783, -0.21681590378284454, -0.08842810988426208, -0.26443541049957275, -0.29172468185424805, -0.532366931438446, -0.7314524054527283, -0.6020891666412354, -0.2656802833080292, -0.0740174725651741, -0.2392944097518921, -0.44982239603996277, -0.25039243698120117, -0.36960387229919434, -0.1284039318561554, -0.6136909127235413, -0.6470255255699158, -0.4766676723957062, -0.4792298972606659, -0.3369055986404419, -0.3933209478855133, -0.11713869124650955, 0.006782607175409794, -0.16188155114650726, -0.38115909695625305, -0.49479207396507263, -0.20075827836990356, -0.13403840363025665, -0.07429660111665726, -0.29208189249038696, -0.23212717473506927, -0.3677726686000824, -0.4227679669857025, -0.48295676708221436, -0.20798392593860626, -0.14496088027954102, -0.4424815773963928, -0.0968572199344635, -0.1688240021467209, -0.5601783394813538, -0.22905677556991577, -0.14635227620601654]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Likelihood of genders for true negative (male)\"}, \"xaxis\": {\"title\": {\"text\": \"Likelihood male\"}}, \"yaxis\": {\"title\": {\"text\": \"Likelihood female\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('899c1d4a-c4ab-4161-8b62-47e4f4ebf087');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                \n",
       "            </script>\n",
       "        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\r\n",
    "\r\n",
    "unzipped_object = zip(*correct_female_pred)\r\n",
    "unzipped_list = list(unzipped_object)\r\n",
    "\r\n",
    "fig = go.Figure(data=go.Scatter(x=unzipped_list[1],\r\n",
    "                                y=unzipped_list[0],\r\n",
    "                                mode='markers'))\r\n",
    "\r\n",
    "fig.update_layout(title='Likelihood of genders for true positive (female)',\r\n",
    "                  xaxis_title=\"Likelihood female\",\r\n",
    "                  yaxis_title=\"Likelihood male\")\r\n",
    "fig.show()\r\n",
    "\r\n",
    "unzipped_object = zip(*correct_male_pred)\r\n",
    "unzipped_list = list(unzipped_object)\r\n",
    "\r\n",
    "fig = go.Figure(data=go.Scatter(x=unzipped_list[0],\r\n",
    "                                y=unzipped_list[1],\r\n",
    "                                mode='markers'))\r\n",
    "\r\n",
    "fig.update_layout(title='Likelihood of genders for true negative (male)',\r\n",
    "                  xaxis_title=\"Likelihood male\",\r\n",
    "                  yaxis_title=\"Likelihood female\")\r\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Healthy_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "038a94e3fd194f43ba9048afc7f61d40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26a4dc9830f44df494cef1f9c55a58ad",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ccedb21ba3934cfcaedb351bae8fc6f5",
      "value": 440473133
     }
    },
    "0b7b023d9c6f4077b238577653e77968": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0eae4ed219fc453e9bd26848d533f451": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2205967556914c50a79455b6dc095ba2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26a4dc9830f44df494cef1f9c55a58ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0f08fcb7ce4b9d9422a139f4398870": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e252a2a98ba4486896994d2bdd5910c",
      "placeholder": "​",
      "style": "IPY_MODEL_cf9ea15ac15242ea9334b76b5b599b9f",
      "value": " 440M/440M [00:08&lt;00:00, 53.4MB/s]"
     }
    },
    "3a80d3af2e334a06a0c156a9e9745dc4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42f969d278a24482b8505c69c012f4da": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "455ab4311729400a9502b829c1703984": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a80d3af2e334a06a0c156a9e9745dc4",
      "placeholder": "​",
      "style": "IPY_MODEL_fbde772b09d343798e684cabedf04044",
      "value": " 433/433 [00:00&lt;00:00, 938B/s]"
     }
    },
    "46eda40062da431cb8e79b51928efd0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a0b9189e17b41178b0d07aa3b65e3d6",
       "IPY_MODEL_cb8a49f7be7146f0a23c3fd5d6cb541f"
      ],
      "layout": "IPY_MODEL_0eae4ed219fc453e9bd26848d533f451"
     }
    },
    "4de423a8edae442294fc162f5c621ea9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51516d65d62a48e49efadbea2151b4ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_038a94e3fd194f43ba9048afc7f61d40",
       "IPY_MODEL_2c0f08fcb7ce4b9d9422a139f4398870"
      ],
      "layout": "IPY_MODEL_cf7adaf9d18e45c3a2b491a108df356c"
     }
    },
    "5a0b9189e17b41178b0d07aa3b65e3d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8089cfc0fac941dfbcfc9b183c6fc77f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b927b4ca4bad4b519e9ecb564eac41f0",
      "value": 231508
     }
    },
    "5e252a2a98ba4486896994d2bdd5910c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63cb15b41d484fdca18a3d3d4411ec42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96a74738aa51488ab52221131b79feb9",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_42f969d278a24482b8505c69c012f4da",
      "value": 433
     }
    },
    "8089cfc0fac941dfbcfc9b183c6fc77f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96a74738aa51488ab52221131b79feb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8fba9f3f87f412a960335af97128fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63cb15b41d484fdca18a3d3d4411ec42",
       "IPY_MODEL_455ab4311729400a9502b829c1703984"
      ],
      "layout": "IPY_MODEL_4de423a8edae442294fc162f5c621ea9"
     }
    },
    "b927b4ca4bad4b519e9ecb564eac41f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cb8a49f7be7146f0a23c3fd5d6cb541f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2205967556914c50a79455b6dc095ba2",
      "placeholder": "​",
      "style": "IPY_MODEL_0b7b023d9c6f4077b238577653e77968",
      "value": " 232k/232k [00:00&lt;00:00, 800kB/s]"
     }
    },
    "ccedb21ba3934cfcaedb351bae8fc6f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "cf7adaf9d18e45c3a2b491a108df356c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf9ea15ac15242ea9334b76b5b599b9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbde772b09d343798e684cabedf04044": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
