{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Aim to build autoencoder trained on only 1 class and measure the reconstruction error for other classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  oh never mind  i misread the change  my mistake   egern  '\n",
      " '    erik for crying out loud you legally can have sex with children but not with underage children underage children are children beneath the legal age of consent different states have 14 15 16 17 or 18 as the underage limit children in most countries means someone under the age of 18 in ireland for example a child aged 17 is over the age of consent in britain it is 16 do you know the first thing about the topic   the name is standard english asage scandal is a pov term allegation allows the discussion of cases that have not yet come to court which cannot for technical reasons be prosecuted and cases that were thrown up as invalid roman catholic church sex abuses allegations in standard english means allegations about the roman catholic church   public relations is minor tabloid point when you are discussing the rape of children so stop adding in an irrelevant topic   your mucking around lost a lot of changes spelling corrections and other problems solved i was caught in an edit conflict with you and given the scale of the changes could do nothing but a cut and paste job i then tried to go in to your version to transfer over accurate changes but then you started this insane fucking around with the articles location using a pov title i will keep moving back to a carefully worded neutral title  '\n",
      " '   pfortuny on pfortunys mediation  apologies for the delay and for the following text which is far too long and i think out of place but the discussion has led us here by no means i intend my opinion to be regarded as more important than any others but for obvious reasons some people are expecting it here it goes the history of this text is in  which i shall clear this evening the text ends at eot below    after reviewing the history and talking to  i have got the following vision of the facts which is the basis for my opinion expressed below alexs editions on the 29 were done without discussion on the talk page this was clearly a serious mistake which led to the first catastrophe but i think we may assume good faith here at least ingnorance not malice especially if one follows the summaries diffs etc  alexs editions on the 9 and 10 november were done using the talk page but he made edits before discussing them and given the complete history of the article the edits were too bold to be accepted without previous discussion this led to their being systematically reverted which is common use in the wp for that behaviour and in the end to the blocking of the page this behaviour was clearly a mistake of alexs and i put it to his not having read the complete history while i am not excusing him i do believe that most of the problems were a result of confusion caused by his aspergers syndrome nobody can agree not to make unilateral edits adams expression in whatever page for whatever reason this is a bit too generic and has no deadline he is aware of the damages he may incur in damages for himself if he repeats the above behaviour before viajeros suggestion alex had told  he was going to try to stay out of mother teresas page for a while he has privately told me he is not going to touch the criticism section al least for a fortnight it sounds nice to be a mediator but on the other hand it is a responsibility i cannot i mean cannot not do not want to or do not like to take up for the moment if alex behaves like he did on the 910 november i will also agree to any rational punishment i want to stress that i was in disagreement with most of his edits i do not think that alex will cause any further problems  before stating my opinion i would like everybody to know that viajeros suggestion on my mediation was done without either mine or alexs prior consent he later expressed his concern about this to  i feel it is only fair to mention it here  hence my opinion is unblock the page if alex behaves in a non cooperative way then the resources of wikipedia to prevent this non cooperative behaviour will be used as with any other user  but i mean as with any other user if he were going to appear in the problem page he has to be told in his talk page in advance if he were going to be banned then he has to be told explicitly in his own page and by a sysop or whatever the standard procedure is    i have read the above and agree with pfortuny     last sentence was inserted by alex after reviewing the text in my subpage    eot   '\n",
      " '   the arbitration committee banned plautus satire enforcement including distinguishing between reincarnations and folks who happen to share the same beliefs is a matter for the community particularly sysops and developers'\n",
      " ' wow youre so clever so smooth stop being an ass so we can compromise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-708a932df6d7>:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  female_vtoxic['comment'] = female_vtoxic['comment'].str.translate(str.maketrans('', '', string.punctuation))\n",
      "<ipython-input-2-708a932df6d7>:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  female_vtoxic['comment'] = female_vtoxic['comment'].replace('\\n',' ', regex=True)\n",
      "<ipython-input-2-708a932df6d7>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  female_vtoxic['comment'] = female_vtoxic['comment'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "#!conda install -y keras\n",
    "# lstm autoencoder recreate sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "#from keras.utils import plot_model\n",
    "from keras.preprocessing.text import one_hot\n",
    "import string\n",
    "\n",
    "# read files and create dataframe\n",
    "toxicity_comments = pd.read_csv('/mnt/d/Advanced Project/toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "toxicity_annotations = pd.read_csv('/mnt/d/Advanced Project/toxicity_annotations.tsv',  sep = '\\t')\n",
    "toxicity_demographics = pd.read_csv('/mnt/d/Advanced Project/toxicity_worker_demographics.tsv', sep = '\\t')\n",
    "\n",
    "toxicity = toxicity_comments.merge(toxicity_annotations, how ='outer', on=\"rev_id\")\n",
    "toxicity = toxicity.merge(toxicity_demographics, how ='outer', on=\"worker_id\").sort_values(by=['rev_id','worker_id'])\n",
    "\n",
    "# remove newline and tab tokens\n",
    "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "# add binary gender columns\n",
    "toxicity = toxicity[toxicity['gender']!='other']\n",
    "toxicity = pd.concat([toxicity, pd.get_dummies(toxicity.gender).rename(columns = \"{}_binary\".format)], axis = 1)\n",
    "\n",
    "# limit size of dataset for testing purposes\n",
    "very_toxic = toxicity[toxicity.toxicity_score == -2.0]\n",
    "female_vtoxic = very_toxic[very_toxic.female_binary == 1]\n",
    "\n",
    "female_vtoxic['comment'] = female_vtoxic['comment'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "female_vtoxic['comment'] = female_vtoxic['comment'].replace('\\n',' ', regex=True)\n",
    "female_vtoxic['comment'] = female_vtoxic['comment'].str.lower()\n",
    "# female_vtoxic['comment'] = female_vtoxic['comment'].str.split() \n",
    "#display(female_vtoxic['comment'].head(5))\n",
    "\n",
    "comments = female_vtoxic.comment.values\n",
    "# testing on smaller set \n",
    "comments = comments[:100]\n",
    "scores = female_vtoxic.toxicity_score.values\n",
    "# for i in range(len(comments)):\n",
    "#   cmt = str(scores[i])+\" \"+comments[i]\n",
    "#   comments[i] = cmt\n",
    "print(comments[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 3 9 4 5 3 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [6 2 9 2 8 1 2 2 4 2 3 9 3 3 1 2 4 5 9 4]\n",
      " [9 6 9 2 2 9 8 6 7 5 8 2 6 7 9 6 9 5 8 8]\n",
      " [5 9 4 1 4 1 1 9 3 7 1 2 8 4 9 8 1 7 9 2]\n",
      " [7 7 4 4 4 6 4 4 8 7 4 2 4 4 0 0 0 0 0 0]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 232 233 324   5 855   1 325  35 326 856]\n",
      " [103 340 338  27  60   2 487  11 895  53 241  24   1 488 896 145   7 334\n",
      "  245   5  29 146 897 164   3   7 898 899 900 245]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# integer encode the documents\n",
    "vocab_size =10\n",
    "encoded_docs = [one_hot(c, vocab_size) for c in comments]\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 20\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs[:5])\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "num_words = 2000\n",
    "maxlen = 30\n",
    "embed_dim = 150\n",
    "batch_size = 16\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "tokenizer.fit_on_texts(comments)\n",
    "seqs = tokenizer.texts_to_sequences(comments)\n",
    "pad_seqs = pad_sequences(seqs, maxlen)\n",
    "print(pad_seqs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 181ms/step - loss: 7.5454\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 1s 180ms/step - loss: 5.6540\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 164ms/step - loss: 5.1384\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 1s 168ms/step - loss: 5.0348\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 124ms/step - loss: 5.0233\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 1s 213ms/step - loss: 4.9878\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 4.9388\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 4.9584\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 173ms/step - loss: 4.8649\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 4.9030\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-55da051878e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m           epochs=10)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras import Input\n",
    "from keras import Model\n",
    "from keras import regularizers\n",
    "from keras.layers import Bidirectional\n",
    "from keras import optimizers\n",
    "\n",
    "encoder_inputs = Input(shape=(maxlen,), name='Encoder-Input')\n",
    "emb_layer = Embedding(num_words, embed_dim,input_length = maxlen, name='Body-Word-Embedding', mask_zero=False)\n",
    "x = emb_layer(encoder_inputs)\n",
    "state_h = Bidirectional(LSTM(128, activation='relu', name='Encoder-Last-LSTM'))(x)\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "decoded = RepeatVector(maxlen)(seq2seq_encoder_out)\n",
    "decoder_lstm = Bidirectional(LSTM(128, return_sequences=True, name='Decoder-LSTM-before'))\n",
    "decoder_lstm_output = decoder_lstm(decoded)\n",
    "decoder_dense = Dense(num_words, activation='softmax', name='Final-Output-Dense-before')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "seq2seq_Model = Model(encoder_inputs, decoder_outputs)\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "history = seq2seq_Model.fit(pad_seqs, np.expand_dims(pad_seqs, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=10)\n",
    "\n",
    "y = history.predict(pad_seqs)\n",
    "print(y)\n",
    "\n",
    "sentence = \"here's a sample unseen sentence\"\n",
    "seq = tokenizer.texts_to_sequences([sentence])\n",
    "pad_seq = pad_sequences(seq, maxlen)\n",
    "sentence_vec = encoder_model.predict(pad_seq)[0]\n",
    "print(sentence_vec)\n",
    "\n",
    "# input_layer = Input(shape=max_length)\n",
    "\n",
    "# ## encoding part\n",
    "# encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "# encoded = Dense(50, activation='relu')(encoded)\n",
    "\n",
    "# ## decoding part\n",
    "# decoded = Dense(50, activation='tanh')(encoded)\n",
    "# decoded = Dense(100, activation='tanh')(decoded)\n",
    "\n",
    "# ## output layer\n",
    "# output_layer = Dense(max_length, activation='relu')(decoded)\n",
    "\n",
    "# autoencoder = Model(input_layer, output_layer)\n",
    "# autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# autoencoder.fit(padded_docs, padded_docs, \n",
    "#                 batch_size = 256, epochs = 10, \n",
    "#                 shuffle = True, validation_split = 0.20);\n",
    "\n",
    "# # hidden_representation = Sequential()\n",
    "# # hidden_representation.add(autoencoder.layers[0])\n",
    "# # hidden_representation.add(autoencoder.layers[1])\n",
    "# # hidden_representation.add(autoencoder.layers[2])\n",
    "# y = autoencoder.predict(padded_docs)\n",
    "# print(y)\n",
    "# print(len(y))\n",
    "# print(len(y[0]))\n",
    "\n",
    "\n",
    "\n",
    "# define input sequence\n",
    "#sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# reshape input into [samples, timesteps, features]\n",
    "#sequence = [[[i] for i in comments[0]]]\n",
    "#n_in = len(sequence)\n",
    "#sequence = sequence.reshape((1, n_in, 1))\n",
    "#print(sequence)\n",
    "# #define model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "# model.add(LSTM(100, activation='relu'))\n",
    "# model.add(RepeatVector(max_length))\n",
    "# model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(1)))\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# # fit model\n",
    "# model.fit(padded_docs, padded_docs, epochs=100, verbose=0)\n",
    "# #plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n",
    "# # demonstrate recreation\n",
    "# yhat = model.predict([padded_docs[0]], verbose=0)\n",
    "# print(yhat)\n",
    "# print(yhat[0,:,0])\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "# model.add(RepeatVector(n_in))\n",
    "# model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(1)))\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# # fit model\n",
    "# model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "# plot_model(model, show_shapes=True, to_file='reconstruct_lstm_autoencoder.png')\n",
    "# # demonstrate recreation\n",
    "# yhat = model.predict(sequence, verbose=0)\n",
    "# print(yhat[0,:,0])\n",
    "\n",
    "# #model for text summarisation\n",
    "# inputs = Input(shape=(max_length))\n",
    "# encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "# encoder2 = LSTM(128)(encoder1)\n",
    "# encoder3 = RepeatVector(max_length)(encoder2)\n",
    "# # decoder output model\n",
    "# decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "# outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "# # tie it together\n",
    "# model = Model(inputs=inputs, outputs=outputs)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# print(comments[0])\n",
    "# print(padded_docs[0])\n",
    "# yhat = model.predict(padded_docs[0], verbose=0)\n",
    "# print(yhat[0,:,0])\n",
    "\n",
    "# # define the model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# # compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# # summarize the model\n",
    "# print(model.summary())\n",
    "# # fit the model\n",
    "# print(padded_docs.shape)\n",
    "# model.fit(padded_docs,padded_docs, epochs=50, verbose=0)\n",
    "# yhat = model.predict(padded_docs[0], verbose=0)\n",
    "# print(yhat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
