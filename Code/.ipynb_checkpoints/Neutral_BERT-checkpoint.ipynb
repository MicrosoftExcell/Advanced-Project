{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Neutral_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ak_Ca3w_rNy"
      },
      "source": [
        "<h3> BERT classifier v5 trained on balanced data for male/female in neutral class, label = gender, tested gender predictions for neutral class</h3>\r\n",
        "<p>Using code from: https://mccormickml.com/2019/07/22/BERT-fine-tuning/</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBWkIenb_xH-",
        "outputId": "32b1b0a5-7689-4318-a32b-49decadc0981"
      },
      "source": [
        "# import libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "#!conda install -y tensorflow\r\n",
        "#!conda install -y pytorch torchvision -c pytorch\r\n",
        "#!pip install transformers\r\n",
        "import tensorflow as tf\r\n",
        "import torch\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "NrVW-FsUABeB",
        "outputId": "5175995f-253d-4c50-8491-a299f37eeff0"
      },
      "source": [
        "# read files and create dataframe\r\n",
        "toxicity_comments = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)\r\n",
        "toxicity_annotations = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/toxicity_annotations.tsv',  sep = '\\t')\r\n",
        "toxicity_demographics = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/toxicity_worker_demographics.tsv', sep = '\\t')\r\n",
        "\r\n",
        "toxicity = toxicity_comments.merge(toxicity_annotations, how ='outer', on=\"rev_id\")\r\n",
        "toxicity = toxicity.merge(toxicity_demographics, how ='outer', on=\"worker_id\").sort_values(by=['rev_id','worker_id'])\r\n",
        "\r\n",
        "# remove newline and tab tokens\r\n",
        "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\r\n",
        "toxicity['comment'] = toxicity['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\r\n",
        "\r\n",
        "# add binary gender columns\r\n",
        "toxicity = toxicity[toxicity['gender']!='other']\r\n",
        "toxicity = pd.concat([toxicity, pd.get_dummies(toxicity.gender).rename(columns = \"{}_binary\".format)], axis = 1)\r\n",
        "\r\n",
        "# limit size of dataset for testing purposes\r\n",
        "very_toxic = toxicity[toxicity.toxicity_score == -2.0]\r\n",
        "toxic = toxicity[toxicity.toxicity_score == -1.0]\r\n",
        "neutral = toxicity[toxicity.toxicity_score == 0.0]\r\n",
        "nontoxic = toxicity[toxicity.toxicity_score == 1.0]\r\n",
        "healthy = toxicity[toxicity.toxicity_score == 2.0]\r\n",
        "\r\n",
        "female_neutral = neutral[neutral.female_binary == 1]\r\n",
        "male_neutral = neutral[neutral.male_binary == 1]\r\n",
        "\r\n",
        "# size = min(female_neutral.shape[0],male_neutral.shape[0])\r\n",
        "# print(size)\r\n",
        "size = 25000\r\n",
        "\r\n",
        "neutral_data = pd.concat([female_neutral.sample(size), male_neutral.sample(size)])\r\n",
        "neutral_data = neutral_data.sample(frac=1)\r\n",
        "print(neutral_data.shape[0])\r\n",
        "test_data = neutral_data.sample(frac = 0.2)\r\n",
        "print(test_data.shape[0])\r\n",
        "neutral_data = neutral_data.drop(test_data.index)\r\n",
        "\r\n",
        "print(neutral_data.shape[0])\r\n",
        "display (neutral_data.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n",
            "40000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rev_id</th>\n",
              "      <th>comment</th>\n",
              "      <th>year</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>ns</th>\n",
              "      <th>sample</th>\n",
              "      <th>split</th>\n",
              "      <th>worker_id</th>\n",
              "      <th>toxicity</th>\n",
              "      <th>toxicity_score</th>\n",
              "      <th>gender</th>\n",
              "      <th>english_first_language</th>\n",
              "      <th>age_group</th>\n",
              "      <th>education</th>\n",
              "      <th>female_binary</th>\n",
              "      <th>male_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>360332</th>\n",
              "      <td>1.38e+07</td>\n",
              "      <td>New comments at bottom, please. ==Complaints==</td>\n",
              "      <td>2005</td>\n",
              "      <td>False</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>2892</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18-30</td>\n",
              "      <td>masters</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631174</th>\n",
              "      <td>6.64e+08</td>\n",
              "      <td>:Thanks, I will watch for this in the future....</td>\n",
              "      <td>2015</td>\n",
              "      <td>True</td>\n",
              "      <td>user</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>1037</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45-60</td>\n",
              "      <td>hs</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1354949</th>\n",
              "      <td>8.10e+07</td>\n",
              "      <td>`  PLease do not add third party companies to ...</td>\n",
              "      <td>2006</td>\n",
              "      <td>True</td>\n",
              "      <td>user</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>1460</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45-60</td>\n",
              "      <td>masters</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366118</th>\n",
              "      <td>2.60e+08</td>\n",
              "      <td>== 1 more thing ==  I noticed plenty of gram...</td>\n",
              "      <td>2008</td>\n",
              "      <td>True</td>\n",
              "      <td>user</td>\n",
              "      <td>blocked</td>\n",
              "      <td>train</td>\n",
              "      <td>3439</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30-45</td>\n",
              "      <td>bachelors</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499681</th>\n",
              "      <td>7.64e+07</td>\n",
              "      <td>` :::::::First, trying to pull ``rank`` with m...</td>\n",
              "      <td>2006</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>blocked</td>\n",
              "      <td>dev</td>\n",
              "      <td>2608</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18-30</td>\n",
              "      <td>hs</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           rev_id  ... male_binary\n",
              "360332   1.38e+07  ...           0\n",
              "631174   6.64e+08  ...           0\n",
              "1354949  8.10e+07  ...           0\n",
              "366118   2.60e+08  ...           0\n",
              "1499681  7.64e+07  ...           1\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM4vJigyDP3j",
        "outputId": "b76d743c-338c-47c5-f345-45100ba629ce"
      },
      "source": [
        "# try to use gpu\r\n",
        "if torch.cuda.is_available():      \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5Q5ntvhDWxN"
      },
      "source": [
        "# extract relevant information\r\n",
        "comments = neutral_data.comment.values\r\n",
        "labels = neutral_data.female_binary.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhlZR6pXD1Jl",
        "outputId": "d27ae9d0-76c8-4c3f-f921-03b4bc3e90d6"
      },
      "source": [
        "! pip install transformers==3.5.1\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "# Load the BERT tokenizer.\r\n",
        "print('Loading BERT tokenizer...')\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.5.1 in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.1.91)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (20.8)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.9.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1) (51.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3gKFb5wD7eS",
        "outputId": "cdd1eee9-3fef-478e-9000-e6d0121a01db"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs.\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "# For every sentence...\r\n",
        "for cmt in comments:\r\n",
        "    # `encode_plus` will:\r\n",
        "    #   (1) Tokenize the sentence.\r\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "    #   (3) Append the `[SEP]` token to the end.\r\n",
        "    #   (4) Map tokens to their IDs.\r\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "    #   (6) Create attention masks for [PAD] tokens.\r\n",
        "    encoded_dict = tokenizer.encode_plus(\r\n",
        "                        cmt,                      # Sentence to encode.\r\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
        "                        max_length = 250,           # Pad & truncate all sentences.\r\n",
        "                        pad_to_max_length = True,\r\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\r\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\r\n",
        "                   )\r\n",
        "    \r\n",
        "    # Add the encoded sentence to the list.    \r\n",
        "    input_ids.append(encoded_dict['input_ids'])\r\n",
        "    \r\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "# Convert the lists into tensors.\r\n",
        "input_ids = torch.cat(input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "labels = torch.tensor(labels)\r\n",
        "\r\n",
        "# # Print sentence 0, now as a list of IDs.\r\n",
        "# print('Original: ', comments[0],scores[0])\r\n",
        "# print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning:\n",
            "\n",
            "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaaSOCAIEZq-",
        "outputId": "d05b921e-65e8-48aa-f138-a5bf1b28da0a"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\r\n",
        "\r\n",
        "# Combine the training inputs into a TensorDataset.\r\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "\r\n",
        "# Create a 90-10 train-validation split.\r\n",
        "\r\n",
        "# Calculate the number of samples to include in each set.\r\n",
        "train_size = int(0.9 * len(dataset))\r\n",
        "val_size = len(dataset) - train_size\r\n",
        "\r\n",
        "# Divide the dataset by randomly selecting samples.\r\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
        "\r\n",
        "print('{:>5,} training samples'.format(train_size))\r\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36,000 training samples\n",
            "4,000 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHf1KGY_GvSj"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \r\n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n",
        "# size of 16 or 32.\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "# We'll take training samples in random order. \r\n",
        "train_dataloader = DataLoader(\r\n",
        "            train_dataset,  # The training samples.\r\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\r\n",
        "            batch_size = batch_size # Trains with this batch size.\r\n",
        "        )\r\n",
        "\r\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\r\n",
        "validation_dataloader = DataLoader(\r\n",
        "            val_dataset, # The validation samples.\r\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n",
        "            batch_size = batch_size # Evaluate with this batch size\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z5LPc0pG265",
        "outputId": "847e76ae-f8a2-40fc-cc08-ce16058bb82e"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "\r\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \r\n",
        "# linear classification layer on top. \r\n",
        "model = BertForSequenceClassification.from_pretrained(\r\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\r\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\r\n",
        "                    # You can increase this for multi-class tasks.   \r\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\r\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
        ")\r\n",
        "\r\n",
        "# # Tell pytorch to run this model on the GPU.\r\n",
        "model.cuda()\r\n",
        "#model.cpu()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AksknLguH3My"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \r\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioeJ5LI8H5QS"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \r\n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\r\n",
        "# training data.\r\n",
        "epochs = 3\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "# (Note that this is not the same as the number of training samples).\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\r\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS__7IZPH-v7"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR6fseNsH__N"
      },
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBM-WqAWIDbS",
        "outputId": "5c678b25-3d1d-4be5-e16c-ce04aabf6e68"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "# This training code is based on the `run_glue.py` script here:\r\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\r\n",
        "\r\n",
        "# Set the seed value all over the place to make this reproducible.\r\n",
        "seed_val = 42\r\n",
        "\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# We'll store a number of quantities such as training and validation loss, \r\n",
        "# validation accuracy, and timings.\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "# Measure the total training time for the whole run.\r\n",
        "total_t0 = time.time()\r\n",
        "\r\n",
        "# For each epoch...\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    \r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    # Perform one full pass over the training set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    # Measure how long the training epoch takes.\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode. Don't be misled--the call to \r\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # For each batch of training data...\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 40 batches.\r\n",
        "        if step % 40 == 0 and not step == 0:\r\n",
        "            # Calculate elapsed time in minutes.\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "            # Report progress.\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "        # `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Always clear any previously calculated gradients before performing a\r\n",
        "        # backward pass. PyTorch doesn't do this automatically because \r\n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "        # The documentation for this `model` function is here: \r\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "        # It returns different numbers of parameters depending on what arguments\r\n",
        "        # arge given and what flags are set. For our useage here, it returns\r\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\r\n",
        "        # outputs prior to activation.\r\n",
        "        loss, logits = model(b_input_ids, \r\n",
        "                             token_type_ids=None, \r\n",
        "                             attention_mask=b_input_mask, \r\n",
        "                             labels=b_labels.long())\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_train_loss += loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass to calculate the gradients.\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters and take a step using the computed gradient.\r\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "        # modified based on their gradients, the learning rate, etc.\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    # Measure how long this epoch took.\r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # ========================================\r\n",
        "    #               Validation\r\n",
        "    # ========================================\r\n",
        "    # After the completion of each training epoch, measure our performance on\r\n",
        "    # our validation set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\r\n",
        "    # during evaluation.\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "    nb_eval_steps = 0\r\n",
        "\r\n",
        "    # Evaluate data for one epoch\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \r\n",
        "        # the `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "        \r\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\r\n",
        "        # the forward pass, since this is only needed for backprop (training).\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            # Forward pass, calculate logit predictions.\r\n",
        "            # token_type_ids is the same as the \"segment ids\", which \r\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
        "            # The documentation for this `model` function is here: \r\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
        "            # values prior to applying an activation function like the softmax.\r\n",
        "            (loss, logits) = model(b_input_ids, \r\n",
        "                                   token_type_ids=None, \r\n",
        "                                   attention_mask=b_input_mask,\r\n",
        "                                   labels=b_labels.long())\r\n",
        "            \r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += loss.item()\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "        # Calculate the accuracy for this batch of test sentences, and\r\n",
        "        # accumulate it over all batches.\r\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        \r\n",
        "\r\n",
        "    # Report the final accuracy for this validation run.\r\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "\r\n",
        "    # Calculate the average loss over all of the batches.\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    \r\n",
        "    # Measure how long the validation run took.\r\n",
        "    validation_time = format_time(time.time() - t0)\r\n",
        "    \r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Record all statistics from this epoch.\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Training complete!\")\r\n",
        "\r\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  2,250.    Elapsed: 0:00:27.\n",
            "  Batch    80  of  2,250.    Elapsed: 0:00:55.\n",
            "  Batch   120  of  2,250.    Elapsed: 0:01:23.\n",
            "  Batch   160  of  2,250.    Elapsed: 0:01:52.\n",
            "  Batch   200  of  2,250.    Elapsed: 0:02:21.\n",
            "  Batch   240  of  2,250.    Elapsed: 0:02:51.\n",
            "  Batch   280  of  2,250.    Elapsed: 0:03:20.\n",
            "  Batch   320  of  2,250.    Elapsed: 0:03:50.\n",
            "  Batch   360  of  2,250.    Elapsed: 0:04:20.\n",
            "  Batch   400  of  2,250.    Elapsed: 0:04:51.\n",
            "  Batch   440  of  2,250.    Elapsed: 0:05:21.\n",
            "  Batch   480  of  2,250.    Elapsed: 0:05:52.\n",
            "  Batch   520  of  2,250.    Elapsed: 0:06:22.\n",
            "  Batch   560  of  2,250.    Elapsed: 0:06:53.\n",
            "  Batch   600  of  2,250.    Elapsed: 0:07:24.\n",
            "  Batch   640  of  2,250.    Elapsed: 0:07:54.\n",
            "  Batch   680  of  2,250.    Elapsed: 0:08:25.\n",
            "  Batch   720  of  2,250.    Elapsed: 0:08:56.\n",
            "  Batch   760  of  2,250.    Elapsed: 0:09:27.\n",
            "  Batch   800  of  2,250.    Elapsed: 0:09:58.\n",
            "  Batch   840  of  2,250.    Elapsed: 0:10:28.\n",
            "  Batch   880  of  2,250.    Elapsed: 0:10:59.\n",
            "  Batch   920  of  2,250.    Elapsed: 0:11:30.\n",
            "  Batch   960  of  2,250.    Elapsed: 0:12:00.\n",
            "  Batch 1,000  of  2,250.    Elapsed: 0:12:31.\n",
            "  Batch 1,040  of  2,250.    Elapsed: 0:13:02.\n",
            "  Batch 1,080  of  2,250.    Elapsed: 0:13:32.\n",
            "  Batch 1,120  of  2,250.    Elapsed: 0:14:03.\n",
            "  Batch 1,160  of  2,250.    Elapsed: 0:14:34.\n",
            "  Batch 1,200  of  2,250.    Elapsed: 0:15:04.\n",
            "  Batch 1,240  of  2,250.    Elapsed: 0:15:35.\n",
            "  Batch 1,280  of  2,250.    Elapsed: 0:16:06.\n",
            "  Batch 1,320  of  2,250.    Elapsed: 0:16:37.\n",
            "  Batch 1,360  of  2,250.    Elapsed: 0:17:07.\n",
            "  Batch 1,400  of  2,250.    Elapsed: 0:17:38.\n",
            "  Batch 1,440  of  2,250.    Elapsed: 0:18:09.\n",
            "  Batch 1,480  of  2,250.    Elapsed: 0:18:39.\n",
            "  Batch 1,520  of  2,250.    Elapsed: 0:19:10.\n",
            "  Batch 1,560  of  2,250.    Elapsed: 0:19:41.\n",
            "  Batch 1,600  of  2,250.    Elapsed: 0:20:11.\n",
            "  Batch 1,640  of  2,250.    Elapsed: 0:20:42.\n",
            "  Batch 1,680  of  2,250.    Elapsed: 0:21:13.\n",
            "  Batch 1,720  of  2,250.    Elapsed: 0:21:43.\n",
            "  Batch 1,760  of  2,250.    Elapsed: 0:22:14.\n",
            "  Batch 1,800  of  2,250.    Elapsed: 0:22:44.\n",
            "  Batch 1,840  of  2,250.    Elapsed: 0:23:15.\n",
            "  Batch 1,880  of  2,250.    Elapsed: 0:23:46.\n",
            "  Batch 1,920  of  2,250.    Elapsed: 0:24:16.\n",
            "  Batch 1,960  of  2,250.    Elapsed: 0:24:47.\n",
            "  Batch 2,000  of  2,250.    Elapsed: 0:25:17.\n",
            "  Batch 2,040  of  2,250.    Elapsed: 0:25:48.\n",
            "  Batch 2,080  of  2,250.    Elapsed: 0:26:19.\n",
            "  Batch 2,120  of  2,250.    Elapsed: 0:26:49.\n",
            "  Batch 2,160  of  2,250.    Elapsed: 0:27:20.\n",
            "  Batch 2,200  of  2,250.    Elapsed: 0:27:51.\n",
            "  Batch 2,240  of  2,250.    Elapsed: 0:28:21.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0:28:29\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:01:10\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  2,250.    Elapsed: 0:00:30.\n",
            "  Batch    80  of  2,250.    Elapsed: 0:01:01.\n",
            "  Batch   120  of  2,250.    Elapsed: 0:01:32.\n",
            "  Batch   160  of  2,250.    Elapsed: 0:02:02.\n",
            "  Batch   200  of  2,250.    Elapsed: 0:02:33.\n",
            "  Batch   240  of  2,250.    Elapsed: 0:03:04.\n",
            "  Batch   280  of  2,250.    Elapsed: 0:03:34.\n",
            "  Batch   320  of  2,250.    Elapsed: 0:04:05.\n",
            "  Batch   360  of  2,250.    Elapsed: 0:04:36.\n",
            "  Batch   400  of  2,250.    Elapsed: 0:05:06.\n",
            "  Batch   440  of  2,250.    Elapsed: 0:05:36.\n",
            "  Batch   480  of  2,250.    Elapsed: 0:06:07.\n",
            "  Batch   520  of  2,250.    Elapsed: 0:06:37.\n",
            "  Batch   560  of  2,250.    Elapsed: 0:07:08.\n",
            "  Batch   600  of  2,250.    Elapsed: 0:07:38.\n",
            "  Batch   640  of  2,250.    Elapsed: 0:08:09.\n",
            "  Batch   680  of  2,250.    Elapsed: 0:08:40.\n",
            "  Batch   720  of  2,250.    Elapsed: 0:09:10.\n",
            "  Batch   760  of  2,250.    Elapsed: 0:09:41.\n",
            "  Batch   800  of  2,250.    Elapsed: 0:10:11.\n",
            "  Batch   840  of  2,250.    Elapsed: 0:10:42.\n",
            "  Batch   880  of  2,250.    Elapsed: 0:11:13.\n",
            "  Batch   920  of  2,250.    Elapsed: 0:11:43.\n",
            "  Batch   960  of  2,250.    Elapsed: 0:12:14.\n",
            "  Batch 1,000  of  2,250.    Elapsed: 0:12:45.\n",
            "  Batch 1,040  of  2,250.    Elapsed: 0:13:15.\n",
            "  Batch 1,080  of  2,250.    Elapsed: 0:13:46.\n",
            "  Batch 1,120  of  2,250.    Elapsed: 0:14:16.\n",
            "  Batch 1,160  of  2,250.    Elapsed: 0:14:47.\n",
            "  Batch 1,200  of  2,250.    Elapsed: 0:15:18.\n",
            "  Batch 1,240  of  2,250.    Elapsed: 0:15:48.\n",
            "  Batch 1,280  of  2,250.    Elapsed: 0:16:19.\n",
            "  Batch 1,320  of  2,250.    Elapsed: 0:16:49.\n",
            "  Batch 1,360  of  2,250.    Elapsed: 0:17:20.\n",
            "  Batch 1,400  of  2,250.    Elapsed: 0:17:51.\n",
            "  Batch 1,440  of  2,250.    Elapsed: 0:18:21.\n",
            "  Batch 1,480  of  2,250.    Elapsed: 0:18:52.\n",
            "  Batch 1,520  of  2,250.    Elapsed: 0:19:23.\n",
            "  Batch 1,560  of  2,250.    Elapsed: 0:19:53.\n",
            "  Batch 1,600  of  2,250.    Elapsed: 0:20:24.\n",
            "  Batch 1,640  of  2,250.    Elapsed: 0:20:55.\n",
            "  Batch 1,680  of  2,250.    Elapsed: 0:21:25.\n",
            "  Batch 1,720  of  2,250.    Elapsed: 0:21:56.\n",
            "  Batch 1,760  of  2,250.    Elapsed: 0:22:27.\n",
            "  Batch 1,800  of  2,250.    Elapsed: 0:22:58.\n",
            "  Batch 1,840  of  2,250.    Elapsed: 0:23:28.\n",
            "  Batch 1,880  of  2,250.    Elapsed: 0:23:59.\n",
            "  Batch 1,920  of  2,250.    Elapsed: 0:24:29.\n",
            "  Batch 1,960  of  2,250.    Elapsed: 0:25:00.\n",
            "  Batch 2,000  of  2,250.    Elapsed: 0:25:31.\n",
            "  Batch 2,040  of  2,250.    Elapsed: 0:26:02.\n",
            "  Batch 2,080  of  2,250.    Elapsed: 0:26:33.\n",
            "  Batch 2,120  of  2,250.    Elapsed: 0:27:03.\n",
            "  Batch 2,160  of  2,250.    Elapsed: 0:27:34.\n",
            "  Batch 2,200  of  2,250.    Elapsed: 0:28:04.\n",
            "  Batch 2,240  of  2,250.    Elapsed: 0:28:35.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0:28:43\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:01:11\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  2,250.    Elapsed: 0:00:31.\n",
            "  Batch    80  of  2,250.    Elapsed: 0:01:02.\n",
            "  Batch   120  of  2,250.    Elapsed: 0:01:32.\n",
            "  Batch   160  of  2,250.    Elapsed: 0:02:03.\n",
            "  Batch   200  of  2,250.    Elapsed: 0:02:34.\n",
            "  Batch   240  of  2,250.    Elapsed: 0:03:04.\n",
            "  Batch   280  of  2,250.    Elapsed: 0:03:35.\n",
            "  Batch   320  of  2,250.    Elapsed: 0:04:06.\n",
            "  Batch   360  of  2,250.    Elapsed: 0:04:36.\n",
            "  Batch   400  of  2,250.    Elapsed: 0:05:07.\n",
            "  Batch   440  of  2,250.    Elapsed: 0:05:38.\n",
            "  Batch   480  of  2,250.    Elapsed: 0:06:08.\n",
            "  Batch   520  of  2,250.    Elapsed: 0:06:39.\n",
            "  Batch   560  of  2,250.    Elapsed: 0:07:10.\n",
            "  Batch   600  of  2,250.    Elapsed: 0:07:41.\n",
            "  Batch   640  of  2,250.    Elapsed: 0:08:11.\n",
            "  Batch   680  of  2,250.    Elapsed: 0:08:42.\n",
            "  Batch   720  of  2,250.    Elapsed: 0:09:13.\n",
            "  Batch   760  of  2,250.    Elapsed: 0:09:43.\n",
            "  Batch   800  of  2,250.    Elapsed: 0:10:14.\n",
            "  Batch   840  of  2,250.    Elapsed: 0:10:45.\n",
            "  Batch   880  of  2,250.    Elapsed: 0:11:15.\n",
            "  Batch   920  of  2,250.    Elapsed: 0:11:46.\n",
            "  Batch   960  of  2,250.    Elapsed: 0:12:17.\n",
            "  Batch 1,000  of  2,250.    Elapsed: 0:12:47.\n",
            "  Batch 1,040  of  2,250.    Elapsed: 0:13:18.\n",
            "  Batch 1,080  of  2,250.    Elapsed: 0:13:49.\n",
            "  Batch 1,120  of  2,250.    Elapsed: 0:14:19.\n",
            "  Batch 1,160  of  2,250.    Elapsed: 0:14:50.\n",
            "  Batch 1,200  of  2,250.    Elapsed: 0:15:21.\n",
            "  Batch 1,240  of  2,250.    Elapsed: 0:15:51.\n",
            "  Batch 1,280  of  2,250.    Elapsed: 0:16:22.\n",
            "  Batch 1,320  of  2,250.    Elapsed: 0:16:53.\n",
            "  Batch 1,360  of  2,250.    Elapsed: 0:17:23.\n",
            "  Batch 1,400  of  2,250.    Elapsed: 0:17:54.\n",
            "  Batch 1,440  of  2,250.    Elapsed: 0:18:24.\n",
            "  Batch 1,480  of  2,250.    Elapsed: 0:18:55.\n",
            "  Batch 1,520  of  2,250.    Elapsed: 0:19:26.\n",
            "  Batch 1,560  of  2,250.    Elapsed: 0:19:56.\n",
            "  Batch 1,600  of  2,250.    Elapsed: 0:20:27.\n",
            "  Batch 1,640  of  2,250.    Elapsed: 0:20:58.\n",
            "  Batch 1,680  of  2,250.    Elapsed: 0:21:28.\n",
            "  Batch 1,720  of  2,250.    Elapsed: 0:21:59.\n",
            "  Batch 1,760  of  2,250.    Elapsed: 0:22:30.\n",
            "  Batch 1,800  of  2,250.    Elapsed: 0:23:00.\n",
            "  Batch 1,840  of  2,250.    Elapsed: 0:23:31.\n",
            "  Batch 1,880  of  2,250.    Elapsed: 0:24:02.\n",
            "  Batch 1,920  of  2,250.    Elapsed: 0:24:32.\n",
            "  Batch 1,960  of  2,250.    Elapsed: 0:25:03.\n",
            "  Batch 2,000  of  2,250.    Elapsed: 0:25:34.\n",
            "  Batch 2,040  of  2,250.    Elapsed: 0:26:04.\n",
            "  Batch 2,080  of  2,250.    Elapsed: 0:26:35.\n",
            "  Batch 2,120  of  2,250.    Elapsed: 0:27:06.\n",
            "  Batch 2,160  of  2,250.    Elapsed: 0:27:37.\n",
            "  Batch 2,200  of  2,250.    Elapsed: 0:28:07.\n",
            "  Batch 2,240  of  2,250.    Elapsed: 0:28:38.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0:28:46\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.51\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:01:10\n",
            "\n",
            "Training complete!\n",
            "Total training took 1:29:28 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyRJP4p6IONm",
        "outputId": "ccab7856-0622-44aa-b92d-9e1e80330e38"
      },
      "source": [
        "# Display floats with two decimal places.\r\n",
        "pd.set_option('precision', 2)\r\n",
        "\r\n",
        "# Create a DataFrame from our training statistics.\r\n",
        "df_stats = pd.DataFrame(data=training_stats)\r\n",
        "\r\n",
        "# Use the 'epoch' as the row index.\r\n",
        "df_stats = df_stats.set_index('epoch')\r\n",
        "\r\n",
        "# A hack to force the column headers to wrap.\r\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\r\n",
        "\r\n",
        "# Display the table.\r\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0:28:29</td>\n",
              "      <td>0:01:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0:28:43</td>\n",
              "      <td>0:01:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.69</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0:28:46</td>\n",
              "      <td>0:01:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.70         0.69           0.50       0:28:29         0:01:10\n",
              "2               0.69         0.69           0.50       0:28:43         0:01:11\n",
              "3               0.69         0.69           0.51       0:28:46         0:01:10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "PCXAxB6tIXY5",
        "outputId": "f61b49e0-96df-4f72-bf23-1bdd7162a4dd"
      },
      "source": [
        "# create test set\r\n",
        "\r\n",
        "correct_female_pred = []\r\n",
        "correct_male_pred = []\r\n",
        "accuracies = []\r\n",
        "for i in range(2):\r\n",
        "  if i == 0:\r\n",
        "    df = test_data[test_data.female_binary==1]\r\n",
        "    print('testing on female')\r\n",
        "  else:\r\n",
        "    df = test_data[test_data.female_binary==0]\r\n",
        "    print('testing on male')\r\n",
        "\r\n",
        "  df = df.sample(frac=1)\r\n",
        "  print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\r\n",
        "\r\n",
        "  comments = df.comment.values\r\n",
        "  labels = df.female_binary.values\r\n",
        "\r\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
        "  input_ids = []\r\n",
        "  attention_masks = []\r\n",
        "\r\n",
        "  # For every sentence...\r\n",
        "  for cmt in comments:\r\n",
        "      # `encode_plus` will:\r\n",
        "      #   (1) Tokenize the sentence.\r\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "      #   (3) Append the `[SEP]` token to the end.\r\n",
        "      #   (4) Map tokens to their IDs.\r\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "      #   (6) Create attention masks for [PAD] tokens.\r\n",
        "      encoded_dict = tokenizer.encode_plus(\r\n",
        "                          cmt,                      # Sentence to encode.\r\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
        "                          max_length = 250,           # Pad & truncate all sentences.\r\n",
        "                          pad_to_max_length = True,\r\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\r\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\r\n",
        "                    )\r\n",
        "      \r\n",
        "      # Add the encoded sentence to the list.    \r\n",
        "      input_ids.append(encoded_dict['input_ids'])\r\n",
        "      \r\n",
        "      # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "  # Convert the lists into tensors.\r\n",
        "  input_ids = torch.cat(input_ids, dim=0)\r\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "  labels = torch.tensor(labels)\r\n",
        "\r\n",
        "  # Set the batch size.  \r\n",
        "  batch_size = 16\r\n",
        "\r\n",
        "  # Create the DataLoader.\r\n",
        "  prediction_data = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\r\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "  # Prediction on test set\r\n",
        "\r\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\r\n",
        "\r\n",
        "  # Put model in evaluation mode\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  # Tracking variables \r\n",
        "  predictions , true_labels = [], []\r\n",
        "\r\n",
        "  # Predict \r\n",
        "  for batch in prediction_dataloader:\r\n",
        "    # Add batch to GPU\r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "    \r\n",
        "    # Unpack the inputs from our dataloader\r\n",
        "    b_input_ids, b_input_mask, b_labels = batch\r\n",
        "    \r\n",
        "    # Telling the model not to compute or store gradients, saving memory and \r\n",
        "    # speeding up prediction\r\n",
        "    with torch.no_grad():\r\n",
        "        # Forward pass, calculate logit predictions\r\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \r\n",
        "                        attention_mask=b_input_mask)\r\n",
        "\r\n",
        "    logits = outputs[0]\r\n",
        "\r\n",
        "    # Move logits and labels to CPU\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    label_ids = b_labels.to('cpu').numpy()\r\n",
        "    \r\n",
        "    # Store predictions and true labels\r\n",
        "    predictions.append(logits)\r\n",
        "    true_labels.append(label_ids)\r\n",
        "\r\n",
        "  print('    DONE.')\r\n",
        "\r\n",
        "  from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\r\n",
        "  import seaborn as sns\r\n",
        "  import matplotlib.pyplot as plt\r\n",
        "  \r\n",
        "  # Combine the results across all batches. \r\n",
        "  flat_pred = np.concatenate(predictions, axis=0)\r\n",
        "  # For each sample, pick the label (0 or 1) with the higher score.\r\n",
        "  flat_predictions = np.argmax(flat_pred, axis=1).flatten()\r\n",
        "\r\n",
        "  # Combine the correct labels for each batch into a single list.\r\n",
        "  flat_true_labels = np.concatenate(true_labels, axis=0)\r\n",
        "\r\n",
        "  for i in range(len(flat_true_labels)):\r\n",
        "    if flat_true_labels[i]==flat_predictions[i]:\r\n",
        "      if flat_true_labels[i] == 1:\r\n",
        "        correct_female_pred.append(tuple(flat_pred[i]))\r\n",
        "      else:\r\n",
        "        correct_male_pred.append(tuple(flat_pred[i]))\r\n",
        "\r\n",
        "  f1 = f1_score(flat_true_labels, flat_predictions)\r\n",
        "  print('Total F1: %.3f' % f1)\r\n",
        "  acc = accuracy_score(flat_true_labels,flat_predictions)\r\n",
        "  print('Accuracy: %.2f' % acc)\r\n",
        "  accuracies.append(acc)\r\n",
        "\r\n",
        "  cf_matrix = confusion_matrix(flat_true_labels, flat_predictions)\r\n",
        "  group_names = ['True Neg','False Pos','False Neg','True Pos']\r\n",
        "  group_counts = ['{0:0.0f}'.format(value) for value in\r\n",
        "                  cf_matrix.flatten()]\r\n",
        "  group_percentages = ['{0:.2%}'.format(value) for value in\r\n",
        "                      cf_matrix.flatten()/np.sum(cf_matrix)]\r\n",
        "  labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\r\n",
        "            zip(group_names,group_counts,group_percentages)]\r\n",
        "  try:\r\n",
        "    labels = np.asarray(labels).reshape(2,2)\r\n",
        "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\r\n",
        "    plt.show()\r\n",
        "  except ValueError:\r\n",
        "    print(\"could not display\")\r\n",
        "\r\n",
        "  # f1_set = []\r\n",
        "\r\n",
        "  # # Evaluate each test batch\r\n",
        "  # print('Calculating and F1 score for each batch...')\r\n",
        "\r\n",
        "  # # For each input batch...\r\n",
        "  # for i in range(len(true_labels)):\r\n",
        "    \r\n",
        "  #   # The predictions for this batch are a 2-column ndarray (one column for \"0\" \r\n",
        "  #   # and one column for \"1\"). Pick the label with the highest value and turn this\r\n",
        "  #   # in to a list of 0s and 1s.\r\n",
        "  #   pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\r\n",
        "    \r\n",
        "  #   # Calculate and store the coef for this batch.  \r\n",
        "  #   score = f1_score(true_labels[i], pred_labels_i)\r\n",
        "  #   f1_set.append(score)\r\n",
        "\r\n",
        "  # # Create a barplot showing the f1 score for each batch of test samples.\r\n",
        "  # ax = sns.barplot(x=list(range(len(f1_set))), y=f1_set, ci=None)\r\n",
        "\r\n",
        "  # plt.title('f1 Score per Batch')\r\n",
        "  # plt.ylabel('f1 Score (0 to +1)')\r\n",
        "  # plt.xlabel('Batch #')\r\n",
        "\r\n",
        "  # plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing on female\n",
            "Number of test sentences: 4,986\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning:\n",
            "\n",
            "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 4,986 test sentences...\n",
            "    DONE.\n",
            "Total F1: 0.337\n",
            "Accuracy: 0.20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "testing on male\n",
            "Number of test sentences: 5,014\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning:\n",
            "\n",
            "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 5,014 test sentences...\n",
            "    DONE.\n",
            "Total F1: 0.000\n",
            "Accuracy: 0.81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qlqP9YN4Jv6S",
        "outputId": "c47d56a8-892d-4c3b-8e1d-b607d30365cf"
      },
      "source": [
        "import plotly.graph_objects as go\r\n",
        "\r\n",
        "unzipped_object = zip(*correct_female_pred)\r\n",
        "unzipped_list = list(unzipped_object)\r\n",
        "\r\n",
        "fig = go.Figure(data=go.Scatter(x=unzipped_list[1],\r\n",
        "                                y=unzipped_list[0],\r\n",
        "                                mode='markers'))\r\n",
        "\r\n",
        "fig.update_layout(title='Likelihood of genders for true positive (female)',\r\n",
        "                  xaxis_title=\"Likelihood female\",\r\n",
        "                  yaxis_title=\"Likelihood male\")\r\n",
        "fig.show()\r\n",
        "\r\n",
        "unzipped_object = zip(*correct_male_pred)\r\n",
        "unzipped_list = list(unzipped_object)\r\n",
        "\r\n",
        "fig = go.Figure(data=go.Scatter(x=unzipped_list[0],\r\n",
        "                                y=unzipped_list[1],\r\n",
        "                                mode='markers'))\r\n",
        "\r\n",
        "fig.update_layout(title='Likelihood of genders for true negative (male)',\r\n",
        "                  xaxis_title=\"Likelihood male\",\r\n",
        "                  yaxis_title=\"Likelihood female\")\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"a3bb4752-9f54-4ec5-977d-fe84ea42e39a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"a3bb4752-9f54-4ec5-977d-fe84ea42e39a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'a3bb4752-9f54-4ec5-977d-fe84ea42e39a',\n",
              "                        [{\"mode\": \"markers\", \"type\": \"scatter\", \"x\": [0.11104227602481842, 0.10409791767597198, 0.10875833034515381, 0.1150292232632637, 0.09816724061965942, 0.09094295650720596, 0.1035795733332634, 0.08918680250644684, 0.10793716460466385, 0.09511387348175049, 0.11971602588891983, 0.11182853579521179, 0.10791146755218506, 0.10698544979095459, 0.1079227477312088, 0.10829703509807587, 0.10470794141292572, 0.09941573441028595, 0.1050269603729248, 0.08894345164299011, 0.08874092996120453, 0.10312025994062424, 0.10837934911251068, 0.10562804341316223, 0.11155195534229279, 0.10326266288757324, 0.10868959128856659, 0.10584806650876999, 0.11068405210971832, 0.09114563465118408, 0.10347756743431091, 0.09634082019329071, 0.09067060053348541, 0.0916827991604805, 0.10948517918586731, 0.10711447149515152, 0.11337217688560486, 0.09883932769298553, 0.09521132707595825, 0.10738851875066757, 0.11054375022649765, 0.09716478735208511, 0.08704903721809387, 0.08839024603366852, 0.10359065979719162, 0.09473657608032227, 0.08052118122577667, 0.10616007447242737, 0.09587693959474564, 0.10829329490661621, 0.10760228335857391, 0.08207637071609497, 0.09613333642482758, 0.08363772183656693, 0.09849514067173004, 0.09332304447889328, 0.09545286744832993, 0.09447206556797028, 0.08443667739629745, 0.10975325852632523, 0.1025398001074791, 0.1001625508069992, 0.10188203305006027, 0.0985928326845169, 0.10194343328475952, 0.11282537877559662, 0.10363902151584625, 0.09761472046375275, 0.11266079545021057, 0.09287939965724945, 0.09928949177265167, 0.08952996879816055, 0.08770446479320526, 0.09288878738880157, 0.09973374009132385, 0.1073230728507042, 0.11408919095993042, 0.09799356013536453, 0.09214553982019424, 0.10475632548332214, 0.11148657649755478, 0.09768248349428177, 0.1081673726439476, 0.10419535636901855, 0.09673569351434708, 0.10409695655107498, 0.1034766361117363, 0.0960223451256752, 0.10768342763185501, 0.10224469751119614, 0.10642150789499283, 0.09634929895401001, 0.09032203257083893, 0.09972922503948212, 0.10186892002820969, 0.10568439960479736, 0.10804356634616852, 0.09873023629188538, 0.1074519157409668, 0.09995493292808533, 0.09719500690698624, 0.09681238979101181, 0.10393102467060089, 0.10399343073368073, 0.10440514981746674, 0.10771441459655762, 0.09764690697193146, 0.097403883934021, 0.10363728553056717, 0.10109502822160721, 0.1047329232096672, 0.09733840823173523, 0.10084974765777588, 0.09252192080020905, 0.09418387711048126, 0.09684400260448456, 0.0842915028333664, 0.10674168914556503, 0.10610989481210709, 0.09646804630756378, 0.10888425260782242, 0.10019925236701965, 0.10010503977537155, 0.10571801662445068, 0.10352109372615814, 0.09815433621406555, 0.09107036143541336, 0.09209034591913223, 0.1116781234741211, 0.1114363744854927, 0.11667878925800323, 0.10124175995588303, 0.11330215632915497, 0.10097542405128479, 0.09532471746206284, 0.10476881265640259, 0.1067553237080574, 0.09792645275592804, 0.10812141001224518, 0.09239739924669266, 0.0947413370013237, 0.10408664494752884, 0.09670371562242508, 0.10244724154472351, 0.11403628438711166, 0.11066987365484238, 0.10772885382175446, 0.11014202982187271, 0.10732018202543259, 0.09678458422422409, 0.1024254560470581, 0.1042790487408638, 0.08910021930932999, 0.09572439640760422, 0.10552714765071869, 0.09291016310453415, 0.09809426218271255, 0.09403220564126968, 0.10735765099525452, 0.0993412658572197, 0.1093246266245842, 0.10772792249917984, 0.10383667796850204, 0.10333390533924103, 0.108977310359478, 0.11399571597576141, 0.10697329789400101, 0.10500427335500717, 0.09513713419437408, 0.1021319180727005, 0.11077745258808136, 0.10702550411224365, 0.10107774287462234, 0.10016189515590668, 0.11201443523168564, 0.10829935222864151, 0.08945689350366592, 0.10247825086116791, 0.09284640103578568, 0.10221284627914429, 0.08931457251310349, 0.09162621945142746, 0.09740239381790161, 0.10756503790616989, 0.10827198624610901, 0.10476668179035187, 0.10169213265180588, 0.09858296066522598, 0.09659691900014877, 0.11218848824501038, 0.10125933587551117, 0.10760550945997238, 0.09879957139492035, 0.10660144686698914, 0.10923397541046143, 0.09865179657936096, 0.10447926819324493, 0.09710309654474258, 0.10821670293807983, 0.10347457975149155, 0.08671506494283676, 0.10079778730869293, 0.1043398454785347, 0.10682880878448486, 0.08969588577747345, 0.1081378310918808, 0.10708468407392502, 0.10507778078317642, 0.09644129127264023, 0.09903199225664139, 0.09692282974720001, 0.10479698330163956, 0.09956525266170502, 0.09644283354282379, 0.10439111292362213, 0.1117323562502861, 0.09617790579795837, 0.10576595366001129, 0.10369479656219482, 0.10017772763967514, 0.09358132630586624, 0.09332304447889328, 0.10901857167482376, 0.10640719532966614, 0.10529831051826477, 0.09626873582601547, 0.09776518493890762, 0.10813964903354645, 0.10081618279218674, 0.1052909716963768, 0.09358132630586624, 0.09427262842655182, 0.10147424042224884, 0.10624286532402039, 0.09484907984733582, 0.0992753803730011, 0.09595873951911926, 0.1154707595705986, 0.09672783315181732, 0.11289548128843307, 0.09754519164562225, 0.10514488071203232, 0.0875861644744873, 0.10227569192647934, 0.09429336339235306, 0.10822157561779022, 0.10712248831987381, 0.10381506383419037, 0.09793781489133835, 0.11122739315032959, 0.09388182312250137, 0.11047814041376114, 0.1161821261048317, 0.11328168213367462, 0.11355620622634888, 0.09883362054824829, 0.09681424498558044, 0.10519994795322418, 0.08316127210855484, 0.08582843840122223, 0.08248016238212585, 0.11323510855436325, 0.07474607229232788, 0.10344423353672028, 0.10764284431934357, 0.1082875058054924, 0.10050831735134125, 0.10078424215316772, 0.11035792529582977, 0.09150639176368713, 0.10006611049175262, 0.0940205454826355, 0.09778532385826111, 0.09185037761926651, 0.09010647237300873, 0.09961619973182678, 0.11343032121658325, 0.10774533450603485, 0.10976879298686981, 0.09484907984733582, 0.09955819696187973, 0.10264256596565247, 0.0933169424533844, 0.10393571853637695, 0.10301046073436737, 0.0985807403922081, 0.103731170296669, 0.10608470439910889, 0.10701724886894226, 0.10432840883731842, 0.11209242045879364, 0.09058842062950134, 0.10599332302808762, 0.10578304529190063, 0.09193027019500732, 0.10550198704004288, 0.10483689606189728, 0.09626541286706924, 0.09041747450828552, 0.07816749811172485, 0.10214158892631531, 0.10618530213832855, 0.11083407700061798, 0.08637917786836624, 0.11494027078151703, 0.10242049396038055, 0.0948442816734314, 0.11129965633153915, 0.11065168678760529, 0.10564709454774857, 0.10857672989368439, 0.08743800222873688, 0.10375416278839111, 0.08798609673976898, 0.08640557527542114, 0.0936591699719429, 0.10079200565814972, 0.11110937595367432, 0.11107472330331802, 0.08679775893688202, 0.09173816442489624, 0.09718170017004013, 0.10701432824134827, 0.0949268639087677, 0.10324251651763916, 0.10796242207288742, 0.10340692102909088, 0.09287828952074051, 0.09926792234182358, 0.09562528133392334, 0.09757746011018753, 0.11081115901470184, 0.09400506317615509, 0.10548952966928482, 0.09946010261774063, 0.10811852663755417, 0.10946948826313019, 0.10451707988977432, 0.10924576222896576, 0.10059189051389694, 0.09708105772733688, 0.09791582077741623, 0.10431486368179321, 0.09532687067985535, 0.09256431460380554, 0.1081327274441719, 0.0994674414396286, 0.09974895417690277, 0.09648632258176804, 0.0988946408033371, 0.10066571086645126, 0.0980018898844719, 0.10432785749435425, 0.09980840981006622, 0.10227569192647934, 0.10555070638656616, 0.08817197382450104, 0.11514277756214142, 0.09864920377731323, 0.11566996574401855, 0.10466183722019196, 0.10868072509765625, 0.09789397567510605, 0.08699136972427368, 0.09941767901182175, 0.09011533856391907, 0.1033041849732399, 0.1070985198020935, 0.10012299567461014, 0.09982812404632568, 0.11390632390975952, 0.10616075992584229, 0.10942992568016052, 0.10480685532093048, 0.1102263480424881, 0.10141819715499878, 0.11126896739006042, 0.10252358019351959, 0.10902298241853714, 0.10311705619096756, 0.10280033946037292, 0.10275817662477493, 0.09783123433589935, 0.10000655800104141, 0.09325876086950302, 0.1032770425081253, 0.10349950939416885, 0.09276074171066284, 0.1087866872549057, 0.0953822210431099, 0.11137239634990692, 0.10803729295730591, 0.10760475695133209, 0.10427355766296387, 0.10501313209533691, 0.10520397126674652, 0.09592883288860321, 0.11439580470323563, 0.10644201934337616, 0.10407392680644989, 0.10869593918323517, 0.0848986804485321, 0.10751791298389435, 0.08801473677158356, 0.10648182034492493, 0.09410203993320465, 0.09783715009689331, 0.10358719527721405, 0.08831629902124405, 0.09162607043981552, 0.09121786057949066, 0.10217489302158356, 0.11110937595367432, 0.10166732966899872, 0.10733341425657272, 0.11029671132564545, 0.11148519068956375, 0.10819005221128464, 0.10520147532224655, 0.09675371646881104, 0.09508633613586426, 0.09616455435752869, 0.08954928815364838, 0.0955502837896347, 0.09826649725437164, 0.09406467527151108, 0.09895755350589752, 0.09773518145084381, 0.10140904784202576, 0.10660583525896072, 0.10769057273864746, 0.10917714238166809, 0.09508666396141052, 0.10385593771934509, 0.1063966378569603, 0.10021034628152847, 0.11090968549251556, 0.10880912095308304, 0.09224113821983337, 0.10874725878238678, 0.10522555559873581, 0.08843828737735748, 0.10755272954702377, 0.10887245833873749, 0.10545648634433746, 0.10080214589834213, 0.10107118636369705, 0.10420653223991394, 0.09686153382062912, 0.10581602156162262, 0.09288836270570755, 0.11085246503353119, 0.10499324649572372, 0.0953025221824646, 0.11042942851781845, 0.09169887006282806, 0.11244964599609375, 0.1109066754579544, 0.06728053838014603, 0.10923852026462555, 0.10286327451467514, 0.09648632258176804, 0.11036770045757294, 0.09429104626178741, 0.09695649147033691, 0.10035045444965363, 0.11026156693696976, 0.09748201072216034, 0.08916125446557999, 0.11374271661043167, 0.09500519186258316, 0.10628466308116913, 0.10625545680522919, 0.10643568634986877, 0.09941767901182175, 0.0968891903758049, 0.11127442866563797, 0.09745572507381439, 0.10238727182149887, 0.10596747696399689, 0.09786170721054077, 0.11471779644489288, 0.08784867823123932, 0.09950073063373566, 0.09756144136190414, 0.10851812362670898, 0.0965716540813446, 0.10803568363189697, 0.10844844579696655, 0.11233431845903397, 0.09992455691099167, 0.11242561787366867, 0.11583434045314789, 0.10944895446300507, 0.11349298059940338, 0.09916254132986069, 0.10194660723209381, 0.09657501429319382, 0.11089908331632614, 0.10151227563619614, 0.0970112755894661, 0.10659240186214447, 0.1107368990778923, 0.10569620132446289, 0.11118091642856598, 0.09218576550483704, 0.09340278059244156, 0.09824264049530029, 0.09560228884220123, 0.09761315584182739, 0.09846814721822739, 0.10739053040742874, 0.0944773405790329, 0.08704903721809387, 0.08898399025201797, 0.09376396983861923, 0.09723494946956635, 0.10226695239543915, 0.10395065695047379, 0.0969776064157486, 0.09428408741950989, 0.10472678393125534, 0.09975381940603256, 0.11181692779064178, 0.1088835820555687, 0.08941031247377396, 0.10780312865972519, 0.08176694810390472, 0.10999327898025513, 0.10316865146160126, 0.10047847032546997, 0.09956195205450058, 0.10787077993154526, 0.10830186307430267, 0.09334704279899597, 0.1100890040397644, 0.10851969569921494, 0.09303340315818787, 0.09437832981348038, 0.10070706903934479, 0.09654639661312103, 0.09006601572036743, 0.10029356181621552, 0.11136001348495483, 0.0917053073644638, 0.09885105490684509, 0.11757218837738037, 0.10409567505121231, 0.09377741068601608, 0.09552590548992157, 0.1078706830739975, 0.10153037309646606, 0.10738150775432587, 0.11494027078151703, 0.11566321551799774, 0.08414197713136673, 0.10707321763038635, 0.11368528008460999, 0.11100052297115326, 0.10010839998722076, 0.10803661495447159, 0.09644201397895813, 0.11111079156398773, 0.10334834456443787, 0.0980299860239029, 0.1101069301366806, 0.10429053008556366, 0.09967558085918427, 0.09306688606739044, 0.1078479066491127, 0.10185588896274567, 0.10009726881980896, 0.1021619439125061, 0.09361136704683304, 0.09880667924880981, 0.09499306976795197, 0.09748201072216034, 0.09376293420791626, 0.10566534847021103, 0.09888652712106705, 0.114112988114357, 0.10887912660837173, 0.10182791203260422, 0.11444389820098877, 0.10738851875066757, 0.10657373070716858, 0.10847073793411255, 0.10900112986564636, 0.10133683681488037, 0.10452163219451904, 0.10992483794689178, 0.10131305456161499, 0.09043366461992264, 0.09079332649707794, 0.11212242394685745, 0.1105625182390213, 0.09658732265233994, 0.1104615330696106, 0.08307544887065887, 0.09249100089073181, 0.09376263618469238, 0.08720002323389053, 0.09627711027860641, 0.09519247710704803, 0.10778938978910446, 0.09470346570014954, 0.09784451872110367, 0.10123826563358307, 0.1091197058558464, 0.09941690415143967, 0.08291732519865036, 0.09827354550361633, 0.09537176787853241, 0.10698419064283371, 0.1020488440990448, 0.10470794141292572, 0.10614168643951416, 0.10954080522060394, 0.10022827982902527, 0.09400506317615509, 0.10996180027723312, 0.10174264013767242, 0.09283020347356796, 0.0960850864648819, 0.10622800141572952, 0.09577789902687073, 0.1140650063753128, 0.10068198293447495, 0.08320106565952301, 0.10238003730773926, 0.09331665933132172, 0.08808324486017227, 0.10043185949325562, 0.09789504110813141, 0.0892295092344284, 0.11157188564538956, 0.100615493953228, 0.08569872379302979, 0.10864826291799545, 0.08387113362550735, 0.10318015515804291, 0.11231528222560883, 0.08911478519439697, 0.1012563556432724, 0.09711159020662308, 0.09726287424564362, 0.10791610926389694, 0.10674703121185303, 0.1132689118385315, 0.09261643141508102, 0.10862839967012405, 0.10657879710197449, 0.09652449190616608, 0.1018461138010025, 0.0997527465224266, 0.09785836935043335, 0.0946321040391922, 0.09689059853553772, 0.10058140009641647, 0.0962882936000824, 0.09407983720302582, 0.09276142716407776, 0.1105191558599472, 0.09764739871025085, 0.11253947764635086, 0.09417146444320679, 0.09031979739665985, 0.09793781489133835, 0.09142494201660156, 0.09560965746641159, 0.11672929674386978, 0.11393813788890839, 0.10209184885025024, 0.0925590842962265, 0.11222448945045471, 0.08274668455123901, 0.09712550789117813, 0.10230302065610886, 0.10712100565433502, 0.10889743268489838, 0.10997477918863297, 0.09139761328697205, 0.10588718950748444, 0.1065109446644783, 0.11528827250003815, 0.08798609673976898, 0.10713669657707214, 0.10872343182563782, 0.0914025604724884, 0.11244096606969833, 0.11052747815847397, 0.09465806186199188, 0.09879904985427856, 0.10012910515069962, 0.09516877681016922, 0.0945606380701065, 0.10561782121658325, 0.11042183637619019, 0.10518999397754669, 0.09778665006160736, 0.10760726034641266, 0.09709193557500839, 0.0990147590637207, 0.10312111675739288, 0.10793481022119522, 0.09723412990570068, 0.09850934892892838, 0.11093246936798096, 0.10869063436985016, 0.0919635146856308, 0.09522388130426407, 0.10002601891756058, 0.10153037309646606, 0.11219479143619537, 0.11054059863090515, 0.103055939078331, 0.10131305456161499, 0.10488739609718323, 0.10494790226221085, 0.1089116781949997, 0.1023465096950531, 0.09925049543380737, 0.09636391699314117, 0.11167531460523605, 0.0909721851348877, 0.10135307908058167, 0.09525332599878311, 0.0965716540813446, 0.10572098940610886, 0.0881526842713356, 0.08698998391628265, 0.10659880191087723, 0.10451452434062958, 0.08930673450231552, 0.09738457947969437, 0.09797840565443039, 0.09486059099435806, 0.10140004754066467, 0.10595477372407913, 0.09851580858230591, 0.08485022187232971, 0.11250878870487213, 0.09474778175354004, 0.09554159641265869, 0.09978239238262177, 0.10267290472984314, 0.10761535167694092, 0.09761177748441696, 0.11036215722560883, 0.099433533847332, 0.1051754504442215, 0.09626913070678711, 0.11122722923755646, 0.09166895598173141, 0.10101557523012161, 0.10384643077850342, 0.09599318355321884, 0.10287085175514221, 0.10564546287059784, 0.10902027040719986, 0.10861654579639435, 0.09660717844963074, 0.11479690670967102, 0.10996180027723312, 0.10435828566551208, 0.10654188692569733, 0.09903262555599213, 0.09767648577690125, 0.09421654045581818, 0.10219437628984451, 0.09979193657636642, 0.09447171539068222, 0.0958808958530426, 0.07864385843276978, 0.10628657788038254, 0.09151861816644669, 0.10869044065475464, 0.10647065192461014, 0.09453162550926208, 0.11104059964418411, 0.11241325736045837, 0.11029411852359772, 0.10514996945858002, 0.09996934235095978, 0.10779710114002228, 0.10200642049312592, 0.11099623143672943, 0.09943899512290955, 0.09419412910938263, 0.10303837060928345, 0.11211907863616943, 0.09687873721122742, 0.10676321387290955, 0.09256629645824432, 0.11268811672925949, 0.10399680584669113, 0.09981022775173187, 0.10674168914556503, 0.09863004088401794, 0.10796023905277252, 0.11313273757696152, 0.09118523448705673, 0.0954170674085617, 0.10656356811523438, 0.10006317496299744, 0.10674984008073807, 0.08400363475084305, 0.10442502796649933, 0.094234898686409, 0.10082672536373138, 0.10053624212741852, 0.0948852151632309, 0.09427928924560547, 0.099433533847332, 0.10483885556459427, 0.10807926952838898, 0.09538314491510391, 0.10917181521654129, 0.09592883288860321, 0.0966121107339859, 0.09099120646715164, 0.11139927059412003, 0.08834859728813171, 0.0980030968785286, 0.09192343056201935, 0.10187423229217529, 0.10051606595516205, 0.10678164660930634, 0.09303279221057892, 0.09290370345115662, 0.09211535006761551, 0.09808460623025894, 0.10207503288984299, 0.08799263089895248, 0.10014353692531586, 0.09670184552669525, 0.1159762516617775, 0.10768468677997589, 0.10860677808523178, 0.10137512534856796, 0.11091577261686325, 0.10910852253437042, 0.09760044515132904, 0.10637710243463516, 0.10987025499343872, 0.09819839894771576, 0.11175700277090073, 0.10290488600730896, 0.10058784484863281, 0.10496336221694946, 0.09888613969087601, 0.11279034614562988, 0.09181591868400574, 0.10833221673965454, 0.10306226462125778, 0.10409821569919586, 0.11000955104827881, 0.11153571307659149, 0.10421887040138245, 0.10186611115932465, 0.10380998253822327, 0.10144513100385666, 0.10299976915121078, 0.10833607614040375, 0.08075369894504547, 0.11409375816583633, 0.10241833329200745, 0.1025848537683487, 0.09974752366542816, 0.10835494846105576, 0.09804822504520416, 0.10786724835634232, 0.09626944363117218, 0.11152107268571854, 0.09902714937925339, 0.11636265367269516, 0.09985204041004181, 0.09964863210916519, 0.10935143381357193, 0.09880474954843521, 0.06888705492019653, 0.08715100586414337, 0.08484771847724915, 0.09883932769298553, 0.1049882024526596, 0.10534877330064774, 0.11273494362831116, 0.11035792529582977, 0.10556378215551376, 0.09631054103374481, 0.09539930522441864, 0.11424659192562103, 0.10081898421049118, 0.09109993278980255, 0.10397140681743622, 0.09918928146362305, 0.10101489722728729, 0.10054941475391388, 0.09729015082120895, 0.09437747299671173, 0.11045055091381073, 0.09456945955753326, 0.10682235658168793, 0.10846135020256042, 0.10718987882137299, 0.11435166001319885, 0.10585753619670868, 0.0901469737291336, 0.08291113376617432, 0.10708369314670563, 0.10669465363025665, 0.0826137363910675, 0.08006526529788971, 0.09768248349428177, 0.10770732164382935, 0.10575108230113983, 0.1063966378569603, 0.10370086133480072, 0.10338646173477173, 0.10996776819229126, 0.09973715245723724, 0.09734102338552475, 0.10476361215114594, 0.10862241685390472, 0.10061892867088318, 0.10102403163909912, 0.11340542137622833, 0.08757708966732025, 0.10686318576335907, 0.09723494946956635, 0.11109134554862976, 0.1082485020160675, 0.10570040345191956, 0.10495299100875854, 0.11249701678752899, 0.10751450061798096, 0.09072539955377579, 0.10153552889823914, 0.10292676091194153, 0.09409941732883453, 0.10964864492416382, 0.11132902652025223, 0.10223415493965149, 0.08656671643257141, 0.11118092387914658, 0.102119080722332, 0.10796621441841125, 0.10300548374652863, 0.09757174551486969, 0.10488256812095642, 0.10406047105789185, 0.10997091978788376, 0.10817660391330719, 0.10717546194791794, 0.10613975673913956, 0.1033187210559845, 0.09665624052286148, 0.1043039932847023, 0.09105678647756577, 0.09756840765476227, 0.0926230251789093, 0.11130822449922562, 0.08311300724744797, 0.09354841709136963, 0.08865248411893845, 0.10640965402126312, 0.1123543381690979, 0.09487735480070114, 0.1091536208987236, 0.10986127704381943, 0.10184860229492188, 0.10228203982114792, 0.11335230618715286, 0.09410671889781952, 0.11326448619365692, 0.09750813245773315, 0.09799095243215561, 0.10721065104007721, 0.11007779091596603, 0.09400463104248047, 0.09509413689374924, 0.09284258633852005, 0.11146176606416702, 0.10205834358930588, 0.10248082131147385, 0.09989090263843536, 0.10339032113552094, 0.09984184801578522, 0.10399355739355087, 0.09583673626184464, 0.09912027418613434, 0.08532426506280899, 0.10922778397798538, 0.09934185445308685, 0.10191884636878967, 0.10021034628152847, 0.09791748225688934, 0.10330753028392792, 0.09451261162757874, 0.08763764053583145, 0.09793467074632645, 0.10466615855693817, 0.11283259838819504, 0.10825835168361664, 0.10590333491563797, 0.10902197659015656, 0.09817354381084442, 0.10235454142093658, 0.11183954030275345, 0.10467538237571716, 0.09290370345115662], \"y\": [0.10628486424684525, 0.10224214941263199, 0.10712610185146332, 0.1077880710363388, 0.09346099197864532, 0.08669374138116837, 0.09842828661203384, 0.08495412021875381, 0.10684924572706223, 0.08862334489822388, 0.1044851765036583, 0.10161349922418594, 0.10665493458509445, 0.09785622358322144, 0.10086830705404282, 0.10143449902534485, 0.1019691750407219, 0.08878916501998901, 0.10016574710607529, 0.06743787229061127, 0.07759223133325577, 0.09312182664871216, 0.0932028666138649, 0.10243048518896103, 0.09699136763811111, 0.10132615268230438, 0.10847708582878113, 0.1052526906132698, 0.10642433166503906, 0.07860135287046432, 0.09681855887174606, 0.09111876040697098, 0.08781508356332779, 0.08456281572580338, 0.10499466955661774, 0.09768768399953842, 0.09528448432683945, 0.09398526698350906, 0.09060858935117722, 0.10459206253290176, 0.10223502665758133, 0.09681311994791031, 0.07745154201984406, 0.08597955107688904, 0.0980227142572403, 0.08467478305101395, 0.07075276225805283, 0.09684593230485916, 0.0918121188879013, 0.10339140146970749, 0.09892421960830688, 0.07731800526380539, 0.08446519821882248, 0.07067887485027313, 0.0968329980969429, 0.08777742832899094, 0.08933764696121216, 0.08670707792043686, 0.07127004861831665, 0.10952898114919662, 0.1016240268945694, 0.09852227568626404, 0.09754684567451477, 0.09527117013931274, 0.10001479834318161, 0.11016140878200531, 0.09401141852140427, 0.09690742194652557, 0.10500486940145493, 0.08470983058214188, 0.09771449863910675, 0.0873163640499115, 0.07445705682039261, 0.09029421955347061, 0.09679017215967178, 0.10684837400913239, 0.0980592668056488, 0.08733823150396347, 0.08971546590328217, 0.10391580313444138, 0.11137223988771439, 0.09701723605394363, 0.10554575175046921, 0.10278212279081345, 0.0915040522813797, 0.09889908134937286, 0.09461343288421631, 0.09392529726028442, 0.10104728490114212, 0.1001058891415596, 0.1063879057765007, 0.08968880027532578, 0.08995092660188675, 0.09301581233739853, 0.10121290385723114, 0.10029458999633789, 0.09709462523460388, 0.0971326008439064, 0.09731963276863098, 0.09626083821058273, 0.09260943531990051, 0.09139203280210495, 0.10283882915973663, 0.09873216599225998, 0.09132926911115646, 0.10755062848329544, 0.09001410007476807, 0.09464047104120255, 0.09041335433721542, 0.10024077445268631, 0.10422932356595993, 0.09262173622846603, 0.09815233200788498, 0.08781436085700989, 0.09336622059345245, 0.09271720796823502, 0.07077596336603165, 0.1029595136642456, 0.10134931653738022, 0.09173423796892166, 0.10708393156528473, 0.0892309620976448, 0.08159352093935013, 0.10154678672552109, 0.10037717968225479, 0.09241456538438797, 0.08187924325466156, 0.08006516844034195, 0.10061182826757431, 0.10106556117534637, 0.10077884048223495, 0.09477106481790543, 0.09693004190921783, 0.0993608757853508, 0.09449060261249542, 0.10273852199316025, 0.09802711009979248, 0.0953235775232315, 0.09436523169279099, 0.08642800897359848, 0.08789510279893875, 0.1037697046995163, 0.08726289123296738, 0.09965135157108307, 0.10790766030550003, 0.1063336580991745, 0.10242099314928055, 0.10932032018899918, 0.09203927963972092, 0.09405063092708588, 0.09966567158699036, 0.09633179754018784, 0.08902806043624878, 0.0949471965432167, 0.09968119859695435, 0.08427533507347107, 0.07253610342741013, 0.08542450517416, 0.10629544407129288, 0.09176184982061386, 0.10697520524263382, 0.10675404220819473, 0.09399068355560303, 0.09121076762676239, 0.10828546434640884, 0.10603877156972885, 0.09826795011758804, 0.10315390676259995, 0.08926467597484589, 0.10139849781990051, 0.1100057065486908, 0.1062641441822052, 0.09494351595640182, 0.09531291574239731, 0.10254696756601334, 0.10737447440624237, 0.08392900973558426, 0.08563899248838425, 0.08502154797315598, 0.09938506782054901, 0.07854054868221283, 0.0883583053946495, 0.09326734393835068, 0.10222870856523514, 0.10425839573144913, 0.10329671204090118, 0.08995817601680756, 0.0914100706577301, 0.09421365708112717, 0.09454555064439774, 0.09705192595720291, 0.1057354137301445, 0.09078235179185867, 0.1065187081694603, 0.10854117572307587, 0.08974017947912216, 0.08847950398921967, 0.09580808132886887, 0.10635796934366226, 0.10217048972845078, 0.06451614201068878, 0.09533745795488358, 0.10216493904590607, 0.10420595854520798, 0.08948161453008652, 0.10655967146158218, 0.10043129324913025, 0.09757783263921738, 0.09420301765203476, 0.09770859777927399, 0.08132442086935043, 0.10373849421739578, 0.09192753583192825, 0.0769900381565094, 0.09532514214515686, 0.10328172892332077, 0.09215758740901947, 0.10517283529043198, 0.0964147076010704, 0.08239586651325226, 0.07475730031728745, 0.08777742832899094, 0.10189376026391983, 0.10209578275680542, 0.10305270552635193, 0.09456589072942734, 0.09715261310338974, 0.10144264250993729, 0.09760770946741104, 0.09845053404569626, 0.07475730031728745, 0.09225848317146301, 0.08469720929861069, 0.10474015027284622, 0.09442442655563354, 0.08959627151489258, 0.09518615901470184, 0.11205225437879562, 0.09532590210437775, 0.09854080528020859, 0.09702444821596146, 0.09894340485334396, 0.08084531873464584, 0.09266769140958786, 0.07564927637577057, 0.10800047963857651, 0.10308197140693665, 0.09717953205108643, 0.09673465043306351, 0.10175787657499313, 0.09371725469827652, 0.10306508094072342, 0.10205834358930588, 0.09484956413507462, 0.09932177513837814, 0.0965341255068779, 0.0903046727180481, 0.09870008379220963, 0.06504661589860916, 0.06822706758975983, 0.07348950952291489, 0.10433783382177353, 0.036237459629774094, 0.08910506963729858, 0.10405819863080978, 0.10515186935663223, 0.09368406981229782, 0.09717575460672379, 0.10343705862760544, 0.08056357502937317, 0.09688769280910492, 0.07888194173574448, 0.08758463710546494, 0.09162957966327667, 0.07359239459037781, 0.0907011404633522, 0.10281134396791458, 0.10247829556465149, 0.10855564475059509, 0.09442442655563354, 0.0964416116476059, 0.10225579887628555, 0.08745948225259781, 0.09754287451505661, 0.09948725253343582, 0.08344652503728867, 0.10342007130384445, 0.09477099031209946, 0.10212145000696182, 0.1013961136341095, 0.11192359775304794, 0.07978817075490952, 0.10177382826805115, 0.10455795377492905, 0.09130861610174179, 0.10417459160089493, 0.1041710153222084, 0.09438277035951614, 0.08316197246313095, 0.05866988003253937, 0.0827663466334343, 0.10013285279273987, 0.10465890914201736, 0.0826210305094719, 0.09926211833953857, 0.101127989590168, 0.08851553499698639, 0.0943654477596283, 0.1016809269785881, 0.10541590303182602, 0.10824001580476761, 0.08011198788881302, 0.10329215228557587, 0.08731764554977417, 0.07798471301794052, 0.08146790415048599, 0.0944475531578064, 0.1025918647646904, 0.10402592271566391, 0.06720122694969177, 0.08811378479003906, 0.09150286018848419, 0.09957874566316605, 0.09241826087236404, 0.0977969542145729, 0.10525177419185638, 0.10169669985771179, 0.09251801669597626, 0.09784895926713943, 0.09455384314060211, 0.08138792216777802, 0.10789968818426132, 0.08978433907032013, 0.1036176085472107, 0.09063193947076797, 0.10802332311868668, 0.10187949240207672, 0.10210030525922775, 0.10622985661029816, 0.10007251054048538, 0.08867616206407547, 0.09575528651475906, 0.10368431359529495, 0.08910441398620605, 0.07159354537725449, 0.10428184270858765, 0.09540174901485443, 0.09893163293600082, 0.09047358483076096, 0.09391242265701294, 0.09826769679784775, 0.09753242135047913, 0.09371552616357803, 0.08488502353429794, 0.09266769140958786, 0.10116802901029587, 0.08069966733455658, 0.10746351629495621, 0.09820915013551712, 0.10023991018533707, 0.10128550231456757, 0.10743842273950577, 0.09106172621250153, 0.0766826793551445, 0.09656437486410141, 0.08982976526021957, 0.09861214458942413, 0.10418438911437988, 0.09595577418804169, 0.09625377506017685, 0.10759557783603668, 0.10604014247655869, 0.10771826654672623, 0.09842795878648758, 0.10575388371944427, 0.10090801864862442, 0.09802757948637009, 0.09189590811729431, 0.10652459412813187, 0.1010194942355156, 0.09915865957736969, 0.08252929896116257, 0.09406290203332901, 0.09705137461423874, 0.08639385551214218, 0.0957750678062439, 0.10182008147239685, 0.07761143147945404, 0.10015717893838882, 0.0814685970544815, 0.10342038422822952, 0.09744275361299515, 0.10711407661437988, 0.09204108268022537, 0.09338738769292831, 0.08532051742076874, 0.0829806849360466, 0.11129890382289886, 0.1020071804523468, 0.10255974531173706, 0.09797465056180954, 0.07709407061338425, 0.10534625500440598, 0.0816308781504631, 0.10193627327680588, 0.0937059074640274, 0.09306787699460983, 0.09233160316944122, 0.07165724039077759, 0.08773063868284225, 0.08648862689733505, 0.0944642499089241, 0.1025918647646904, 0.09436129033565521, 0.10472502559423447, 0.10157907009124756, 0.10346176475286484, 0.10292646288871765, 0.1042926013469696, 0.09025606513023376, 0.07943440973758698, 0.08596797287464142, 0.0755690336227417, 0.09177374839782715, 0.09367930889129639, 0.09012246131896973, 0.08520843833684921, 0.09359999001026154, 0.09689375758171082, 0.09358153492212296, 0.10689546912908554, 0.10211961716413498, 0.09504840523004532, 0.09951736032962799, 0.104240283370018, 0.0993758961558342, 0.10593856871128082, 0.10213477164506912, 0.09071360528469086, 0.1067211851477623, 0.09395578503608704, 0.07682404667139053, 0.10441695898771286, 0.10497178882360458, 0.1022716537117958, 0.09767956286668777, 0.10064107924699783, 0.10349965840578079, 0.09190060943365097, 0.09563611447811127, 0.08207868784666061, 0.10526331514120102, 0.10429906100034714, 0.09331116080284119, 0.09680695086717606, 0.08524733036756516, 0.1072995737195015, 0.10894232988357544, 0.043334368616342545, 0.09888742119073868, 0.10140037536621094, 0.09047358483076096, 0.09838249534368515, 0.09207691997289658, 0.09279746562242508, 0.09423954039812088, 0.1026722863316536, 0.08733800053596497, 0.07495898753404617, 0.09585465490818024, 0.07943855971097946, 0.10601955652236938, 0.09940189123153687, 0.10498347878456116, 0.09656437486410141, 0.08790569752454758, 0.10926617681980133, 0.0874088853597641, 0.09335466474294662, 0.1043623611330986, 0.08987059444189072, 0.0998067781329155, 0.07439780980348587, 0.09934801608324051, 0.08704795688390732, 0.1082497164607048, 0.0920274555683136, 0.09844190627336502, 0.10666487365961075, 0.09590614587068558, 0.08600170165300369, 0.09404980391263962, 0.10353720188140869, 0.10256625711917877, 0.10707160085439682, 0.09661853313446045, 0.09525647759437561, 0.09282375127077103, 0.10070070624351501, 0.1000036746263504, 0.09571629762649536, 0.10404808819293976, 0.10398586094379425, 0.09816968441009521, 0.09869948029518127, 0.08291567116975784, 0.08836442977190018, 0.07924848794937134, 0.09318024665117264, 0.07871139794588089, 0.08562098443508148, 0.10561607033014297, 0.09332079440355301, 0.07745154201984406, 0.08685228228569031, 0.09338938444852829, 0.09508612006902695, 0.0891970843076706, 0.10297658294439316, 0.0936722457408905, 0.08464637398719788, 0.1042419895529747, 0.09421650320291519, 0.10118021070957184, 0.10733216255903244, 0.08854275941848755, 0.1066560372710228, 0.07352259010076523, 0.09449175000190735, 0.09489904344081879, 0.09534768015146255, 0.09545105695724487, 0.10549954324960709, 0.10676810890436172, 0.08768271654844284, 0.10697802156209946, 0.10096253454685211, 0.08176076412200928, 0.07839351147413254, 0.09441430121660233, 0.09560112655162811, 0.07475543022155762, 0.09846844524145126, 0.1087096706032753, 0.07981386035680771, 0.08922968059778214, 0.1024462953209877, 0.10343869030475616, 0.09294723719358444, 0.0915822833776474, 0.10695800930261612, 0.09716614335775375, 0.10618602484464645, 0.09926211833953857, 0.10797496885061264, 0.07650244235992432, 0.10674965381622314, 0.09405762702226639, 0.108135886490345, 0.08079301565885544, 0.10579092800617218, 0.09428544342517853, 0.09830303490161896, 0.09771380573511124, 0.08784971386194229, 0.10771988332271576, 0.0982077345252037, 0.09706173092126846, 0.08626502752304077, 0.10589762777090073, 0.08389387279748917, 0.09884665161371231, 0.09334935992956161, 0.08389300107955933, 0.09849821031093597, 0.0928802415728569, 0.08733800053596497, 0.09013048559427261, 0.10141681134700775, 0.09802967309951782, 0.10829687118530273, 0.10731357336044312, 0.09769553691148758, 0.10107535868883133, 0.10459206253290176, 0.09479101002216339, 0.10480134934186935, 0.10711710900068283, 0.09509221464395523, 0.09971807897090912, 0.10887929797172546, 0.09419239312410355, 0.08543854206800461, 0.09026245027780533, 0.10889887809753418, 0.10876825451850891, 0.09026356041431427, 0.10133093595504761, 0.06374987959861755, 0.09134804457426071, 0.08302520960569382, 0.0848245844244957, 0.09102042764425278, 0.07487177848815918, 0.10411661118268967, 0.0856737568974495, 0.08897474408149719, 0.0947648361325264, 0.09822849184274673, 0.09506797790527344, 0.06936080753803253, 0.09580431878566742, 0.08053920418024063, 0.10286140441894531, 0.0972675085067749, 0.1019691750407219, 0.10100992769002914, 0.09765281528234482, 0.08909524232149124, 0.08978433907032013, 0.10228642076253891, 0.09961947798728943, 0.0854720026254654, 0.0942034050822258, 0.10307586938142776, 0.09137026965618134, 0.09815894812345505, 0.09374184906482697, 0.0672084242105484, 0.10194327682256699, 0.09143973141908646, 0.08188183605670929, 0.0999564453959465, 0.09536226838827133, 0.07493340224027634, 0.10466336458921432, 0.1004263311624527, 0.06968525797128677, 0.09561484307050705, 0.05718091502785683, 0.10315047949552536, 0.09530845284461975, 0.08041471242904663, 0.09266587346792221, 0.09636136889457703, 0.08139138668775558, 0.10788047313690186, 0.09218642860651016, 0.09935786575078964, 0.09045331180095673, 0.10388395935297012, 0.10637683421373367, 0.08825894445180893, 0.1010323092341423, 0.098914235830307, 0.09393897652626038, 0.09330376237630844, 0.09411059319972992, 0.09262481331825256, 0.0868380144238472, 0.08763442188501358, 0.08715227991342545, 0.09762558341026306, 0.09161273390054703, 0.11023055762052536, 0.08667779713869095, 0.08924919366836548, 0.09673465043306351, 0.07390592247247696, 0.09340647608041763, 0.10545330494642258, 0.10929781943559647, 0.0881894901394844, 0.08702727407217026, 0.09582732617855072, 0.0809665247797966, 0.09129752963781357, 0.09694202244281769, 0.10299849510192871, 0.09181158989667892, 0.1075887456536293, 0.08272445201873779, 0.09988794475793839, 0.10575540363788605, 0.10797189921140671, 0.08731764554977417, 0.1040334403514862, 0.1061418429017067, 0.08770601451396942, 0.09744361788034439, 0.0986577495932579, 0.08367867022752762, 0.09224098920822144, 0.09614410996437073, 0.08484546095132828, 0.09352986514568329, 0.105023093521595, 0.10777797549962997, 0.10476525872945786, 0.09373736381530762, 0.10511072725057602, 0.09076886624097824, 0.09365268796682358, 0.08894093334674835, 0.10783764719963074, 0.09572731703519821, 0.09136847406625748, 0.09218631684780121, 0.10597791522741318, 0.08977056294679642, 0.09453131258487701, 0.0988759994506836, 0.09716614335775375, 0.10175282508134842, 0.10817472636699677, 0.10082917660474777, 0.09419239312410355, 0.10139790177345276, 0.10123195499181747, 0.10837144404649734, 0.10107577592134476, 0.09201306849718094, 0.09303983300924301, 0.10464761406183243, 0.08004799485206604, 0.09974775463342667, 0.08103392273187637, 0.0920274555683136, 0.10302498191595078, 0.0869317427277565, 0.08072813600301743, 0.10284171998500824, 0.10279244184494019, 0.06991773843765259, 0.09637292474508286, 0.09778275340795517, 0.08967951685190201, 0.09730418771505356, 0.09670518338680267, 0.09203604608774185, 0.07597513496875763, 0.10335034877061844, 0.08379808813333511, 0.09030517190694809, 0.08675336837768555, 0.10018379241228104, 0.10038623958826065, 0.08805468678474426, 0.10036613792181015, 0.08631296455860138, 0.10237253457307816, 0.08369053155183792, 0.10163329541683197, 0.08720462024211884, 0.09684545546770096, 0.09600039571523666, 0.09587705880403519, 0.09765007346868515, 0.09357393532991409, 0.10494565963745117, 0.09339530766010284, 0.09474518895149231, 0.10587657243013382, 0.10228642076253891, 0.09666504710912704, 0.10640770941972733, 0.09200193732976913, 0.09397048503160477, 0.08671104907989502, 0.1011865958571434, 0.08948885649442673, 0.07140044122934341, 0.08700298517942429, 0.06792665272951126, 0.0935911014676094, 0.07892937958240509, 0.10098294168710709, 0.10215012729167938, 0.08875762671232224, 0.10140025615692139, 0.10063429921865463, 0.09984523057937622, 0.0997086763381958, 0.0985136553645134, 0.10426284372806549, 0.09441985189914703, 0.1101001426577568, 0.09681060165166855, 0.09273711591959, 0.09164069592952728, 0.10412047058343887, 0.09617908298969269, 0.09655676037073135, 0.08006621152162552, 0.09604661911725998, 0.10059219598770142, 0.09795305132865906, 0.1029595136642456, 0.09754360467195511, 0.10764404386281967, 0.1106165274977684, 0.06819258630275726, 0.08885559439659119, 0.09205209463834763, 0.09889186918735504, 0.08915720880031586, 0.08107886463403702, 0.10389047861099243, 0.09216981381177902, 0.09758999198675156, 0.09910648316144943, 0.09400500357151031, 0.08228618651628494, 0.08631296455860138, 0.10412679612636566, 0.10639064759016037, 0.08954954147338867, 0.102094367146492, 0.0829806849360466, 0.09487204998731613, 0.08131267875432968, 0.11104290932416916, 0.0745091587305069, 0.09741459041833878, 0.08895044028759003, 0.09824829548597336, 0.09938240796327591, 0.10472588241100311, 0.09209111332893372, 0.09012474864721298, 0.08440188318490982, 0.08787459880113602, 0.09854225814342499, 0.0707395002245903, 0.0896764025092125, 0.09525543451309204, 0.09833989292383194, 0.09519288688898087, 0.09783641248941422, 0.10038838535547256, 0.11034906655550003, 0.10508274286985397, 0.09257822483778, 0.08915219455957413, 0.10867051035165787, 0.08759218454360962, 0.10281768441200256, 0.10055778175592422, 0.09778232127428055, 0.0974239930510521, 0.09558103233575821, 0.10387442260980606, 0.0796068087220192, 0.10413328558206558, 0.10208595544099808, 0.0953081026673317, 0.10089267045259476, 0.10726340860128403, 0.10079064220190048, 0.09460233896970749, 0.09595035761594772, 0.08348005264997482, 0.09657569974660873, 0.10548902302980423, 0.08000383526086807, 0.11018825322389603, 0.08811014890670776, 0.09962896257638931, 0.09851115196943283, 0.10511644929647446, 0.0862160176038742, 0.0986032709479332, 0.08689289540052414, 0.10038144886493683, 0.09500018507242203, 0.1017717495560646, 0.09348937124013901, 0.09673482924699783, 0.10422413796186447, 0.09282538294792175, 0.056313931941986084, 0.07229000329971313, 0.0752398744225502, 0.09398526698350906, 0.09690132737159729, 0.103575699031353, 0.11061536520719528, 0.10343705862760544, 0.10446051508188248, 0.09456603974103928, 0.09242715686559677, 0.09520112723112106, 0.08978576213121414, 0.07946404814720154, 0.10124120861291885, 0.09178476780653, 0.09642945975065231, 0.09953285753726959, 0.09017989784479141, 0.08386296778917313, 0.10989440977573395, 0.09255029261112213, 0.09768838435411453, 0.09078759700059891, 0.10506153106689453, 0.10541949421167374, 0.10548306256532669, 0.08052733540534973, 0.08079808205366135, 0.10051856935024261, 0.1041395291686058, 0.06420163810253143, 0.07945521175861359, 0.09701723605394363, 0.10411214083433151, 0.10223739594221115, 0.104240283370018, 0.09017110615968704, 0.09537111967802048, 0.10221913456916809, 0.0919153243303299, 0.09703696519136429, 0.09296908229589462, 0.09756352007389069, 0.09379518777132034, 0.0928238108754158, 0.10915153473615646, 0.08701193332672119, 0.09995130449533463, 0.09508612006902695, 0.10397987812757492, 0.09934427589178085, 0.10286041349172592, 0.10250882059335709, 0.10357562452554703, 0.10557608306407928, 0.08689989149570465, 0.0976448804140091, 0.0912005603313446, 0.091756671667099, 0.1066073402762413, 0.11019373685121536, 0.0994231179356575, 0.07821708917617798, 0.10423136502504349, 0.09604146331548691, 0.10163024812936783, 0.0975114032626152, 0.09688988327980042, 0.10053138434886932, 0.10172762721776962, 0.10551225394010544, 0.10241072624921799, 0.10477044433355331, 0.09286786615848541, 0.10047020763158798, 0.09419897943735123, 0.10323033481836319, 0.09096768498420715, 0.09104994684457779, 0.07910125702619553, 0.10578661412000656, 0.07822460681200027, 0.08924074470996857, 0.08507922291755676, 0.09919226914644241, 0.10867064446210861, 0.08229595422744751, 0.10713359713554382, 0.10307281464338303, 0.09252599626779556, 0.09939407557249069, 0.10678809136152267, 0.08569800108671188, 0.11053066700696945, 0.09410703927278519, 0.08828337490558624, 0.10529810935258865, 0.09936685115098953, 0.0919850692152977, 0.09456468373537064, 0.08358647674322128, 0.10382553935050964, 0.0983409509062767, 0.09772079437971115, 0.09642872959375381, 0.09687989950180054, 0.09590578079223633, 0.10271073877811432, 0.08364661782979965, 0.09888331592082977, 0.07887471467256546, 0.10335593670606613, 0.0872601792216301, 0.09845816344022751, 0.0993758961558342, 0.0977182611823082, 0.10024815797805786, 0.09384874999523163, 0.07629051804542542, 0.07613525539636612, 0.1036958396434784, 0.11212578415870667, 0.10793337970972061, 0.10429683327674866, 0.10284369438886642, 0.08872126787900925, 0.10025101900100708, 0.09447214752435684, 0.10032325237989426, 0.09012478590011597]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Likelihood of genders for true positive (female)\"}, \"xaxis\": {\"title\": {\"text\": \"Likelihood female\"}}, \"yaxis\": {\"title\": {\"text\": \"Likelihood male\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a3bb4752-9f54-4ec5-977d-fe84ea42e39a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"02a8abc6-41c5-4693-aad9-6e5135894d38\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"02a8abc6-41c5-4693-aad9-6e5135894d38\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '02a8abc6-41c5-4693-aad9-6e5135894d38',\n",
              "                        [{\"mode\": \"markers\", \"type\": \"scatter\", \"x\": [0.10458124428987503, 0.11049388349056244, 0.10355816781520844, 0.11023961752653122, 0.11522287875413895, 0.11047102510929108, 0.11682604998350143, 0.11332305520772934, 0.11176060885190964, 0.1191902682185173, 0.11201874166727066, 0.11207925528287888, 0.11690402030944824, 0.10943260043859482, 0.10269667953252792, 0.1203332468867302, 0.11006252467632294, 0.11465919762849808, 0.11257515102624893, 0.11550599336624146, 0.11702954024076462, 0.11423038691282272, 0.11805025488138199, 0.10597876459360123, 0.11745107173919678, 0.10415820777416229, 0.11579767614603043, 0.11518336087465286, 0.11364952474832535, 0.10652871429920197, 0.11819106340408325, 0.11801616847515106, 0.11835812777280807, 0.10947864502668381, 0.11041363328695297, 0.10753748565912247, 0.11879365146160126, 0.11339548230171204, 0.11716429889202118, 0.11199919134378433, 0.12230538576841354, 0.11095957458019257, 0.1047329381108284, 0.11737660318613052, 0.11693120747804642, 0.11947297304868698, 0.1163666844367981, 0.11282199621200562, 0.11816726624965668, 0.10876138508319855, 0.11670155078172684, 0.11644253879785538, 0.11745814234018326, 0.11323171108961105, 0.1146426722407341, 0.11538460105657578, 0.1183684691786766, 0.1139327883720398, 0.11419426649808884, 0.11872197687625885, 0.1111869290471077, 0.11864646524190903, 0.11426303535699844, 0.11539316922426224, 0.11368611454963684, 0.11230155825614929, 0.1173468679189682, 0.11552365124225616, 0.11087870597839355, 0.11181575059890747, 0.10864584147930145, 0.11268576234579086, 0.12109266966581345, 0.11605557799339294, 0.11193503439426422, 0.10806933790445328, 0.11498118191957474, 0.12117872387170792, 0.10113510489463806, 0.11308885365724564, 0.11164730042219162, 0.11398068815469742, 0.12060565501451492, 0.12119230628013611, 0.1123862937092781, 0.1142730638384819, 0.11105913668870926, 0.10725622624158859, 0.11176344007253647, 0.11912258714437485, 0.10899161547422409, 0.10987967997789383, 0.1166771873831749, 0.11997514963150024, 0.10389730334281921, 0.12111460417509079, 0.10547962039709091, 0.10499835759401321, 0.10689422488212585, 0.11245567351579666, 0.11613289266824722, 0.10382109880447388, 0.12170352041721344, 0.11875472962856293, 0.11152435839176178, 0.10849258303642273, 0.11848438531160355, 0.1123725175857544, 0.11930399388074875, 0.11280619353055954, 0.11884037405252457, 0.1130119040608406, 0.10618879646062851, 0.12374500185251236, 0.11023910343647003, 0.11437822878360748, 0.1246677041053772, 0.1164042055606842, 0.12013821303844452, 0.10855195671319962, 0.10299991816282272, 0.11941555887460709, 0.12294062227010727, 0.11773748695850372, 0.11042259633541107, 0.11646387726068497, 0.10932470858097076, 0.11578668653964996, 0.11838134378194809, 0.1086784079670906, 0.11787265539169312, 0.11212135851383209, 0.11973877996206284, 0.093973807990551, 0.11935402452945709, 0.1138983964920044, 0.11344025284051895, 0.12195413559675217, 0.12172259390354156, 0.11517924815416336, 0.11831735819578171, 0.09681948274374008, 0.11214607208967209, 0.11923471838235855, 0.09805908054113388, 0.11029466241598129, 0.11725632101297379, 0.12028982490301132, 0.11012399196624756, 0.11044905334711075, 0.11865977197885513, 0.11724601686000824, 0.12118923664093018, 0.09738639742136002, 0.10320493578910828, 0.11084036529064178, 0.11671290546655655, 0.12013018876314163, 0.11816251277923584, 0.1111045554280281, 0.11362140625715256, 0.1144779697060585, 0.11842667311429977, 0.11392989009618759, 0.11664839833974838, 0.10936534404754639, 0.12114575505256653, 0.10651542991399765, 0.11839381605386734, 0.1198224201798439, 0.11353305727243423, 0.11266963928937912, 0.12209437042474747, 0.10076887160539627, 0.1149456575512886, 0.11252935975790024, 0.11313372850418091, 0.11800649017095566, 0.10734330862760544, 0.11449039727449417, 0.11186745017766953, 0.11298388242721558, 0.10860263556241989, 0.11631462723016739, 0.11851248890161514, 0.10603685677051544, 0.10394275188446045, 0.12121149897575378, 0.1182694211602211, 0.08787892013788223, 0.10292435437440872, 0.12317072600126266, 0.10855845361948013, 0.11499109119176865, 0.09910047054290771, 0.10745594650506973, 0.11457153409719467, 0.1119508221745491, 0.1163211539387703, 0.11103158444166183, 0.11942235380411148, 0.10889343917369843, 0.11780460923910141, 0.11808223277330399, 0.11160804331302643, 0.11312682181596756, 0.1148548573255539, 0.11146152019500732, 0.11843381822109222, 0.11691843718290329, 0.11795573681592941, 0.11833126842975616, 0.11804313212633133, 0.11923131346702576, 0.11590927094221115, 0.11355374753475189, 0.11574506759643555, 0.11400143802165985, 0.10989878326654434, 0.11590374261140823, 0.11520485579967499, 0.10204967111349106, 0.1189304068684578, 0.11455430090427399, 0.11684894561767578, 0.11199935525655746, 0.10236545652151108, 0.11161570250988007, 0.11977004259824753, 0.118702732026577, 0.10767516493797302, 0.11316553503274918, 0.12197790294885635, 0.10921424627304077, 0.09983064234256744, 0.09911858290433884, 0.11013735830783844, 0.12060565501451492, 0.11158274859189987, 0.11098446696996689, 0.12358670681715012, 0.10789156705141068, 0.1182592511177063, 0.11402196437120438, 0.11131351441144943, 0.10965601354837418, 0.10552368313074112, 0.1207868829369545, 0.11270617693662643, 0.1203407272696495, 0.11279410123825073, 0.11414951831102371, 0.1156143918633461, 0.11800205707550049, 0.10742288827896118, 0.1182933822274208, 0.12068955600261688, 0.10364708304405212, 0.10724615305662155, 0.10665493458509445, 0.12118347734212875, 0.10482201725244522, 0.10535962134599686, 0.10151102393865585, 0.11324087530374527, 0.10821902006864548, 0.1238054558634758, 0.11298558115959167, 0.11046113818883896, 0.11514456570148468, 0.11414686590433121, 0.10980311781167984, 0.1061294674873352, 0.11518561840057373, 0.11005397886037827, 0.11045856028795242, 0.11661828309297562, 0.10506570339202881, 0.11958195269107819, 0.10678276419639587, 0.11799285560846329, 0.12109438329935074, 0.12150954455137253, 0.11916451901197433, 0.11453422904014587, 0.11529815196990967, 0.12486706674098969, 0.11224719136953354, 0.12024407833814621, 0.11622333526611328, 0.1122455820441246, 0.11578209698200226, 0.11820816248655319, 0.11792520433664322, 0.1168479397892952, 0.11642419546842575, 0.10824153572320938, 0.11408825218677521, 0.11977893114089966, 0.11797461658716202, 0.11995728313922882, 0.12116847187280655, 0.11308583617210388, 0.11870596557855606, 0.11866630613803864, 0.12368594855070114, 0.11861156672239304, 0.10981905460357666, 0.11176075786352158, 0.11234379559755325, 0.11154966056346893, 0.10885509103536606, 0.11996840685606003, 0.11235768347978592, 0.11943210661411285, 0.11190450936555862, 0.1094105914235115, 0.10813602805137634, 0.11542186886072159, 0.11162910610437393, 0.10138318687677383, 0.10924083739519119, 0.11832521110773087, 0.10870213806629181, 0.10443522781133652, 0.11991863697767258, 0.11532863229513168, 0.11140214651823044, 0.11893097311258316, 0.1084500104188919, 0.10918650031089783, 0.0988064631819725, 0.11524151265621185, 0.11783262342214584, 0.11685682088136673, 0.10896334797143936, 0.12088609486818314, 0.11190766841173172, 0.11872704327106476, 0.11849237233400345, 0.11655810475349426, 0.10634011030197144, 0.1109813004732132, 0.11603053659200668, 0.11705844104290009, 0.11715002357959747, 0.11313017457723618, 0.10999564826488495, 0.12083960324525833, 0.11050481349229813, 0.11256692558526993, 0.11775249987840652, 0.11232444643974304, 0.1169235110282898, 0.11960581690073013, 0.1094694435596466, 0.11444287747144699, 0.10302489250898361, 0.11115231364965439, 0.12031812965869904, 0.11838247627019882, 0.11611685901880264, 0.11228359490633011, 0.11536715179681778, 0.11155349761247635, 0.10940837115049362, 0.11506792902946472, 0.11633516103029251, 0.10577329248189926, 0.10334399342536926, 0.10086169093847275, 0.10848674923181534, 0.111510269343853, 0.10847026109695435, 0.12015000730752945, 0.11908622831106186, 0.11743474751710892, 0.11361107975244522, 0.11305084824562073, 0.11820968240499496, 0.11533931642770767, 0.11796990036964417, 0.12374413758516312, 0.10564389079809189, 0.11805353313684464, 0.09828714281320572, 0.09986359626054764, 0.11774420738220215, 0.10917998850345612, 0.10805752128362656, 0.1145661473274231, 0.1140207052230835, 0.11285082250833511, 0.1036037728190422, 0.11878472566604614, 0.11003518849611282, 0.10430354624986649, 0.11885916441679001, 0.11228794604539871, 0.1252484917640686, 0.1158764436841011, 0.11683718860149384, 0.11840599030256271, 0.11230408400297165, 0.10792745649814606, 0.11596816778182983, 0.11427049338817596, 0.10302197188138962, 0.11311570554971695, 0.11913224309682846, 0.105782650411129, 0.11690574139356613, 0.10212184488773346, 0.12049777805805206, 0.10715117305517197, 0.11216910928487778, 0.11476285010576248, 0.1138344258069992, 0.11895687133073807, 0.1149962842464447, 0.11397326737642288, 0.11949411779642105, 0.09938803315162659, 0.11330444365739822, 0.12160464376211166, 0.11297249048948288, 0.11728356033563614, 0.10594922304153442, 0.11343267560005188, 0.10803361982107162, 0.11491336673498154, 0.11856376379728317, 0.11094439774751663, 0.11445482820272446, 0.11406394094228745, 0.1118432953953743, 0.11413827538490295, 0.0998644232749939, 0.12456122040748596, 0.10311521589756012, 0.11750731617212296, 0.11206325888633728, 0.11634672433137894, 0.12097997218370438, 0.11891473084688187, 0.1144523173570633, 0.11679660528898239, 0.1162547916173935, 0.11407525092363358, 0.119436115026474, 0.12114747613668442, 0.1175449788570404, 0.11885695904493332, 0.11641105264425278, 0.11841975897550583, 0.10819755494594574, 0.11786779016256332, 0.11873368918895721, 0.1186261996626854, 0.11478975415229797, 0.11772763729095459, 0.11883875727653503, 0.11283490061759949, 0.11513832956552505, 0.09897637367248535, 0.11703141033649445, 0.11133268475532532, 0.1175992414355278, 0.11130235344171524, 0.11351708322763443, 0.12057620286941528, 0.10663239657878876, 0.10501598566770554, 0.11542274802923203, 0.12028158456087112, 0.10769672691822052, 0.11673565953969955, 0.11845792829990387, 0.12045370787382126, 0.10323828458786011, 0.11471589654684067, 0.10103321075439453, 0.1184510812163353, 0.10748562216758728, 0.12433821707963943, 0.12061986327171326, 0.10291606932878494, 0.11656781286001205, 0.11112534999847412, 0.10946434736251831, 0.1146046444773674, 0.11365141719579697, 0.11346957087516785, 0.11512082815170288, 0.120686836540699, 0.12314169108867645, 0.12270711362361908, 0.1102713868021965, 0.11538024991750717, 0.11242814362049103, 0.11232586950063705, 0.1196453869342804, 0.10736507922410965, 0.10769256949424744, 0.11661200225353241, 0.12103328853845596, 0.11214587837457657, 0.11509213596582413, 0.10466723889112473, 0.09542988240718842, 0.11871793121099472, 0.11421594768762589, 0.11110465973615646, 0.11141779273748398, 0.11322823911905289, 0.11528390645980835, 0.11517404019832611, 0.11728613823652267, 0.09833358228206635, 0.11807379126548767, 0.11655744910240173, 0.10706739872694016, 0.10634882003068924, 0.12026331573724747, 0.10644783079624176, 0.12182678282260895, 0.12026668339967728, 0.10471928864717484, 0.10376910120248795, 0.11639419198036194, 0.10783059895038605, 0.11556167900562286, 0.11391172558069229, 0.11039578914642334, 0.12001744657754898, 0.10780692100524902, 0.12218453735113144, 0.11657912284135818, 0.11018114537000656, 0.1137290671467781, 0.10195838660001755, 0.10840674489736557, 0.11099494993686676, 0.12124046683311462, 0.11160025745630264, 0.11456616967916489, 0.12042705714702606, 0.09425144642591476, 0.11660326272249222, 0.10026782751083374, 0.11659818887710571, 0.11132339388132095, 0.11367500573396683, 0.11279352009296417, 0.09771443158388138, 0.11830686032772064, 0.11969335377216339, 0.12013740837574005, 0.10725369304418564, 0.11879309266805649, 0.11553158611059189, 0.10338578373193741, 0.11771587282419205, 0.11357628554105759, 0.11265388876199722, 0.11856698989868164, 0.10442095249891281, 0.12264865636825562, 0.10725615918636322, 0.11177582293748856, 0.11766485869884491, 0.10202784091234207, 0.11467989534139633, 0.11830658465623856, 0.12077365070581436, 0.12099756300449371, 0.11852995306253433, 0.11013084650039673, 0.12064940482378006, 0.11905229091644287, 0.11598760634660721, 0.1205371618270874, 0.11092100292444229, 0.11417225748300552, 0.11911482363939285, 0.11870432645082474, 0.11315753310918808, 0.11816231906414032, 0.1166117861866951, 0.1147027313709259, 0.11812783032655716, 0.10140799731016159, 0.11246158927679062, 0.11521121859550476, 0.12288402020931244, 0.1061006635427475, 0.10710816085338593, 0.09357506781816483, 0.10490204393863678, 0.10329350084066391, 0.11550652235746384, 0.11685152351856232, 0.11234722286462784, 0.10716139525175095, 0.11783935129642487, 0.11243685334920883, 0.11848265677690506, 0.11026419699192047, 0.11969335377216339, 0.11539939045906067, 0.10455039143562317, 0.120198555290699, 0.11929283291101456, 0.12112490832805634, 0.11651494354009628, 0.10870379954576492, 0.11562094837427139, 0.10979031771421432, 0.11189070343971252, 0.11647883802652359, 0.11968018859624863, 0.10700177401304245, 0.10210692882537842, 0.12018612772226334, 0.10592248290777206, 0.11193389445543289, 0.11402720212936401, 0.11626195162534714, 0.11685331165790558, 0.1200062558054924, 0.10781406611204147, 0.11117582023143768, 0.1122230663895607, 0.1128767803311348, 0.10407710075378418, 0.12299442291259766, 0.11926499009132385, 0.11233732104301453, 0.11417152732610703, 0.10064608603715897, 0.1172713115811348, 0.1100831851363182, 0.11365709453821182, 0.12108036130666733, 0.118179552257061, 0.11152277141809464, 0.11518731713294983, 0.11907992511987686, 0.1169031411409378, 0.11368557065725327, 0.11002298444509506, 0.09053944051265717, 0.11368261277675629, 0.11747828871011734, 0.1140749379992485, 0.1125665083527565, 0.10226646810770035, 0.11988672614097595, 0.1114584282040596, 0.11923938989639282, 0.11010395735502243, 0.09324117749929428, 0.11333176493644714, 0.10842786729335785, 0.10681575536727905, 0.10748562216758728, 0.10692115128040314, 0.11231139302253723, 0.10761850327253342, 0.11803659051656723, 0.09965062886476517, 0.1068725436925888, 0.11496776342391968, 0.11738360673189163, 0.11167795211076736, 0.11833403259515762, 0.10677541047334671, 0.11412788927555084, 0.11647782474756241, 0.11712943017482758, 0.11901991814374924, 0.10626046359539032, 0.10968666523694992, 0.10389801114797592, 0.11233657598495483, 0.11869978159666061, 0.11286459118127823, 0.11167826503515244, 0.10525555908679962, 0.10493195801973343, 0.1120443195104599, 0.1176285669207573, 0.11491646617650986, 0.09761147946119308, 0.11849173158407211, 0.10776572674512863, 0.10793349891901016, 0.10209447145462036, 0.11496329307556152, 0.10892617702484131, 0.11186255514621735, 0.11953172832727432, 0.12066353112459183, 0.1098007783293724, 0.11366161704063416, 0.11576587706804276, 0.11558644473552704, 0.10501255095005035, 0.12093532085418701, 0.114250548183918, 0.10860488563776016, 0.11394432932138443, 0.11789623647928238, 0.11491703242063522, 0.12287331372499466, 0.1195211336016655, 0.11428122222423553, 0.10019176453351974, 0.10439508408308029, 0.10712087899446487, 0.10679727047681808, 0.11080887913703918, 0.1136263832449913, 0.11031509190797806, 0.1066734567284584, 0.1153469905257225, 0.11605709791183472, 0.12154737859964371, 0.11394358426332474, 0.10894710570573807, 0.11082712560892105, 0.11013630032539368, 0.09886599332094193, 0.1142551526427269, 0.10961107909679413, 0.10868141055107117, 0.1077909991145134, 0.11691843718290329, 0.1127370297908783, 0.11956051737070084, 0.12083960324525833, 0.11306125670671463, 0.0995018407702446, 0.09655804187059402, 0.10450167953968048, 0.11572570353746414, 0.11323054134845734, 0.10823777318000793, 0.11008859425783157, 0.11714264005422592, 0.11592534929513931, 0.10296477377414703, 0.1107276901602745, 0.10511550307273865, 0.10970734059810638, 0.10605091601610184, 0.09909004718065262, 0.11456587165594101, 0.11933977901935577, 0.11156477779150009, 0.116269551217556, 0.12018761783838272, 0.11137891560792923, 0.11955607682466507, 0.09701543301343918, 0.1143159493803978, 0.11820078641176224, 0.11358166486024857, 0.1132039800286293, 0.10948633402585983, 0.11856579035520554, 0.11714093387126923, 0.10077434778213501, 0.11304166167974472, 0.11462175101041794, 0.11884787678718567, 0.10954218357801437, 0.10545533150434494, 0.11681986600160599, 0.11613398790359497, 0.11580119282007217, 0.11114556342363358, 0.12218382954597473, 0.11004117876291275, 0.11914464086294174, 0.10826658457517624, 0.10616809874773026, 0.10435011237859726, 0.11818257719278336, 0.11974316835403442, 0.11590971797704697, 0.10776390880346298, 0.10922937095165253, 0.10717689245939255, 0.09280678629875183, 0.11573498696088791, 0.11834043264389038, 0.11793424934148788, 0.11946616321802139, 0.11605557799339294, 0.11643616855144501, 0.11178407073020935, 0.12000615149736404, 0.12129905074834824, 0.12088797241449356, 0.10314295440912247, 0.11274021118879318, 0.11875760555267334, 0.11697668582201004, 0.09890895336866379, 0.12085305899381638, 0.11071974784135818, 0.1174711361527443, 0.11173803359270096, 0.1177472323179245, 0.11218080669641495, 0.11917063593864441, 0.1048060804605484, 0.10705945640802383, 0.11849690228700638, 0.11774397641420364, 0.12426910549402237, 0.11548268795013428, 0.12185247987508774, 0.11656730622053146, 0.1082489863038063, 0.11685258895158768, 0.10835202783346176, 0.1151997521519661, 0.12286771088838577, 0.11686702817678452, 0.11636468023061752, 0.11836037784814835, 0.11475689709186554, 0.11817742139101028, 0.1185959205031395, 0.1128678098320961, 0.11745981872081757, 0.11283273994922638, 0.11692694574594498, 0.11665662378072739, 0.10982511192560196, 0.11208058148622513, 0.12005510181188583, 0.11562299728393555, 0.1163249984383583, 0.10544147342443466, 0.09897637367248535, 0.11947882175445557, 0.1116737499833107, 0.11477821320295334, 0.11810886114835739, 0.1149437427520752, 0.11433117091655731, 0.0965607538819313, 0.12227560579776764, 0.11355818063020706, 0.1087566688656807, 0.10826794803142548, 0.11286810040473938, 0.11224565654993057, 0.11632349342107773, 0.1170618012547493, 0.11092973500490189, 0.112909696996212, 0.11918684095144272, 0.11687207967042923, 0.1188674196600914, 0.11615397781133652, 0.11823432892560959, 0.12025945633649826, 0.11800963431596756, 0.11698980629444122, 0.12102765589952469, 0.10348210483789444, 0.11922574788331985, 0.11670947074890137, 0.11372196674346924, 0.10227292776107788, 0.11230314522981644, 0.1170753613114357, 0.10586822032928467, 0.10097231715917587, 0.11368484795093536, 0.10988771915435791, 0.11425844579935074, 0.114642433822155, 0.11776746064424515, 0.11319702118635178, 0.11539404094219208, 0.12259626388549805, 0.10030855983495712, 0.1049252301454544, 0.11364807933568954, 0.11864218860864639, 0.11102300137281418, 0.10930641740560532, 0.10777651518583298, 0.11769313365221024, 0.11706872284412384, 0.11241436749696732, 0.11439532786607742, 0.12028644979000092, 0.11462953686714172, 0.11677221208810806, 0.10565067082643509, 0.11649409681558609, 0.11346734315156937, 0.09732375293970108, 0.1110827699303627, 0.12256018072366714, 0.11679818481206894, 0.09665166586637497, 0.10772395133972168, 0.11614760756492615, 0.11922000348567963, 0.11282012611627579, 0.11483202129602432, 0.11859268695116043, 0.11852957308292389, 0.1094866469502449, 0.1068205013871193, 0.11565858870744705, 0.11564350128173828, 0.12245199084281921, 0.1140785738825798, 0.11965619027614594, 0.0974586084485054, 0.10869427770376205, 0.10862424224615097, 0.11301610618829727, 0.11320524662733078, 0.11985691636800766, 0.11217900365591049, 0.10644552856683731, 0.12223006784915924, 0.11308682709932327, 0.11742278933525085, 0.12198173999786377, 0.11876849085092545, 0.11002036929130554, 0.11772647500038147, 0.1108010858297348, 0.10620028525590897, 0.10246603935956955, 0.10762867331504822, 0.1170710101723671, 0.11409000307321548, 0.1060829609632492, 0.11355168372392654, 0.10872622579336166, 0.10463009029626846, 0.11131767183542252, 0.10763884335756302, 0.11769217997789383, 0.10632746666669846, 0.11157186329364777, 0.1088440790772438, 0.10607593506574631, 0.12010720372200012, 0.10932672023773193, 0.11735835671424866, 0.11757128685712814, 0.11162763088941574, 0.10881733149290085, 0.12306050956249237, 0.10939545184373856, 0.11703295260667801, 0.11015184968709946, 0.10837825387716293, 0.11752980947494507, 0.11479660123586655, 0.12110304832458496, 0.10770335048437119, 0.11554732173681259, 0.10783041268587112, 0.11465781927108765, 0.11959455162286758, 0.098509781062603, 0.11901908367872238, 0.11353326588869095, 0.10631183534860611, 0.10402799397706985, 0.1170787587761879, 0.11402596533298492, 0.10438259690999985, 0.11545480787754059, 0.11284097284078598, 0.12230654060840607, 0.11233889311552048, 0.1078040674328804, 0.1072988361120224, 0.11212920397520065, 0.11667947471141815, 0.12086128443479538, 0.11656031757593155, 0.10965380817651749, 0.10357936471700668, 0.12069668620824814, 0.1159576028585434, 0.11813147366046906, 0.11959938704967499, 0.11147993803024292, 0.12035500258207321, 0.11052580177783966, 0.1027139350771904, 0.12393752485513687, 0.10992629081010818, 0.10162672400474548, 0.11922426521778107, 0.10158584266901016, 0.1112799197435379, 0.11785975843667984, 0.10188888758420944, 0.11889883130788803, 0.11396066099405289, 0.11227738112211227, 0.11436943709850311, 0.11914757639169693, 0.11858155578374863, 0.10997413098812103, 0.10996438562870026, 0.11342296004295349, 0.11588132381439209, 0.10944206267595291, 0.12374278903007507, 0.10366347432136536, 0.1108776107430458, 0.124360091984272, 0.10514631122350693, 0.11390377581119537, 0.11472298204898834, 0.11583171039819717, 0.10966373234987259, 0.11963758617639542, 0.09401343762874603, 0.09853953868150711, 0.10791472345590591, 0.11119849234819412, 0.11305459588766098, 0.10760299861431122, 0.11362293362617493, 0.11032024770975113, 0.11630126088857651, 0.10796743631362915, 0.11660218983888626, 0.11098689585924149, 0.11643870174884796, 0.11722846329212189, 0.11716300994157791, 0.10299991816282272, 0.11431887000799179, 0.10649014264345169, 0.12093395739793777, 0.12194311618804932, 0.10787702351808548, 0.12056544423103333, 0.11589910835027695, 0.11035289615392685, 0.11710996180772781, 0.11383766680955887, 0.12077365070581436, 0.12180924415588379, 0.11902924627065659, 0.11369194835424423, 0.12082279473543167, 0.10327017307281494, 0.10583748668432236, 0.1112990751862526, 0.11245209723711014, 0.10483065992593765, 0.1141490563750267, 0.11203210800886154, 0.11032947152853012, 0.11153649538755417, 0.10447417944669724, 0.11372512578964233, 0.116912841796875, 0.11316965520381927, 0.10969201475381851, 0.11921921372413635, 0.11999841779470444, 0.11699683219194412, 0.11451312154531479, 0.11735490709543228, 0.11699283868074417, 0.1182861179113388, 0.10557719320058823, 0.11154484003782272, 0.11163581162691116, 0.10991640388965607, 0.10369494557380676, 0.09373748302459717, 0.11170244961977005, 0.12072048336267471, 0.12032780796289444, 0.12052556127309799, 0.10942096263170242, 0.10521458834409714, 0.1128372922539711, 0.10284055024385452, 0.10270152986049652, 0.10435763746500015, 0.11180852353572845, 0.11282540112733841, 0.11054813861846924, 0.11773572862148285, 0.11564373970031738, 0.11588780581951141, 0.11404632031917572, 0.12168822437524796, 0.10980165749788284, 0.11752644926309586, 0.1121315062046051, 0.11092007905244827, 0.10852812975645065, 0.09911122918128967, 0.10415313392877579, 0.1162523478269577, 0.10513047873973846, 0.11151155084371567, 0.12081120908260345, 0.1192823275923729, 0.11281203478574753, 0.11664260178804398, 0.11629732698202133, 0.11563777178525925, 0.12143512815237045, 0.11005515605211258, 0.10975687205791473, 0.10794231295585632, 0.11301989108324051, 0.11002077162265778, 0.11032446473836899, 0.11219202727079391, 0.10467280447483063, 0.11552303284406662, 0.1082741990685463, 0.10866261273622513, 0.11762461811304092, 0.1140969917178154, 0.11876412481069565, 0.10508376359939575, 0.11971450597047806, 0.12001203745603561, 0.11792460829019547, 0.11266165971755981, 0.11872390657663345, 0.1193283423781395, 0.11595176160335541, 0.1146605834364891, 0.11708135157823563, 0.12355980277061462, 0.11128891259431839, 0.11871384084224701, 0.1200701892375946, 0.12307225912809372, 0.10978076606988907, 0.11294814944267273, 0.10379555076360703, 0.11943210661411285, 0.11001624912023544, 0.11410689353942871, 0.12367052584886551, 0.11051393300294876, 0.1199631616473198, 0.11193781346082687, 0.10450096428394318, 0.11563464254140854, 0.10944220423698425, 0.1079704537987709, 0.11506792902946472, 0.10399997979402542, 0.10897461324930191, 0.11593199521303177, 0.11750342696905136, 0.11382734775543213, 0.11807094514369965, 0.12258145958185196, 0.11726183444261551, 0.11767589300870895, 0.11088185757398605, 0.11192923784255981, 0.10934901982545853, 0.09638505429029465, 0.12045008689165115, 0.11557980626821518, 0.1194877177476883, 0.11628123372793198, 0.11866360157728195, 0.11581935733556747, 0.11514921486377716, 0.11375687271356583, 0.1112697646021843, 0.10860831290483475, 0.11830742657184601, 0.11564850062131882, 0.1107281967997551, 0.12015163153409958, 0.11819396167993546, 0.11721467226743698, 0.11581852287054062, 0.11227110773324966, 0.11163099110126495, 0.11719948798418045, 0.10970164090394974, 0.10711433738470078, 0.11634021997451782, 0.11649515479803085, 0.11747895926237106, 0.11939296871423721, 0.11368933320045471, 0.11525072902441025, 0.12292800098657608, 0.1165803074836731, 0.11211449652910233, 0.10808775573968887, 0.10867279022932053, 0.11538714915513992, 0.11444438993930817, 0.11209506541490555, 0.11970838904380798, 0.11214794218540192, 0.12040653079748154, 0.10610450804233551, 0.11834665387868881, 0.1158217340707779, 0.11504568159580231, 0.11564779281616211, 0.10573841631412506, 0.11000555008649826, 0.10519213229417801, 0.11483074724674225, 0.12130629271268845, 0.11751659214496613, 0.12235233932733536, 0.11986727267503738, 0.11825260519981384, 0.10581661015748978, 0.11832126975059509, 0.11887068301439285, 0.11286894977092743, 0.11290889233350754, 0.11686893552541733, 0.11538305133581161, 0.09631585329771042, 0.12069672346115112, 0.08532372117042542, 0.11422533541917801, 0.1229187622666359, 0.09561026096343994, 0.10687687993049622, 0.1171749159693718, 0.10399798303842545, 0.11132428050041199, 0.11618181318044662, 0.11747150868177414, 0.11776319891214371, 0.10423330217599869, 0.11874737590551376, 0.11069055646657944, 0.11800480633974075, 0.1107814684510231, 0.1216050460934639, 0.10666697472333908, 0.10836242139339447, 0.11835707724094391, 0.11541970819234848, 0.10837666690349579, 0.11650175601243973, 0.1231658086180687, 0.11057353764772415, 0.11818110942840576, 0.11514853686094284, 0.10959237068891525, 0.11420996487140656, 0.12399094551801682, 0.11590620130300522, 0.10690440982580185, 0.11524837464094162, 0.10644876211881638, 0.10157617181539536, 0.12011914700269699, 0.09479285031557083, 0.1245611384510994, 0.10820019990205765, 0.11080813407897949, 0.10736247152090073, 0.10248734802007675, 0.11635132879018784, 0.12196888774633408, 0.11215704679489136, 0.1196170225739479, 0.11325468122959137, 0.11624554544687271, 0.11645480990409851, 0.11467525362968445, 0.11437023431062698, 0.11206357181072235, 0.11464253067970276, 0.12055084854364395, 0.10532976686954498, 0.10232073813676834, 0.11637384444475174, 0.1195884644985199, 0.10906218737363815, 0.10668893158435822, 0.11438578367233276, 0.11523542553186417, 0.10623357445001602, 0.11267925798892975, 0.11032900959253311, 0.11331978440284729, 0.10317899286746979, 0.11462462693452835, 0.11348224431276321, 0.11254208534955978, 0.11666668206453323, 0.11473943293094635, 0.1100616306066513, 0.10523148626089096, 0.11481154710054398, 0.12276072800159454, 0.10462640970945358, 0.11225482076406479, 0.11590403318405151, 0.11353471130132675, 0.11035416275262833, 0.11854919791221619, 0.11806970089673996, 0.12115912139415741, 0.11202175170183182, 0.10412174463272095, 0.09813638031482697, 0.1116311177611351, 0.11561799794435501, 0.11818628758192062, 0.11324245482683182, 0.11474637687206268, 0.10759404301643372, 0.11437822878360748, 0.12045910209417343, 0.11332342028617859, 0.11266758292913437, 0.1121157631278038, 0.09694423526525497, 0.11273814737796783, 0.11709580570459366, 0.10711251199245453, 0.11331071704626083, 0.11532258987426758, 0.1087813600897789, 0.10968223214149475, 0.10855069011449814, 0.11597630381584167, 0.11060307174921036, 0.11568615585565567, 0.12035129219293594, 0.12016826122999191, 0.11338935047388077, 0.11487995088100433, 0.11828695982694626, 0.10765289515256882, 0.11296392977237701, 0.1205194965004921, 0.10858917981386185, 0.12028826028108597, 0.10990010201931, 0.10156063735485077, 0.11725427955389023, 0.11488857120275497, 0.11883550137281418, 0.11892910301685333, 0.11393237859010696, 0.11101130396127701, 0.11549391597509384, 0.12004601210355759, 0.10958575457334518, 0.1061294674873352, 0.11638206988573074, 0.12345775216817856, 0.11684529483318329, 0.10200387239456177, 0.117368683218956, 0.11406057327985764, 0.11794085800647736, 0.11653345823287964, 0.10403846204280853, 0.1140022799372673, 0.11286304146051407, 0.11635132879018784, 0.11197749525308609, 0.11158571392297745, 0.11998868733644485, 0.12234634906053543, 0.11967331916093826, 0.11155290901660919, 0.11397159844636917, 0.09502845257520676, 0.1034337654709816, 0.09748568385839462, 0.11729905009269714, 0.11348257213830948, 0.11127962917089462, 0.10251860320568085, 0.09997396916151047, 0.1221390813589096, 0.11529712378978729, 0.11773130297660828, 0.11550088971853256, 0.10887961834669113, 0.11236200481653214, 0.11166680604219437, 0.10165707767009735, 0.11713317781686783, 0.11358067393302917, 0.11129254847764969, 0.11692358553409576, 0.1137031838297844, 0.11596927046775818, 0.12034597992897034, 0.11058784276247025, 0.11064302176237106, 0.11409852653741837, 0.10897547751665115, 0.11477168649435043, 0.11979680508375168, 0.09907680749893188, 0.1196151003241539, 0.10008861869573593, 0.11608246713876724, 0.11490406095981598, 0.11988019943237305, 0.10983891785144806, 0.11140117049217224, 0.10875606536865234, 0.11762253940105438, 0.11302851885557175, 0.11586475372314453, 0.11169648170471191, 0.11503804475069046, 0.11227359622716904, 0.10440949350595474, 0.11412505060434341, 0.1065855473279953, 0.11080806702375412, 0.118992879986763, 0.11907228827476501, 0.11496805399656296, 0.11971735209226608, 0.10671941190958023, 0.11662867665290833, 0.10199356824159622, 0.11705987900495529, 0.11870015412569046, 0.1155938133597374, 0.12039106339216232, 0.12140623480081558, 0.12328606843948364, 0.1164732426404953, 0.11197367310523987, 0.11748439073562622, 0.116682268679142, 0.11160347610712051, 0.11578048765659332, 0.11623280495405197, 0.11632949858903885, 0.11079832166433334, 0.11032865941524506, 0.10867273807525635, 0.10600302368402481, 0.12230236083269119, 0.11881288141012192, 0.11545006930828094, 0.1146208792924881, 0.11533727496862411, 0.11407189071178436, 0.1058618500828743, 0.09570849686861038, 0.11290570348501205, 0.11564627289772034, 0.11449528485536575, 0.10510372370481491, 0.11479019373655319, 0.10503236204385757, 0.09425406903028488, 0.11117526888847351, 0.11808868497610092, 0.10299456119537354, 0.10595028847455978, 0.10814657062292099, 0.11450682580471039, 0.11777273565530777, 0.10399292409420013, 0.09621129930019379, 0.11411945521831512, 0.10778368264436722, 0.11533705145120621, 0.11940212547779083, 0.10334905236959457, 0.1111847460269928, 0.1220577135682106, 0.12034577131271362, 0.11184544116258621, 0.10822383314371109, 0.09923417121171951, 0.10513212531805038, 0.10876529663801193, 0.11389424651861191, 0.11582314968109131, 0.11256685107946396, 0.11372672766447067, 0.1191745176911354, 0.11732207238674164, 0.12124566733837128, 0.09324117749929428, 0.12456295639276505, 0.10946904867887497, 0.11719994992017746, 0.11887023597955704, 0.1167878806591034, 0.11767833679914474, 0.11095084249973297, 0.10844772309064865, 0.11516942083835602, 0.11723719537258148, 0.11421718448400497, 0.10771027952432632, 0.1132587119936943, 0.10592520982027054, 0.11449307948350906, 0.10081633180379868, 0.11890947073698044, 0.1192491352558136, 0.11491040140390396, 0.10919850319623947, 0.1080629751086235, 0.11570440977811813, 0.11540117114782333, 0.10672227293252945, 0.10606758296489716, 0.11026746779680252, 0.11283822357654572, 0.11870882660150528, 0.11942236125469208, 0.11365102976560593, 0.11236510425806046, 0.11259929090738297, 0.1115865483880043, 0.1131429523229599, 0.12023977190256119, 0.1159774512052536, 0.11949411779642105, 0.11996296048164368, 0.1136060431599617, 0.11616170406341553, 0.12508034706115723, 0.11642234772443771, 0.11353723704814911, 0.11827190965414047, 0.10661941021680832, 0.11353909969329834, 0.11200563609600067, 0.11590965837240219, 0.11684352159500122, 0.1167740598320961, 0.11489977687597275, 0.10434850305318832, 0.10624877363443375, 0.10337761789560318, 0.11630584299564362, 0.11473547667264938, 0.11384636908769608, 0.09664063900709152, 0.11837442219257355, 0.11432770639657974, 0.11130773276090622, 0.11901896446943283, 0.12131674587726593, 0.11914064735174179, 0.1179356724023819, 0.1176108717918396, 0.1151941567659378, 0.08811601251363754, 0.10708165913820267, 0.10698990523815155, 0.11081723123788834, 0.11069056391716003, 0.11387436091899872, 0.11314166337251663, 0.10952279716730118, 0.09886393696069717, 0.10916460305452347, 0.11699596792459488, 0.10734786838293076, 0.12410562485456467, 0.11373934894800186, 0.11525855213403702, 0.11168701946735382, 0.11151978373527527, 0.11977315694093704, 0.11489664763212204, 0.11592211574316025, 0.10835202783346176, 0.12104421108961105, 0.10613185912370682, 0.11493843048810959, 0.11673261970281601, 0.10401589423418045, 0.1206444725394249, 0.11166641861200333, 0.11450522392988205, 0.10468084365129471, 0.1191147193312645, 0.09855352342128754, 0.11433305591344833, 0.10621621459722519, 0.11597634106874466, 0.11459533125162125, 0.11802346259355545, 0.10899035632610321, 0.10688243061304092, 0.11803019791841507, 0.12495603412389755, 0.10315565764904022, 0.10496816784143448, 0.11148561537265778, 0.1031266301870346, 0.10643326491117477, 0.12096715718507767, 0.11536580324172974, 0.10534985363483429, 0.12315737456083298, 0.10160898417234421, 0.1079176515340805, 0.10021437704563141, 0.10251059383153915, 0.11233390122652054, 0.12401852011680603, 0.11183824390172958, 0.11862454563379288, 0.12186682969331741, 0.11506099253892899, 0.11930502206087112, 0.11692347377538681, 0.10969013720750809, 0.10520488768815994, 0.11394007503986359, 0.1037282720208168, 0.11001624912023544, 0.11004310846328735, 0.12108677625656128, 0.10672760754823685, 0.10925228148698807, 0.11457314342260361, 0.10164839774370193, 0.10786350816488266, 0.10451314598321915, 0.10753780603408813, 0.12395162135362625, 0.10760680586099625, 0.10728832334280014, 0.09917392581701279, 0.11607862263917923, 0.09716520458459854, 0.11934351176023483, 0.11366509646177292, 0.11529826372861862, 0.12298084050416946, 0.12198615819215775, 0.12116533517837524, 0.11859383434057236, 0.11254920065402985, 0.11690983921289444, 0.1173979640007019, 0.11113320291042328, 0.10967796295881271, 0.10794013738632202, 0.11742323637008667, 0.1115744560956955, 0.11420794576406479, 0.11459125578403473, 0.11898057907819748, 0.11886947602033615, 0.11362683027982712, 0.12508471310138702, 0.11703097075223923, 0.11950541287660599, 0.11004551500082016, 0.11669931560754776, 0.11636345088481903, 0.1124805435538292, 0.10043483227491379, 0.1190374493598938, 0.11415234953165054, 0.10825063288211823, 0.1169772669672966, 0.10099736601114273, 0.1084844097495079, 0.11440280079841614, 0.11595959961414337, 0.11665593832731247, 0.11493684351444244, 0.11478205025196075, 0.11548896878957748, 0.12506838142871857, 0.11873191595077515, 0.11903662234544754, 0.10737916082143784, 0.11801154166460037, 0.10685717314481735, 0.117238849401474, 0.11806228011846542, 0.10375598818063736, 0.11977720260620117, 0.10615978389978409, 0.10612860321998596, 0.11448080092668533, 0.11338373273611069, 0.11600501835346222, 0.11577226221561432, 0.12105744332075119, 0.11486051231622696, 0.11361739784479141, 0.12171417474746704, 0.10308094322681427, 0.11942131072282791, 0.12101567536592484, 0.11696979403495789, 0.10903794318437576, 0.1172625795006752, 0.109748475253582, 0.11030811816453934, 0.11202628165483475, 0.11244311183691025, 0.11339888721704483, 0.11557484418153763, 0.11264165490865707, 0.11608608067035675, 0.11352399736642838, 0.11419419199228287, 0.110928975045681, 0.10847487300634384, 0.10056192427873611, 0.11601825803518295, 0.12098658829927444, 0.10640160739421844, 0.10748249292373657, 0.11220667511224747, 0.11255978047847748, 0.10781791061162949, 0.11790768057107925, 0.12406175583600998, 0.09971341490745544, 0.10747087746858597, 0.11752399057149887, 0.10668347775936127, 0.11657612025737762, 0.1184510737657547, 0.1174294650554657, 0.1184326559305191, 0.10545268654823303, 0.1190788745880127, 0.11517404019832611, 0.11542963236570358, 0.11385999619960785, 0.1089572086930275, 0.10915856063365936, 0.10424616187810898, 0.12458520382642746, 0.1006355881690979, 0.11615868657827377, 0.10602723062038422, 0.0897267609834671, 0.11311060190200806, 0.11486084014177322, 0.10979682952165604, 0.10873477905988693, 0.11728961020708084, 0.11233346909284592, 0.11873557418584824, 0.1114640086889267, 0.11834432929754257, 0.1151980608701706, 0.11999287456274033, 0.10753732919692993, 0.11658909916877747, 0.09924464672803879, 0.1151452586054802, 0.12522046267986298, 0.117390938103199, 0.1207074448466301, 0.10966596007347107, 0.11807537823915482, 0.12195833772420883, 0.11040002107620239, 0.11861523985862732, 0.11724770069122314, 0.12035787850618362, 0.11814859509468079, 0.11254991590976715, 0.11247935146093369, 0.10024680197238922, 0.09645437449216843, 0.11344729363918304, 0.11739254742860794, 0.11700505018234253, 0.11569427698850632, 0.11820860207080841, 0.09782058745622635, 0.10897497087717056, 0.11465391516685486, 0.11487012356519699, 0.11057710647583008, 0.10305815190076828, 0.11183574050664902, 0.1182410940527916, 0.11637993901968002, 0.09256989508867264, 0.10568584501743317, 0.11014632135629654, 0.11240112036466599, 0.1001538410782814, 0.10705321282148361, 0.1168082132935524, 0.11935495585203171, 0.1080896183848381, 0.11818008869886398, 0.10951267182826996, 0.1121712327003479, 0.10590563714504242, 0.11902903765439987, 0.11976867914199829, 0.11854969710111618, 0.11646368354558945, 0.11003216356039047, 0.11631274968385696, 0.12089479714632034, 0.1049039289355278, 0.11155018955469131, 0.11604718118906021, 0.10652768611907959, 0.10297159105539322, 0.11238926649093628, 0.11482914537191391, 0.11980805546045303, 0.09971201419830322, 0.11574315279722214, 0.11870627850294113, 0.10467051714658737, 0.10775487869977951, 0.10216833651065826, 0.11520460993051529, 0.11790113896131516, 0.10934298485517502, 0.12048930674791336, 0.12107012420892715, 0.11217515915632248, 0.12100726366043091, 0.1121140867471695, 0.10364708304405212, 0.11354129016399384, 0.12057539075613022, 0.09782548993825912, 0.10582760721445084, 0.10241935402154922, 0.11458776891231537, 0.11767073720693588, 0.10890503972768784, 0.11541129648685455, 0.11233837157487869, 0.12038060277700424, 0.09893345832824707, 0.11303181201219559, 0.09599144011735916, 0.11388290673494339, 0.11391560733318329, 0.10235556215047836, 0.11771028488874435, 0.11988653987646103, 0.10365135222673416, 0.10775523632764816, 0.11584781855344772, 0.11264017224311829, 0.10686656087636948, 0.10640723258256912, 0.11736967414617538, 0.10787655413150787, 0.11797359585762024, 0.10261162370443344, 0.12192460149526596, 0.10904423892498016, 0.12102410197257996, 0.11036848276853561, 0.1055300310254097, 0.12084086984395981, 0.10903625935316086, 0.11623526364564896, 0.12277671694755554, 0.11086296290159225, 0.1117263063788414, 0.12107487767934799, 0.11827418953180313, 0.11027542501688004, 0.11823485791683197, 0.11518926173448563, 0.10638568550348282, 0.11334224045276642, 0.11645074188709259, 0.1117786392569542, 0.1172795370221138, 0.11187499016523361, 0.11604621261358261, 0.10976437479257584, 0.11870276182889938, 0.11047925055027008, 0.10455039143562317, 0.11222999542951584, 0.11435527354478836, 0.11425754427909851, 0.11647504568099976, 0.11486306041479111, 0.11619453132152557, 0.11213190853595734, 0.117986299097538, 0.1144414097070694, 0.11941332370042801, 0.10863158851861954, 0.11381252110004425, 0.11321456730365753, 0.11915305256843567, 0.10364483296871185, 0.11603053659200668, 0.11475688964128494, 0.11754579842090607, 0.11333509534597397, 0.1186261847615242, 0.1184123083949089, 0.11895569413900375, 0.1177009716629982, 0.1171693503856659, 0.12017534673213959, 0.10497570037841797, 0.11979761719703674, 0.09102571755647659, 0.11357995122671127, 0.10347656160593033, 0.110035739839077, 0.11544046550989151, 0.10424903780221939, 0.10980600863695145, 0.12115456908941269, 0.11668669432401657, 0.11225692927837372, 0.12003219872713089, 0.1118195652961731, 0.11799953877925873, 0.1174200177192688, 0.10315202921628952, 0.10518524795770645, 0.11657015234231949, 0.10701071470975876, 0.11794112622737885, 0.10691193491220474, 0.11377983540296555, 0.1146329715847969, 0.11191993206739426, 0.11462511867284775, 0.10550102591514587, 0.11805068701505661, 0.1154588982462883, 0.0986805185675621, 0.11425459384918213, 0.10977447777986526, 0.11834100633859634, 0.11582965403795242, 0.10761421173810959, 0.10364998877048492, 0.12370999157428741, 0.11245854943990707, 0.10904506593942642, 0.10731474310159683, 0.11516136676073074, 0.11229432374238968, 0.11885784566402435, 0.10799660533666611, 0.10948558896780014, 0.09313638508319855, 0.11864437907934189, 0.09636712074279785, 0.11237350851297379, 0.11251400411128998, 0.1125735267996788, 0.11476660519838333, 0.11810848116874695, 0.11584768444299698, 0.11742570251226425, 0.10006029903888702, 0.11617843061685562, 0.09862809628248215, 0.10919801145792007, 0.10110015422105789, 0.1145499125123024, 0.10616547614336014, 0.11985038965940475, 0.11869481950998306, 0.1155199185013771, 0.10884127765893936, 0.11780913174152374, 0.11679469794034958, 0.11400677263736725, 0.10708530247211456, 0.12297872453927994, 0.11565045267343521, 0.12002190202474594, 0.1146712675690651, 0.10865380614995956, 0.11484400182962418, 0.11669708788394928, 0.11871802061796188, 0.11121636629104614, 0.11300411075353622, 0.11202329397201538, 0.11405696719884872, 0.10719209909439087, 0.1061800941824913, 0.117122121155262, 0.11238295584917068, 0.11631975322961807, 0.11960336565971375, 0.1191091388463974, 0.11224482208490372, 0.11839718371629715, 0.12029516696929932, 0.11891429871320724, 0.1137165054678917, 0.11393434554338455, 0.12329743057489395, 0.10356730222702026, 0.11756516247987747, 0.09122002869844437, 0.12457578629255295, 0.12065654247999191, 0.1212652176618576, 0.10575295984745026, 0.11079321056604385, 0.11868010461330414, 0.12097223848104477, 0.10458541661500931, 0.11709866672754288, 0.11652303487062454, 0.09914194792509079, 0.10837391763925552, 0.11656228452920914, 0.12152252346277237, 0.11105962842702866, 0.10350881516933441, 0.10297601670026779, 0.11489563435316086, 0.11225013434886932, 0.11618373543024063, 0.10284831374883652, 0.1079266369342804, 0.11884170770645142, 0.1094275563955307, 0.10788001120090485, 0.11677203327417374, 0.11255086213350296, 0.11638571321964264, 0.10619357228279114, 0.11634598672389984, 0.11908786743879318, 0.10841578245162964, 0.11801516264677048, 0.10174231231212616, 0.11453047394752502, 0.1100485548377037, 0.11540499329566956, 0.11277250200510025, 0.1045890524983406, 0.11515264958143234, 0.11938399821519852, 0.11041667312383652, 0.12094911932945251, 0.12181344628334045, 0.11611609905958176, 0.11770496517419815, 0.11514399200677872, 0.11110509186983109, 0.11602850258350372, 0.11717524379491806, 0.1098002940416336, 0.10263710469007492, 0.1082979217171669, 0.11058707535266876, 0.10796322673559189, 0.11343909054994583, 0.11469902098178864, 0.12124304473400116, 0.10855584591627121, 0.11480840295553207, 0.11443999409675598, 0.10962671786546707, 0.11690669506788254, 0.11556126922369003, 0.11409909278154373, 0.12225600332021713, 0.10731346160173416, 0.11492512375116348, 0.11800319701433182, 0.11645671725273132, 0.11224392801523209, 0.11177078634500504, 0.10520128905773163, 0.11024586111307144, 0.1097663938999176, 0.10958998650312424, 0.11966396868228912, 0.09467967599630356, 0.117431640625, 0.10956675559282303, 0.10386969894170761, 0.1165861114859581, 0.11652621626853943, 0.11899050325155258, 0.11380010098218918, 0.10914745181798935, 0.12179744243621826, 0.1176336482167244, 0.11139930784702301, 0.11287123709917068, 0.09921153634786606, 0.1229810044169426, 0.11330219358205795, 0.11013873666524887, 0.11069397628307343, 0.09900730103254318, 0.12210818380117416, 0.11324212700128555, 0.1196795180439949, 0.10765601694583893, 0.11728563904762268, 0.11566871404647827, 0.11310523003339767, 0.09981586784124374, 0.11487395316362381, 0.11606127768754959, 0.11669417470693588, 0.11738020181655884, 0.10780514031648636, 0.10415820777416229, 0.11721492558717728, 0.11718414723873138, 0.10904475301504135, 0.11659053713083267, 0.11549150198698044, 0.10950342565774918, 0.10245328396558762, 0.11860636621713638, 0.11497703939676285, 0.11367456614971161, 0.1200704500079155, 0.11679693311452866, 0.1154627576470375, 0.1047746017575264, 0.11780638247728348, 0.11183501034975052, 0.11523853987455368, 0.11499331146478653, 0.11205453425645828, 0.11310819536447525, 0.12040242552757263, 0.11472588032484055, 0.10947035253047943, 0.11456277221441269, 0.11521496623754501, 0.10607493668794632, 0.1136590912938118, 0.12410210072994232, 0.11911030858755112, 0.12092921882867813, 0.100197434425354, 0.11394946277141571, 0.11964336782693863, 0.11917287111282349, 0.11024672538042068, 0.1127009242773056, 0.11252087354660034, 0.10678605735301971, 0.11996345967054367, 0.1068718284368515, 0.11120326071977615, 0.11215236037969589, 0.10913335531949997, 0.1040329709649086, 0.11471361666917801, 0.11979654431343079, 0.1167239248752594, 0.11893560737371445, 0.12075484544038773, 0.11389424651861191, 0.12214265018701553, 0.10413283854722977, 0.12107611447572708, 0.11589299887418747, 0.11607203632593155, 0.11603997647762299, 0.11689306050539017, 0.1228984072804451, 0.10913335531949997, 0.1101728156208992, 0.10866820067167282, 0.11285846680402756, 0.10646501183509827, 0.11172355711460114, 0.11004716902971268, 0.11687733232975006, 0.10456003993749619, 0.09591007232666016, 0.11938484758138657, 0.11108405888080597, 0.12046339362859726, 0.11010241508483887, 0.12288551032543182, 0.12369661033153534, 0.11526937037706375, 0.12208591401576996, 0.10173124819993973, 0.1051555871963501, 0.11557484418153763, 0.10983923822641373, 0.11891604959964752, 0.111408531665802, 0.11820806562900543, 0.11350145190954208, 0.10501252114772797, 0.11190912872552872, 0.11328468471765518, 0.10438190400600433, 0.11981546878814697, 0.10619598627090454, 0.11360085010528564, 0.11539877206087112, 0.11603950709104538, 0.11819105595350266, 0.10488290339708328, 0.105081707239151, 0.10275263339281082, 0.1156352087855339, 0.118323415517807, 0.11276286840438843, 0.12016228586435318, 0.11473648250102997, 0.11530761420726776, 0.11884506791830063, 0.10638286918401718, 0.11872946470975876, 0.11696597188711166, 0.10917706787586212, 0.12098988890647888, 0.10856490582227707, 0.11465998739004135, 0.11133386939764023, 0.11822088807821274, 0.12020069360733032, 0.11769130080938339, 0.11857087165117264, 0.11648483574390411, 0.09835333377122879, 0.10908827930688858, 0.12088356167078018, 0.10814670473337173, 0.10997895151376724, 0.11663522571325302, 0.11233599483966827, 0.1238551214337349, 0.1061912253499031, 0.11848805099725723, 0.1175832748413086, 0.11763712018728256, 0.11269266158342361, 0.11988116055727005, 0.11852896958589554, 0.11373446881771088, 0.10315421223640442, 0.10807283967733383, 0.1142064705491066, 0.11656730622053146, 0.10754314064979553, 0.11972767114639282, 0.11412506550550461, 0.12033899128437042, 0.11024339497089386, 0.10597407817840576, 0.11951853334903717, 0.10326860100030899, 0.10468786209821701, 0.1196550726890564, 0.12052331864833832, 0.1127508357167244, 0.11513795703649521, 0.11127495765686035, 0.12295462191104889, 0.11376894265413284, 0.11754295974969864, 0.11718697100877762, 0.10786792635917664, 0.11276771873235703, 0.10922373831272125, 0.11180884391069412, 0.11114076524972916, 0.11998016387224197, 0.10783493518829346, 0.11768276244401932, 0.10621747374534607, 0.11804472655057907, 0.10867541283369064, 0.11198322474956512, 0.1188289076089859, 0.11438364535570145, 0.10912241786718369, 0.11475820094347, 0.11374106258153915, 0.10435301065444946, 0.11478271335363388, 0.11113367229700089, 0.1173553317785263, 0.1216576099395752, 0.11290358006954193, 0.10961245000362396, 0.11842970550060272, 0.11163752526044846, 0.11438269168138504, 0.11240870505571365, 0.1096772626042366, 0.11894653737545013, 0.11220677196979523, 0.11882719397544861, 0.10976862162351608, 0.11560170352458954, 0.11733638495206833, 0.09324117749929428, 0.11737049371004105, 0.12206839770078659, 0.11224224418401718, 0.1124849021434784, 0.11516962945461273, 0.10329058766365051, 0.12132718414068222, 0.12233544141054153, 0.11427713930606842, 0.11319145560264587, 0.11489292234182358, 0.11839339882135391, 0.11366374045610428, 0.10360831022262573, 0.11367693543434143, 0.11179981380701065, 0.10625284910202026, 0.11676399409770966, 0.10922009497880936, 0.09648597985506058, 0.10387084633111954, 0.10777373611927032, 0.12269677966833115, 0.11450599879026413, 0.11667254567146301, 0.11871431022882462, 0.10046175122261047, 0.11738492548465729, 0.11890499293804169, 0.10676629096269608, 0.11218339949846268, 0.11528268456459045, 0.10817532986402512, 0.11264453083276749, 0.12459595501422882, 0.1195840910077095, 0.11639991402626038, 0.11883952468633652, 0.1167089194059372, 0.11770018935203552, 0.11688736826181412, 0.10943680256605148, 0.11674609035253525, 0.11818694323301315, 0.11609608680009842, 0.10976319760084152, 0.10579700022935867, 0.11705855280160904, 0.11516676098108292, 0.10927844047546387, 0.11331578344106674, 0.10591931641101837, 0.1082782968878746, 0.11981429904699326, 0.10926876217126846, 0.11490573734045029, 0.1129475012421608, 0.11437589675188065, 0.11137659102678299, 0.11178558319807053, 0.09678460657596588, 0.11989908665418625, 0.11863250285387039, 0.11835533380508423, 0.12082279473543167, 0.11249856650829315, 0.11531583219766617, 0.12169575691223145, 0.11177114397287369, 0.11701826006174088, 0.11231742054224014, 0.11036762595176697, 0.10490666329860687, 0.1149507537484169, 0.11422549933195114, 0.1181153729557991, 0.10821336507797241, 0.1176634430885315, 0.10638515651226044, 0.11387873440980911, 0.11772147566080093, 0.11816304922103882, 0.12093525379896164, 0.11657015234231949, 0.11537051945924759, 0.11510322988033295, 0.11836562305688858, 0.11335018277168274, 0.11759058386087418, 0.10965631902217865, 0.11938474327325821, 0.11499736458063126, 0.12446895241737366, 0.1048664078116417, 0.11815442144870758, 0.11205033212900162, 0.12342939525842667, 0.11645081639289856, 0.11111662536859512, 0.11936352401971817, 0.10724347084760666, 0.11150812357664108, 0.12144019454717636, 0.11334977298974991, 0.12209130823612213, 0.11905167996883392, 0.11730626970529556, 0.10912445932626724, 0.11880593746900558, 0.1051688939332962, 0.11438024789094925, 0.10308094322681427, 0.11127548664808273, 0.12329594045877457, 0.11762578785419464, 0.1141735091805458, 0.09803802520036697, 0.11564595252275467, 0.12040010839700699, 0.10204152017831802, 0.11653928458690643, 0.11082429438829422, 0.1168639287352562, 0.11236421763896942, 0.11848825216293335, 0.11593274772167206, 0.1128820925951004, 0.1061873510479927, 0.11741889268159866, 0.11546065658330917, 0.10922100394964218, 0.11336764693260193, 0.11606267839670181, 0.11800426244735718, 0.11274012178182602, 0.10577503591775894, 0.1211753562092781, 0.11435764282941818, 0.11277429014444351, 0.1152992695569992, 0.1089443489909172, 0.11011015623807907, 0.11402203887701035, 0.12180700153112411, 0.11824800819158554, 0.11331059783697128, 0.1019786149263382, 0.11661580204963684, 0.11897171288728714, 0.11584407836198807, 0.12160288542509079, 0.10665896534919739, 0.11975129693746567, 0.11313928663730621, 0.11887365579605103, 0.11797910183668137, 0.11126013100147247, 0.09674165397882462, 0.11317475885152817, 0.10476936399936676, 0.11097436398267746, 0.11989473551511765, 0.11120878905057907, 0.11657226830720901, 0.11673157662153244, 0.10314590483903885, 0.10663334280252457, 0.11501329392194748, 0.11905672401189804, 0.11010988801717758, 0.12014509737491608, 0.11674929410219193, 0.1038321778178215, 0.1200355663895607, 0.11942283064126968, 0.11943789571523666, 0.12120371311903, 0.112620510160923, 0.12012993544340134, 0.1166670173406601, 0.11810803413391113, 0.10179407149553299, 0.11542350798845291, 0.11533813178539276, 0.1143597736954689, 0.11680920422077179, 0.11353815346956253, 0.10717084258794785, 0.11313027143478394, 0.11526747792959213, 0.11501508206129074, 0.1142338216304779, 0.11306983977556229, 0.1136205866932869, 0.10655737668275833, 0.11562351137399673, 0.114026740193367, 0.11931706219911575, 0.11632774770259857, 0.11610530316829681, 0.12206291407346725, 0.10382147878408432, 0.11892599612474442, 0.1231272742152214, 0.1231723353266716, 0.10990377515554428, 0.11883070319890976, 0.11938297748565674, 0.11657371371984482, 0.11876924335956573, 0.09805227816104889, 0.120830237865448, 0.10588380694389343, 0.11444197595119476, 0.11986392736434937, 0.12182148545980453, 0.10784236341714859, 0.11759059131145477, 0.10526750981807709, 0.11897700279951096, 0.11705025285482407, 0.1152474507689476, 0.11612707376480103, 0.11142325401306152, 0.0943857803940773, 0.11885320395231247, 0.12069449573755264, 0.1037021353840828, 0.11307676136493683, 0.11945383995771408, 0.09255071729421616, 0.12067721039056778, 0.11659929901361465, 0.11635953187942505, 0.11872462183237076, 0.11983336508274078, 0.11021921783685684, 0.11776961386203766, 0.11163588613271713, 0.11502844095230103, 0.11565837264060974, 0.11843379586935043, 0.11823341995477676, 0.10251860320568085, 0.11865461617708206, 0.09783818572759628, 0.11809853464365005, 0.10630328208208084, 0.10800481587648392, 0.10798243433237076, 0.12366952747106552, 0.10315565764904022, 0.12051918357610703, 0.11175735294818878, 0.12215284258127213, 0.11602024734020233, 0.11292757838964462, 0.11469341069459915, 0.12220583111047745, 0.11415468901395798, 0.09978274255990982, 0.11145562678575516, 0.10652486979961395, 0.11994046717882156, 0.11434245854616165, 0.10561592131853104, 0.11445752531290054, 0.11127901077270508, 0.11074130982160568, 0.11284182220697403, 0.11234339326620102, 0.11722349375486374, 0.11424032598733902, 0.11103102564811707, 0.10902268439531326, 0.12276511639356613, 0.1127832904458046, 0.11587055027484894, 0.10637121647596359, 0.11821527779102325, 0.11809413135051727, 0.11360956728458405, 0.12029179185628891, 0.1135714128613472, 0.11094101518392563, 0.12056215107440948, 0.1171678975224495, 0.12026668339967728, 0.11309251189231873, 0.11010917276144028, 0.1111045852303505, 0.09626706689596176, 0.1125810518860817, 0.10843206197023392, 0.11133290827274323, 0.10984805971384048, 0.1057540625333786, 0.11879641562700272, 0.1090925931930542, 0.11226309090852737, 0.11266645044088364, 0.09684408456087112, 0.11164519935846329, 0.11266970634460449, 0.10956178605556488, 0.11828112602233887, 0.1085190549492836, 0.11224266141653061, 0.11968930810689926, 0.11780457943677902, 0.11549363285303116, 0.11447729915380478, 0.1143639013171196, 0.12517830729484558, 0.11919530481100082, 0.11247388273477554, 0.10802239179611206, 0.11193476617336273, 0.11140887439250946, 0.11310041695833206, 0.11798291653394699, 0.11187881976366043, 0.10240204632282257, 0.11725153028964996, 0.10999003797769547, 0.11327485740184784, 0.11316029727458954, 0.11583618074655533, 0.10378315299749374, 0.11196363717317581, 0.11572808772325516, 0.11369974166154861, 0.1139274537563324, 0.11959921568632126, 0.11845824122428894, 0.12330923229455948, 0.12328606843948364, 0.11384794116020203, 0.11459860950708389, 0.11233989149332047, 0.110874705016613, 0.12069672346115112, 0.12054269015789032, 0.0971376821398735, 0.11110499501228333, 0.11984867602586746, 0.11870674043893814, 0.11465565115213394, 0.11374006420373917, 0.10667134076356888, 0.09977998584508896, 0.11360406130552292, 0.12255057692527771, 0.11775589734315872, 0.106647789478302, 0.11150116473436356, 0.11142128705978394, 0.11083536595106125, 0.11992140859365463, 0.11496104300022125, 0.10535641759634018, 0.11716978251934052, 0.12239032238721848, 0.11821971088647842, 0.11378729343414307, 0.10364564508199692, 0.1235821396112442, 0.12016548961400986, 0.11112736910581589, 0.12085432559251785, 0.11304984241724014, 0.10058864206075668, 0.10998103022575378, 0.11159633845090866, 0.11781778931617737, 0.1159583255648613, 0.11058097332715988, 0.11284857243299484, 0.10074692219495773, 0.10937193036079407, 0.12075493484735489, 0.11278076469898224, 0.10741248726844788, 0.12160538882017136, 0.11809848994016647, 0.11846179515123367, 0.0960179790854454, 0.11418455094099045, 0.12108664214611053, 0.1148795410990715, 0.12124385684728622, 0.11902063339948654, 0.12320908159017563, 0.114430271089077, 0.09834304451942444, 0.11331959813833237, 0.12017782777547836, 0.11745340377092361, 0.11250893026590347, 0.11006268113851547, 0.11941250413656235, 0.1157572865486145, 0.1164177805185318, 0.12109013646841049, 0.11042582988739014, 0.10435687005519867, 0.0963008925318718, 0.1104508712887764, 0.12168023735284805, 0.12086151540279388, 0.11528747528791428, 0.10560214519500732, 0.1223406195640564, 0.11462714523077011, 0.11321427673101425, 0.11168549954891205, 0.11549616605043411, 0.11435412615537643, 0.11856715381145477, 0.11279300600290298, 0.11851715296506882, 0.11717928200960159, 0.11580623686313629, 0.12133349478244781, 0.11203517019748688, 0.11403623223304749, 0.12050347775220871, 0.1129164919257164, 0.11702049523591995, 0.11053609102964401, 0.12064860761165619, 0.11785893142223358, 0.11119693517684937, 0.11661459505558014, 0.10250825434923172, 0.1114729717373848, 0.11690118163824081, 0.1115633025765419, 0.11761903017759323, 0.11565489321947098, 0.11834637075662613, 0.0958826094865799, 0.10909204930067062, 0.1161140501499176, 0.11778660863637924, 0.1171514168381691, 0.11638256162405014, 0.10792609304189682, 0.11737117916345596, 0.10713713616132736, 0.11508926004171371, 0.11378918588161469, 0.11909028142690659, 0.11683013290166855, 0.11505226045846939, 0.10543601214885712, 0.11608989536762238, 0.10779872536659241, 0.1206449642777443, 0.12426435202360153, 0.11444849520921707, 0.11857187747955322, 0.1177295595407486, 0.1169508621096611, 0.10783701390028, 0.12244144082069397, 0.11667236685752869, 0.10665661841630936, 0.11809145659208298, 0.1181880384683609, 0.12197927385568619, 0.11425670236349106, 0.0879482850432396, 0.10879478603601456, 0.12040897458791733, 0.10510288923978806, 0.11593812704086304, 0.102051742374897, 0.11821296066045761, 0.1175977811217308, 0.10353288799524307, 0.11512454599142075, 0.11824329197406769, 0.11792463809251785, 0.10294478386640549, 0.11978530138731003, 0.10845838487148285, 0.10236876457929611, 0.11005318909883499, 0.12121158838272095, 0.12134374678134918, 0.09935779124498367, 0.10089277476072311, 0.10909240692853928, 0.11676657944917679, 0.11562702804803848, 0.11639684438705444, 0.11423103511333466, 0.11706528812646866, 0.10340975224971771, 0.11814176291227341, 0.1128169447183609, 0.11395038664340973, 0.09295684844255447, 0.10979124158620834, 0.11251358687877655, 0.10869444906711578, 0.10009102523326874, 0.103753462433815, 0.11602985858917236, 0.10867676138877869, 0.10591945052146912, 0.12103057652711868, 0.12101969122886658, 0.1112980917096138, 0.11415992677211761, 0.11279352754354477, 0.09863720089197159, 0.11687988042831421, 0.12372628599405289, 0.11195919662714005, 0.1119779422879219, 0.11027588695287704, 0.11179179698228836, 0.11991123110055923, 0.10428814589977264, 0.1200164407491684, 0.12027142196893692, 0.112302765250206, 0.11658497899770737, 0.11114076524972916, 0.11776404827833176, 0.10178258270025253, 0.11789164692163467, 0.10020985454320908, 0.12295687943696976, 0.10846755653619766, 0.11581245064735413, 0.11456914991140366, 0.1170518770813942, 0.11216602474451065, 0.11794952303171158, 0.11392877995967865, 0.12070857733488083, 0.11380010098218918, 0.1186099722981453, 0.11469060927629471, 0.10723555833101273, 0.11171301454305649, 0.12194200605154037, 0.1055283322930336, 0.09380005300045013, 0.11715026944875717, 0.11181554198265076, 0.11078321188688278, 0.12222937494516373, 0.1155318096280098, 0.1126445084810257, 0.10025695711374283, 0.10930290073156357, 0.11441773921251297, 0.11837232857942581, 0.11661387234926224, 0.10892979055643082, 0.09656339883804321, 0.11288908869028091, 0.11078117042779922, 0.11483197659254074, 0.11293386667966843, 0.10503856837749481, 0.11646483838558197, 0.11853295564651489, 0.12015069276094437, 0.10767516493797302, 0.1165308877825737, 0.10960791260004044, 0.12098222970962524, 0.11645551770925522, 0.12003053724765778, 0.1102214828133583, 0.12157300114631653, 0.1185624971985817, 0.10533604770898819, 0.11342296004295349, 0.11096429079771042, 0.11224072426557541, 0.10945678502321243, 0.1144578754901886, 0.11333649605512619, 0.1171506717801094, 0.10518442839384079, 0.12550589442253113, 0.11388114839792252, 0.10920356214046478, 0.10886423289775848, 0.10923398286104202, 0.11986395716667175, 0.10758814215660095, 0.10958587378263474, 0.11775938421487808, 0.11848371475934982, 0.11654766649007797, 0.12001606076955795, 0.1212320551276207, 0.1085720881819725, 0.1018003597855568, 0.10853085666894913, 0.12087886780500412, 0.11269162595272064, 0.10450216382741928, 0.11411344259977341, 0.11035031825304031, 0.11462812125682831, 0.1042599007487297, 0.11762712150812149, 0.12251333147287369, 0.11978530138731003, 0.11317596584558487, 0.1017865538597107, 0.12160253524780273, 0.1121346727013588, 0.12031839042901993, 0.1055743619799614, 0.11552365124225616, 0.11566117405891418, 0.099139004945755, 0.11570339649915695, 0.11404355615377426, 0.10715442895889282, 0.1163531020283699, 0.10769360512495041, 0.09921153634786606, 0.1168481782078743, 0.11569865792989731, 0.11673503369092941, 0.11282006651163101, 0.09584365785121918, 0.11700183898210526, 0.11366570740938187, 0.10800657421350479, 0.1170334592461586, 0.12235233932733536, 0.09238166362047195, 0.11625991761684418, 0.11299871653318405, 0.11813034862279892, 0.11784061789512634, 0.12017110735177994, 0.11361496895551682, 0.11749715358018875, 0.11240272969007492, 0.11669362336397171, 0.11453161388635635, 0.11025410145521164, 0.10769082605838776, 0.11454930156469345, 0.12022409588098526, 0.1134638860821724, 0.10903819650411606, 0.11315435916185379, 0.11730078607797623, 0.10923460870981216, 0.11735222488641739, 0.10962968319654465, 0.09872210025787354, 0.11176923662424088, 0.11347989737987518, 0.1163141131401062, 0.10305684804916382, 0.11776559799909592, 0.1171749159693718, 0.11243816465139389, 0.12153679132461548, 0.12122461944818497, 0.11855977028608322, 0.10951194167137146, 0.11662743240594864, 0.12090238928794861, 0.09254595637321472, 0.11949575692415237, 0.11946838349103928, 0.11742131412029266, 0.10612120479345322, 0.1138361468911171, 0.11580446362495422, 0.12055773288011551, 0.11103574186563492, 0.11337653547525406, 0.12599866092205048, 0.10774285346269608, 0.11639755219221115, 0.11707136780023575, 0.11637244373559952, 0.11503799259662628, 0.10599611699581146, 0.11073564738035202, 0.10620739310979843, 0.11109556257724762, 0.11748765408992767, 0.10231443494558334, 0.11880496144294739, 0.11780189722776413, 0.11299365758895874, 0.11745243519544601, 0.1149677038192749, 0.11709172278642654, 0.11379186064004898, 0.11536777764558792, 0.12039344012737274, 0.11858882755041122, 0.11670633405447006, 0.11633067578077316, 0.1082763746380806, 0.11337368190288544, 0.10852870345115662, 0.11111586540937424, 0.12064992636442184, 0.1147301197052002, 0.11534678936004639, 0.1032831147313118, 0.11618473380804062, 0.12241589277982712, 0.11671609431505203, 0.10812930017709732, 0.12095823138952255, 0.12349198758602142, 0.11818770319223404, 0.1193920448422432, 0.11407197266817093, 0.10990267992019653, 0.1169181764125824, 0.1184248998761177, 0.11590374261140823, 0.10064288973808289, 0.11145543307065964, 0.1109645664691925, 0.09939701110124588, 0.10203223675489426, 0.11726699024438858, 0.11609379202127457, 0.12298960983753204, 0.12133125960826874, 0.12083066999912262, 0.11322949826717377, 0.11505592614412308, 0.10884925723075867, 0.11062020063400269, 0.11550816148519516, 0.09914247691631317, 0.12056147307157516, 0.10392133891582489, 0.11542534083127975, 0.11320514231920242, 0.11381042748689651, 0.11638045310974121, 0.10619743913412094, 0.11373956501483917, 0.1174052432179451, 0.11241353303194046, 0.110721156001091, 0.11268417537212372, 0.11343660205602646, 0.10048508644104004, 0.11740940064191818, 0.11462709307670593, 0.11197041720151901, 0.09830694645643234, 0.10559024661779404, 0.11486927419900894, 0.11894236505031586, 0.12037830799818039, 0.11991798132658005, 0.10835202783346176, 0.11385805904865265, 0.11483652144670486, 0.11752572655677795, 0.11878369748592377, 0.10529740154743195, 0.12084297090768814, 0.11789599806070328, 0.12109515815973282, 0.11567927151918411, 0.11984965950250626, 0.11924811452627182, 0.11664915829896927, 0.11265866458415985, 0.11034151911735535, 0.11796412616968155, 0.11737003177404404, 0.0997501015663147, 0.11085982620716095, 0.11767181754112244, 0.11154455691576004, 0.10606451332569122, 0.10963770002126694, 0.1101241484284401, 0.12074210494756699, 0.11959072202444077, 0.11549808084964752, 0.10934905707836151, 0.104437917470932, 0.11219292134046555, 0.1139584630727768, 0.11578311771154404, 0.11791280657052994, 0.11442278325557709, 0.10908371955156326, 0.11633086204528809, 0.12037581950426102, 0.11116375774145126, 0.11311217397451401, 0.10274059325456619, 0.1091185212135315, 0.11474674940109253, 0.10162429511547089, 0.11964067071676254, 0.10859637707471848, 0.11871786415576935, 0.1090228408575058, 0.1160280629992485, 0.10845902562141418, 0.11619069427251816, 0.11702141910791397, 0.11691176146268845, 0.12215226143598557, 0.1157672181725502, 0.11533113569021225, 0.11613638699054718, 0.1171506717801094, 0.11880296468734741, 0.09760481864213943, 0.10935104638338089, 0.10878461599349976, 0.1072881668806076, 0.10862035304307938, 0.11834637075662613, 0.11510085314512253, 0.10114719718694687, 0.11473988741636276, 0.10634397715330124, 0.11757714301347733, 0.11981911957263947, 0.11844731122255325, 0.10996558517217636, 0.12042032927274704, 0.1157151535153389, 0.10238059610128403, 0.12168575078248978, 0.11880513280630112, 0.12151599675416946, 0.11007368564605713, 0.11562122404575348, 0.11672761291265488, 0.12151003628969193, 0.11370987445116043, 0.10529066622257233, 0.11131647974252701, 0.10521142929792404, 0.12371993064880371, 0.12281651049852371, 0.11921222507953644, 0.11322714388370514, 0.11848992854356766, 0.10926251113414764, 0.11096227914094925, 0.1038590595126152, 0.11536844819784164, 0.11830515414476395, 0.10504010319709778, 0.09996481984853745, 0.11940684914588928, 0.10582474619150162, 0.11064787209033966, 0.11318153887987137, 0.10532130300998688, 0.11530550569295883, 0.11613329499959946, 0.10819947719573975, 0.10835002362728119, 0.09973268210887909, 0.1044599637389183, 0.11758338660001755, 0.10963379591703415, 0.10865094512701035, 0.11882534623146057, 0.09803931415081024, 0.11247671395540237, 0.11107869446277618, 0.11254208534955978, 0.10639794915914536, 0.11284006386995316, 0.1056094691157341, 0.11497008800506592, 0.11211708933115005, 0.11183768510818481, 0.10251142084598541, 0.11707159131765366, 0.12116739153862, 0.11360406130552292, 0.10777649283409119, 0.11797969788312912, 0.11456070095300674, 0.11723019927740097, 0.09382176399230957, 0.11468181759119034, 0.11622858047485352, 0.11641264706850052, 0.11195901781320572, 0.11896554380655289, 0.11433703452348709, 0.12176834791898727, 0.12353303283452988, 0.09625466912984848, 0.12211713194847107, 0.11316830664873123, 0.11666951328516006, 0.11553923040628433, 0.10880371183156967, 0.12522698938846588, 0.11700912564992905, 0.11819975823163986, 0.11942310631275177, 0.1217934712767601, 0.11337915807962418, 0.10924328118562698, 0.11001630872488022, 0.11840721219778061, 0.1084333062171936, 0.12250076979398727, 0.09588614106178284, 0.11732085794210434, 0.12265194207429886, 0.11188148707151413, 0.10550850629806519, 0.11781837791204453, 0.10812615603208542, 0.11776802688837051, 0.10620028525590897, 0.11463570594787598, 0.12171529233455658, 0.11937322467565536, 0.11729893833398819, 0.1140267550945282, 0.11689780652523041, 0.10289927572011948, 0.12366922944784164, 0.09942946583032608, 0.11812005192041397, 0.1068434789776802, 0.11728984862565994, 0.11305023729801178, 0.11282012611627579, 0.11531253904104233, 0.11948149651288986, 0.11366737633943558, 0.1183691918849945, 0.1228526160120964, 0.11059732735157013, 0.11147088557481766, 0.1208057776093483, 0.11305973678827286, 0.11952808499336243, 0.10895203053951263, 0.10214599221944809, 0.11314719170331955, 0.12068004161119461, 0.10609466582536697, 0.11560916900634766, 0.11711574345827103, 0.09708552062511444, 0.10855168849229813, 0.11840759962797165, 0.11175559461116791, 0.11709126830101013, 0.10567346960306168, 0.11790253967046738, 0.11603701859712601, 0.1111542135477066, 0.11796722561120987, 0.11400679498910904, 0.10530209541320801, 0.11856383085250854, 0.11495506018400192, 0.1233982965350151, 0.10491102188825607, 0.11657198518514633, 0.12114587426185608, 0.10795021057128906, 0.12077027559280396, 0.11183074861764908, 0.10223793238401413, 0.11834748834371567, 0.11294814944267273, 0.11810772120952606, 0.10650788992643356, 0.11258084326982498, 0.11309405416250229, 0.11717981845140457, 0.11147942394018173, 0.11757194995880127, 0.10251059383153915, 0.10849534720182419, 0.11908609420061111, 0.10972640663385391, 0.11370931565761566, 0.12144898623228073, 0.11765477061271667, 0.10848379880189896, 0.11949820816516876, 0.10829479247331619, 0.1150406152009964, 0.11573616415262222, 0.11111028492450714, 0.11523384600877762, 0.11411783844232559, 0.11231961101293564, 0.11655053496360779, 0.11718488484621048, 0.10926346480846405, 0.11436247825622559, 0.09816507250070572, 0.12007462978363037, 0.11315753310918808, 0.11705067753791809, 0.11891963332891464, 0.11916815489530563, 0.11434650421142578, 0.11813738197088242, 0.11181745678186417, 0.11045069247484207, 0.10943106561899185, 0.10301677137613297, 0.11845631152391434, 0.12039843946695328, 0.11129015684127808, 0.11218619346618652, 0.11546162515878677, 0.12093350291252136, 0.11750394850969315, 0.11400754004716873, 0.1165468692779541, 0.1119670495390892, 0.11200468987226486, 0.10772094130516052, 0.1128309965133667, 0.11012226343154907, 0.11567527800798416, 0.11897700279951096, 0.11957533657550812, 0.11535310000181198, 0.11215575039386749, 0.11433017998933792, 0.11727989464998245, 0.11362127214670181, 0.12287083268165588, 0.11681679636240005, 0.11100691556930542, 0.11519104987382889, 0.1034860834479332, 0.10350169241428375, 0.11359117180109024, 0.11762567609548569, 0.11194512993097305, 0.11419537663459778, 0.1192898377776146, 0.11510419100522995, 0.11064787209033966, 0.09922979027032852, 0.11665451526641846, 0.12365546822547913, 0.10107424110174179, 0.11815584450960159, 0.11889267712831497, 0.12154143303632736, 0.11510809510946274, 0.11197034269571304, 0.11406518518924713, 0.11331680417060852, 0.11413722485303879, 0.11259613186120987, 0.11807838827371597, 0.108943410217762, 0.11299390345811844, 0.10949204117059708, 0.11925189197063446, 0.09945686906576157, 0.11889906972646713, 0.11939651519060135, 0.11459431797266006, 0.1148870438337326, 0.11308281868696213, 0.10209145396947861, 0.10115606337785721, 0.11564799398183823, 0.10304757207632065, 0.11847654730081558, 0.11871305853128433, 0.10834569483995438, 0.10684137791395187, 0.10169094055891037, 0.1168069988489151, 0.10497750341892242, 0.10312598198652267, 0.1227593943476677, 0.11827190965414047, 0.11300262808799744, 0.11689739674329758, 0.09396574646234512, 0.11658383160829544, 0.11073260754346848, 0.11045045405626297, 0.11609183996915817, 0.12193582206964493, 0.1142076775431633, 0.11167705804109573, 0.11968541145324707, 0.105860635638237, 0.12096715718507767, 0.11238770931959152, 0.1125471219420433, 0.11116372048854828, 0.11747030168771744, 0.10332430154085159, 0.11524433642625809, 0.1110319122672081, 0.10673121362924576, 0.11431751400232315, 0.1190570741891861, 0.11901769042015076, 0.12599866092205048, 0.11743957549333572, 0.11171767860651016, 0.11436925083398819, 0.11146580427885056, 0.11582405865192413, 0.10564450174570084, 0.11251893639564514, 0.11771735548973083, 0.11488784104585648, 0.12056990712881088, 0.11745298653841019, 0.11625077575445175, 0.11395075917243958, 0.11605360358953476, 0.1152089387178421, 0.10605838894844055, 0.09557263553142548, 0.11996536701917648, 0.12071093171834946, 0.11100908368825912, 0.11576641350984573, 0.11163636296987534, 0.11201956868171692, 0.11302170157432556, 0.12025771290063858, 0.11588478833436966, 0.11759757250547409, 0.10835059732198715, 0.10072027891874313, 0.10516660660505295, 0.10814254730939865, 0.11947441101074219, 0.11683813482522964, 0.11343909054994583, 0.1081024706363678, 0.12077658623456955, 0.10697459429502487, 0.12045293301343918, 0.11216079443693161, 0.10035280138254166, 0.1210004910826683, 0.10639794915914536, 0.10715820640325546, 0.11512285470962524, 0.12133460491895676, 0.11487274616956711, 0.11443444341421127, 0.11438639461994171, 0.11542484909296036, 0.11162251234054565, 0.10682652145624161, 0.09891854971647263, 0.10987675935029984, 0.11956838518381119, 0.10192018002271652, 0.11611004918813705, 0.1035194844007492, 0.12004733830690384, 0.11198943108320236, 0.12171664088964462, 0.11647816002368927, 0.1056910827755928, 0.11161112040281296, 0.11627926677465439, 0.11550997197628021, 0.10304971039295197, 0.12067175656557083, 0.1179971843957901, 0.10166484862565994, 0.11328061670064926, 0.09940338879823685, 0.11455509066581726, 0.11763415485620499, 0.11663055419921875, 0.10641840845346451, 0.10808177292346954, 0.11705826967954636, 0.11425631493330002, 0.11219727247953415, 0.11200276762247086, 0.11201813071966171, 0.10598674416542053, 0.11231028288602829, 0.1197231262922287, 0.11805202811956406, 0.10701692849397659, 0.10964188724756241, 0.10795854777097702, 0.11641093343496323, 0.10904187709093094, 0.117649607360363, 0.12007126957178116, 0.11254433542490005, 0.11186689883470535, 0.11685336381196976, 0.10471149533987045, 0.1179506704211235, 0.11607437580823898, 0.10757363587617874, 0.09712199121713638, 0.11320208013057709, 0.1193506047129631, 0.12327253073453903, 0.10604685544967651, 0.11465400457382202, 0.11655034124851227, 0.11437451094388962, 0.12170758098363876, 0.11604202538728714, 0.1095157191157341, 0.10438190400600433, 0.11349833756685257, 0.11374183744192123, 0.10642562061548233, 0.114218570291996, 0.11363286525011063, 0.11949128657579422, 0.1080869659781456, 0.0822015106678009, 0.11603044718503952, 0.11615590751171112, 0.11885891109704971, 0.12177520245313644, 0.1147347167134285, 0.11798430234193802, 0.1079498901963234, 0.11981191486120224, 0.10928436368703842, 0.11458776891231537, 0.11823640018701553, 0.11857672035694122, 0.1140354573726654, 0.11108005046844482, 0.12118596583604813, 0.11242391914129257, 0.10289310663938522, 0.1239171102643013, 0.12080033123493195, 0.12000026553869247, 0.10663239657878876, 0.11734209209680557, 0.11352167278528214, 0.09547625482082367, 0.12078354507684708, 0.11463852226734161, 0.12121545523405075, 0.11687181144952774, 0.10084877908229828, 0.1185474619269371, 0.10348769277334213, 0.11686151474714279, 0.10197322815656662, 0.1141570657491684, 0.11613697558641434, 0.11240983754396439, 0.10735181719064713, 0.1071695014834404, 0.11483074724674225, 0.11599376052618027, 0.10957259684801102, 0.11664426326751709, 0.1179484948515892, 0.11046840995550156, 0.10861723870038986, 0.11317264288663864, 0.11716396361589432, 0.12215273827314377, 0.11277443915605545, 0.1152290403842926, 0.11131248623132706, 0.11811602115631104, 0.10224483162164688, 0.09247686713933945, 0.10429114103317261, 0.1137515977025032, 0.11336180567741394, 0.11976242065429688, 0.11246395856142044, 0.11589504033327103, 0.12013787031173706, 0.11948590725660324, 0.10991664230823517, 0.11300785094499588, 0.118063785135746, 0.10333431512117386, 0.10936236381530762, 0.11828594654798508, 0.1058463305234909, 0.11619170755147934, 0.10453366488218307, 0.11939412355422974, 0.11844928562641144, 0.11328793317079544, 0.11204208433628082, 0.1145804151892662, 0.11528288573026657, 0.12532790005207062, 0.1113826259970665, 0.10689439624547958, 0.11455847322940826, 0.11997797340154648, 0.11939748376607895, 0.11964862793684006, 0.11168015748262405, 0.12374686449766159, 0.11743775010108948, 0.12208399176597595, 0.11990345269441605, 0.10593396425247192, 0.11914370208978653, 0.11260712891817093, 0.11858360469341278, 0.1155935749411583, 0.10894263535737991, 0.11189281195402145, 0.11510039865970612, 0.12004747241735458, 0.11367587745189667, 0.11720658093690872, 0.11778443306684494, 0.11145738512277603, 0.0944562777876854, 0.11498179286718369, 0.10903886705636978, 0.10495200753211975, 0.1003103032708168, 0.10444367676973343, 0.10754593461751938, 0.10485952347517014, 0.11512058973312378, 0.11261363327503204, 0.11173228919506073, 0.10727029293775558, 0.10978849977254868, 0.1136731281876564, 0.11495190113782883, 0.11018393188714981, 0.11421610414981842, 0.11530838161706924, 0.11463755369186401, 0.11981674283742905, 0.10658139735460281, 0.09842225909233093, 0.11353305727243423, 0.12065814435482025, 0.12051280587911606, 0.11345136910676956, 0.10421757400035858, 0.11739323288202286, 0.11389710754156113, 0.09618722647428513, 0.11695197969675064, 0.11657000333070755, 0.11764232069253922, 0.1104443296790123, 0.11263072490692139, 0.10067114979028702, 0.11143164336681366, 0.11806196719408035, 0.11110561341047287, 0.11325523257255554, 0.11012639850378036, 0.12456881999969482, 0.1177995428442955, 0.1196482703089714, 0.10165707767009735, 0.12241705507040024, 0.10230402648448944, 0.1053420677781105, 0.11266782879829407, 0.11041083186864853, 0.11962387710809708, 0.117708720266819, 0.11180135607719421, 0.11051353812217712, 0.1179066002368927, 0.12129081785678864, 0.11430513858795166, 0.10589303821325302, 0.11345691978931427, 0.10832888633012772, 0.12213108688592911, 0.11818008124828339, 0.11366406828165054, 0.09597403556108475, 0.11585403233766556, 0.1112234815955162, 0.11717765033245087, 0.09144306927919388, 0.11875270307064056, 0.1152854636311531, 0.12035346776247025, 0.11279276758432388, 0.10998103022575378, 0.12263268977403641, 0.11326823383569717, 0.11246846616268158, 0.11500583589076996, 0.11289442330598831, 0.1125621572136879, 0.11771760880947113, 0.1134907677769661, 0.1198984906077385, 0.12328474223613739, 0.11714289337396622, 0.08915384113788605, 0.10747762769460678, 0.11380010098218918, 0.1172761619091034, 0.10801243782043457, 0.11364109069108963, 0.1055997684597969, 0.11310753971338272, 0.11317415535449982, 0.0986805185675621, 0.09306112676858902, 0.10910730808973312, 0.11494205892086029, 0.12388207763433456, 0.11884888261556625, 0.1168697401881218, 0.10454706102609634, 0.10217737406492233, 0.12151236832141876, 0.11844532936811447, 0.12038085609674454, 0.11769910156726837, 0.12061358243227005, 0.1073133796453476, 0.10193180292844772, 0.11852297931909561, 0.10743851214647293, 0.1174093633890152, 0.10393931716680527, 0.12255003303289413, 0.10162821412086487, 0.11983954161405563, 0.10334279388189316, 0.10824153572320938, 0.10602723062038422, 0.11560647189617157, 0.12123025953769684, 0.12546338140964508, 0.1068120002746582, 0.10752610117197037, 0.12090174853801727, 0.11592049896717072, 0.11183703690767288, 0.10489068180322647, 0.11335361748933792, 0.11824708431959152, 0.12275968492031097, 0.12355516105890274, 0.08969251066446304, 0.11778654158115387, 0.11969251930713654, 0.1144557073712349, 0.11501336097717285, 0.09648172557353973, 0.10290826857089996, 0.11524421721696854, 0.10646144300699234, 0.09550557285547256, 0.11246376484632492, 0.11652269214391708, 0.1152091696858406, 0.10924430936574936, 0.11141376942396164, 0.12433547526597977, 0.11732940375804901, 0.11384259909391403, 0.11854935437440872, 0.10694420337677002, 0.11863525211811066, 0.10185271501541138, 0.11334844678640366, 0.1139419674873352, 0.11322139203548431, 0.12454209476709366, 0.11325500160455704, 0.10975338518619537, 0.12103439122438431, 0.11524248868227005, 0.11281055212020874, 0.1182813048362732, 0.11875740438699722, 0.12256034463644028, 0.11334788054227829, 0.10922137647867203, 0.11983814090490341, 0.11065933108329773, 0.11891662329435349, 0.12031321227550507, 0.11099126189947128, 0.11488928645849228, 0.11164069920778275, 0.11156287789344788, 0.11519655585289001, 0.1123492643237114, 0.12291914969682693, 0.10552220046520233, 0.12380927801132202, 0.11409390717744827, 0.10364513099193573, 0.11543097347021103, 0.12123893201351166, 0.121738001704216, 0.12201876938343048, 0.09624336659908295, 0.11769755929708481, 0.11718488484621048, 0.11072493344545364, 0.10866755992174149, 0.11242494732141495, 0.11159444600343704, 0.10181955248117447, 0.11871068924665451, 0.11971975117921829, 0.11181411892175674, 0.11595211923122406, 0.11415557563304901, 0.10315513610839844, 0.11322157829999924, 0.10378936678171158, 0.11352167278528214, 0.1099090501666069, 0.10871033370494843, 0.1130107194185257, 0.10198481380939484, 0.12052088230848312, 0.11194323003292084, 0.1205461397767067, 0.11838901787996292, 0.12135829031467438, 0.11850234866142273, 0.11740417033433914, 0.11961125582456589, 0.12038567662239075, 0.11713186651468277, 0.11595585197210312, 0.1194840669631958, 0.11007082462310791, 0.1160297840833664, 0.11388926208019257, 0.11830778419971466, 0.11548229306936264, 0.11131913959980011, 0.12050365656614304, 0.10978363454341888, 0.11626333743333817, 0.1168697401881218, 0.1136181652545929, 0.11252864450216293, 0.10004421323537827, 0.10864829272031784, 0.09906522929668427, 0.10654965788125992, 0.11790236830711365, 0.11349450051784515, 0.11850433051586151, 0.11693792790174484, 0.11940588802099228, 0.10889721661806107, 0.11410344392061234, 0.11776705831289291, 0.10549384355545044, 0.11049182713031769, 0.1100461408495903, 0.11844723671674728, 0.10662392526865005, 0.10970873385667801, 0.1194012239575386, 0.10265596956014633, 0.1044677123427391, 0.11903497576713562, 0.10742873698472977, 0.11487603932619095, 0.1098213791847229, 0.12473402172327042, 0.11425772309303284, 0.1080888882279396, 0.11796838790178299, 0.11601073294878006, 0.10921424627304077, 0.11549051105976105, 0.11380501836538315, 0.11210999637842178, 0.11644794046878815, 0.11812688410282135, 0.10719991475343704, 0.12328606843948364, 0.11808671057224274, 0.10279358178377151, 0.12033379822969437, 0.11540669947862625, 0.10989613831043243, 0.10917852073907852, 0.11257819086313248, 0.10801968723535538, 0.11633571237325668, 0.11743966490030289, 0.10388286411762238, 0.10923008620738983, 0.11600568890571594, 0.11104640364646912, 0.11520230025053024, 0.11705514788627625, 0.12186448276042938, 0.11969656497240067, 0.11842062324285507, 0.11369668692350388, 0.121556855738163, 0.12065105140209198, 0.11364971846342087, 0.10704465955495834, 0.11396116763353348, 0.10201986879110336, 0.1199597418308258, 0.11615516990423203, 0.09308792650699615, 0.1121886596083641, 0.11821483075618744, 0.10574394464492798, 0.11670294404029846, 0.11940109729766846, 0.11984188854694366, 0.11068020015954971, 0.10483256727457047, 0.10620203614234924, 0.12099470943212509, 0.11729271709918976, 0.11216532438993454, 0.1167718693614006, 0.11321919411420822, 0.11469683796167374, 0.10536011308431625, 0.11054031550884247, 0.11851432174444199, 0.11923161894083023, 0.1148354709148407, 0.11469060927629471, 0.11588730663061142, 0.12011287361383438, 0.10452398657798767, 0.1140369400382042, 0.09828714281320572, 0.11137701570987701, 0.11533237993717194, 0.11686128377914429, 0.11582799255847931, 0.11304053664207458, 0.11059032380580902, 0.1216299906373024, 0.11423014849424362, 0.11431054025888443, 0.11690589785575867, 0.11656444519758224, 0.1058574765920639, 0.10802114009857178, 0.1177641972899437, 0.11508627980947495, 0.11368653178215027, 0.1187736988067627, 0.10901365429162979, 0.11895984411239624, 0.11110850423574448, 0.11686628311872482, 0.1188604012131691, 0.10345641523599625, 0.09843515604734421, 0.12114142626523972, 0.1128009483218193, 0.11284259706735611], \"y\": [0.10220491886138916, 0.1017153412103653, 0.09864474087953568, 0.10126703977584839, 0.10222671180963516, 0.09901297092437744, 0.10081156343221664, 0.09616240859031677, 0.09815794974565506, 0.10048066079616547, 0.10008571296930313, 0.10158602893352509, 0.09916295856237411, 0.10386446118354797, 0.09801812469959259, 0.10117477178573608, 0.10377156734466553, 0.10629358887672424, 0.10269612073898315, 0.10277565568685532, 0.09956304728984833, 0.10089723020792007, 0.10501852631568909, 0.09994585812091827, 0.10288034379482269, 0.09932452440261841, 0.10834991186857224, 0.1029343381524086, 0.09706666320562363, 0.10302546620368958, 0.09994810819625854, 0.09458588063716888, 0.09838400781154633, 0.10185039788484573, 0.10129797458648682, 0.10085395723581314, 0.10251685976982117, 0.10188095271587372, 0.10486676543951035, 0.10253550112247467, 0.09854494780302048, 0.09964758157730103, 0.09987224638462067, 0.09681386500597, 0.09736096858978271, 0.10007283836603165, 0.10528163611888885, 0.10485682636499405, 0.09869340062141418, 0.0987902283668518, 0.09651726484298706, 0.10251417756080627, 0.10109861940145493, 0.10074231773614883, 0.10169461369514465, 0.1042102724313736, 0.09830857813358307, 0.09919493645429611, 0.10704228281974792, 0.10452423244714737, 0.10315962880849838, 0.09912605583667755, 0.09802857786417007, 0.09699612110853195, 0.09825265407562256, 0.10568860918283463, 0.09933661669492722, 0.09745098650455475, 0.10338566452264786, 0.10551444441080093, 0.10467208176851273, 0.10112114995718002, 0.09758490324020386, 0.09770191460847855, 0.09918594360351562, 0.10520470142364502, 0.10068388283252716, 0.10031727701425552, 0.09869293123483658, 0.10594028234481812, 0.10557916015386581, 0.10618960857391357, 0.10255090892314911, 0.10022342205047607, 0.09490876644849777, 0.10271713137626648, 0.10669383406639099, 0.0983738899230957, 0.10414480417966843, 0.10184837877750397, 0.10705488920211792, 0.10540808737277985, 0.09976577758789062, 0.0988023430109024, 0.10252706706523895, 0.09802629053592682, 0.09764532744884491, 0.10029715299606323, 0.10339153558015823, 0.10319704562425613, 0.09904075413942337, 0.09826353937387466, 0.1037532240152359, 0.09836933016777039, 0.09650839865207672, 0.10034163296222687, 0.10186223685741425, 0.10090366005897522, 0.09790945053100586, 0.10244890302419662, 0.10502337664365768, 0.10746482014656067, 0.10553757101297379, 0.10294847190380096, 0.09942546486854553, 0.09918463975191116, 0.0959530919790268, 0.11552481353282928, 0.10091685503721237, 0.10010156780481339, 0.1012994647026062, 0.10067814588546753, 0.09922658652067184, 0.10552137345075607, 0.10241061449050903, 0.0986306294798851, 0.10343056172132492, 0.09998917579650879, 0.09793168306350708, 0.09604325890541077, 0.09950169920921326, 0.10559462755918503, 0.10056594759225845, 0.0931038036942482, 0.09966246783733368, 0.10157179832458496, 0.10228229314088821, 0.09639196842908859, 0.10558854043483734, 0.10649075359106064, 0.09920816123485565, 0.09438805282115936, 0.0986207127571106, 0.09517760574817657, 0.09172895550727844, 0.0969691127538681, 0.09924280643463135, 0.09881504625082016, 0.10313805192708969, 0.10335467010736465, 0.09865213185548782, 0.09839856624603271, 0.10400731861591339, 0.09680940955877304, 0.09755460917949677, 0.09564793854951859, 0.10294400900602341, 0.09722093492746353, 0.10046655684709549, 0.10657674074172974, 0.09997785091400146, 0.10222433507442474, 0.0991983488202095, 0.10626251250505447, 0.1012020856142044, 0.09783852845430374, 0.10028102993965149, 0.10048320144414902, 0.1028606966137886, 0.0992327407002449, 0.0966726765036583, 0.10054054856300354, 0.09977791458368301, 0.09092105180025101, 0.10032474249601364, 0.09735190868377686, 0.10478080064058304, 0.10183103382587433, 0.09737798571586609, 0.09911727160215378, 0.09943152219057083, 0.10321652889251709, 0.10559482872486115, 0.10441222786903381, 0.10062247514724731, 0.10343779623508453, 0.0974631980061531, 0.1004004031419754, 0.10778866708278656, 0.08490949869155884, 0.10185498744249344, 0.09661747515201569, 0.09872978925704956, 0.09797628968954086, 0.09865392744541168, 0.10236933827400208, 0.09766078740358353, 0.10508020222187042, 0.10018778592348099, 0.10535535216331482, 0.10271983593702316, 0.1007281020283699, 0.10183978080749512, 0.10050181299448013, 0.10708972811698914, 0.1054348275065422, 0.10207939893007278, 0.09689930826425552, 0.10469873249530792, 0.09893542528152466, 0.1004369854927063, 0.10052891075611115, 0.10081703960895538, 0.09620259702205658, 0.10089723020792007, 0.10397280752658844, 0.09983083605766296, 0.10335693508386612, 0.10825050622224808, 0.0998644307255745, 0.10261468589305878, 0.09933368861675262, 0.09762109071016312, 0.0998855009675026, 0.09790012240409851, 0.10281649976968765, 0.09933841228485107, 0.10690072923898697, 0.10032466053962708, 0.10144098848104477, 0.10134454071521759, 0.1042933538556099, 0.10117054730653763, 0.10505843162536621, 0.0996973067522049, 0.0969262570142746, 0.10599321871995926, 0.10255090892314911, 0.1011914387345314, 0.10634761303663254, 0.09769105911254883, 0.10463210940361023, 0.0961717963218689, 0.10125620663166046, 0.10932306945323944, 0.09901445358991623, 0.10532067716121674, 0.10674463212490082, 0.10196348279714584, 0.10252069681882858, 0.099931500852108, 0.09896142780780792, 0.10070818662643433, 0.09862688183784485, 0.0963665172457695, 0.09984617680311203, 0.09867939352989197, 0.1024426519870758, 0.10714583098888397, 0.09592095017433167, 0.09847359359264374, 0.10036535561084747, 0.10106434673070908, 0.09958517551422119, 0.10228985548019409, 0.10572278499603271, 0.09716639667749405, 0.10024102032184601, 0.10432706028223038, 0.10398366302251816, 0.10418237000703812, 0.10586152970790863, 0.09693829715251923, 0.10339182615280151, 0.10145192593336105, 0.09726764261722565, 0.09842918068170547, 0.0966620221734047, 0.09898481518030167, 0.09487074613571167, 0.10413458198308945, 0.10449537634849548, 0.09640514105558395, 0.10108137130737305, 0.10208868980407715, 0.10230647027492523, 0.10021024197340012, 0.09997633099555969, 0.09528127312660217, 0.10006318986415863, 0.10374617576599121, 0.09832357615232468, 0.09894084930419922, 0.10179542750120163, 0.10175393521785736, 0.10016967356204987, 0.10543690621852875, 0.10235902667045593, 0.10059899091720581, 0.10196227580308914, 0.1009274572134018, 0.10375984758138657, 0.10567785054445267, 0.09863580018281937, 0.10507867485284805, 0.09992565959692001, 0.10441653430461884, 0.1078673005104065, 0.10545506328344345, 0.0983181819319725, 0.10303391516208649, 0.10272889584302902, 0.10161278396844864, 0.0994468629360199, 0.10331663489341736, 0.09751303493976593, 0.10507268458604813, 0.097494937479496, 0.10565344989299774, 0.10117174685001373, 0.09055454283952713, 0.10310723632574081, 0.09533494710922241, 0.10239697992801666, 0.09645918756723404, 0.09873885661363602, 0.09915429353713989, 0.10033994168043137, 0.09619326889514923, 0.10645034164190292, 0.09666252136230469, 0.09836772829294205, 0.09995788335800171, 0.10041773319244385, 0.10114948451519012, 0.09591569751501083, 0.0955786406993866, 0.10652749985456467, 0.10168299823999405, 0.09562762081623077, 0.1024574488401413, 0.09418606758117676, 0.09827818721532822, 0.10036253929138184, 0.09820051491260529, 0.09831558167934418, 0.10130471736192703, 0.10827656090259552, 0.099319227039814, 0.10195724666118622, 0.10587240755558014, 0.10445162653923035, 0.10086823999881744, 0.09783997386693954, 0.09874150902032852, 0.09766396880149841, 0.1050807386636734, 0.10001872479915619, 0.10082589834928513, 0.10140197724103928, 0.09914875030517578, 0.10162436217069626, 0.10181387513875961, 0.10025741159915924, 0.10098984092473984, 0.10261402279138565, 0.10421542078256607, 0.10177667438983917, 0.0963866338133812, 0.09844925999641418, 0.09812743961811066, 0.09899835288524628, 0.11060559749603271, 0.10097479820251465, 0.10003363341093063, 0.10152700543403625, 0.10384795069694519, 0.10243465006351471, 0.09760075807571411, 0.10815447568893433, 0.11129143834114075, 0.09746456891298294, 0.09877974539995193, 0.10132040083408356, 0.09855076670646667, 0.09594486653804779, 0.09950576722621918, 0.0984681025147438, 0.10695119202136993, 0.0995309054851532, 0.10556896775960922, 0.10468795150518417, 0.09891974180936813, 0.100224569439888, 0.10551009327173233, 0.10615407675504684, 0.10065983980894089, 0.10023230314254761, 0.10435038805007935, 0.09634979814291, 0.09812699258327484, 0.10489446669816971, 0.09934641420841217, 0.09979933500289917, 0.10761325061321259, 0.10460451245307922, 0.10097412765026093, 0.09519235789775848, 0.10650698095560074, 0.09901995956897736, 0.09980735182762146, 0.10259407013654709, 0.1015150249004364, 0.10471432656049728, 0.10249866545200348, 0.09813490509986877, 0.10005727410316467, 0.09959254413843155, 0.0958731546998024, 0.1018223911523819, 0.10369297116994858, 0.09914810955524445, 0.09859257936477661, 0.10447841137647629, 0.09525299072265625, 0.10327751934528351, 0.10353835672140121, 0.10279931873083115, 0.10021135210990906, 0.09588328748941422, 0.10343945026397705, 0.10071974247694016, 0.10016340017318726, 0.10679131746292114, 0.1031128317117691, 0.10140237212181091, 0.10475698113441467, 0.09948020428419113, 0.09721668809652328, 0.09940894693136215, 0.09785139560699463, 0.10059773921966553, 0.10250210762023926, 0.09946175664663315, 0.09834127128124237, 0.10265341401100159, 0.10340994596481323, 0.10225177556276321, 0.1024448499083519, 0.1049048975110054, 0.09893384575843811, 0.10419648885726929, 0.10344897955656052, 0.10257686674594879, 0.0989968329668045, 0.1036161333322525, 0.1093631237745285, 0.09778925776481628, 0.09682445228099823, 0.10899436473846436, 0.10016970336437225, 0.0981723889708519, 0.09979341924190521, 0.10354196280241013, 0.09001845121383667, 0.09921455383300781, 0.09985819458961487, 0.09875970333814621, 0.10884393751621246, 0.10118909180164337, 0.09695503115653992, 0.10412280261516571, 0.10075487196445465, 0.10277938842773438, 0.10078275203704834, 0.10196631401777267, 0.096893809735775, 0.10134105384349823, 0.10125544667243958, 0.09706147760152817, 0.09758981317281723, 0.09903933107852936, 0.09914202988147736, 0.10482734441757202, 0.09853410720825195, 0.10138604044914246, 0.10263262689113617, 0.105383962392807, 0.10495791584253311, 0.09746111929416656, 0.10244233161211014, 0.09914655983448029, 0.09191849827766418, 0.09821990877389908, 0.10132915526628494, 0.09987340867519379, 0.09800536185503006, 0.1057543084025383, 0.10139207541942596, 0.10665503144264221, 0.10039100050926208, 0.09857414662837982, 0.10099560767412186, 0.10231561213731766, 0.10319037735462189, 0.09994760155677795, 0.10090341418981552, 0.10258284956216812, 0.0978233814239502, 0.09232266247272491, 0.10230772197246552, 0.10043585300445557, 0.10575897991657257, 0.10781826078891754, 0.10144484043121338, 0.09864777326583862, 0.09919336438179016, 0.09713052213191986, 0.09744464606046677, 0.10274349898099899, 0.10230910778045654, 0.10058313608169556, 0.09782905876636505, 0.10221895575523376, 0.09976499527692795, 0.09713366627693176, 0.09860873967409134, 0.09953838586807251, 0.09855140000581741, 0.10134143382310867, 0.09610645473003387, 0.10297424346208572, 0.100923553109169, 0.10115182399749756, 0.10505180060863495, 0.09961475431919098, 0.09913554787635803, 0.10197845846414566, 0.10644681751728058, 0.09904246777296066, 0.09849850833415985, 0.10218162089586258, 0.10340134054422379, 0.09942957758903503, 0.10185182839632034, 0.10047156363725662, 0.09694740921258926, 0.09238846600055695, 0.10476762801408768, 0.09674021601676941, 0.10060486942529678, 0.09738092124462128, 0.10242612659931183, 0.10938847064971924, 0.0974336639046669, 0.10360913723707199, 0.10541974008083344, 0.09842712432146072, 0.10507950186729431, 0.10300609469413757, 0.0994545966386795, 0.10038643330335617, 0.09747511148452759, 0.09909310936927795, 0.10402786731719971, 0.10282161831855774, 0.09827399253845215, 0.0964733213186264, 0.09931749105453491, 0.1030241921544075, 0.09832066297531128, 0.1007472351193428, 0.09989587217569351, 0.10036759823560715, 0.10365757346153259, 0.09957130998373032, 0.10263475775718689, 0.10768934339284897, 0.09684314578771591, 0.10006002336740494, 0.10432862490415573, 0.0975986197590828, 0.10415811836719513, 0.09951697289943695, 0.10193576663732529, 0.0970836952328682, 0.10258662700653076, 0.10267125815153122, 0.09950578957796097, 0.10112034529447556, 0.09899964183568954, 0.09970469772815704, 0.1023353785276413, 0.10083368420600891, 0.09842868894338608, 0.09558392316102982, 0.10637623816728592, 0.0931791439652443, 0.10221929848194122, 0.09490276128053665, 0.1050553098320961, 0.11517959088087082, 0.09974382817745209, 0.10237111151218414, 0.09883611649274826, 0.09854386746883392, 0.09921862930059433, 0.1046966165304184, 0.10541974008083344, 0.10294150561094284, 0.10159067809581757, 0.10133827477693558, 0.09876173734664917, 0.10414018481969833, 0.10538361221551895, 0.10117660462856293, 0.10288640111684799, 0.10182467103004456, 0.10525349527597427, 0.10794924199581146, 0.09826137125492096, 0.09941428899765015, 0.10154657065868378, 0.10100101679563522, 0.10062938183546066, 0.10824483633041382, 0.10405058413743973, 0.10195236653089523, 0.10766609013080597, 0.09833817183971405, 0.10138294845819473, 0.11106248199939728, 0.1016010195016861, 0.10239828377962112, 0.09962289035320282, 0.0969640463590622, 0.10221025347709656, 0.10614636540412903, 0.09693747013807297, 0.09587237238883972, 0.09838394075632095, 0.10838103294372559, 0.10026461631059647, 0.09927957504987717, 0.10672799497842789, 0.1098545715212822, 0.09974393248558044, 0.10099494457244873, 0.10237349569797516, 0.09765730798244476, 0.10436736047267914, 0.08722081035375595, 0.10562640428543091, 0.10141533613204956, 0.10164013504981995, 0.10757654905319214, 0.09697924554347992, 0.103369802236557, 0.10477913916110992, 0.10033178329467773, 0.10523666441440582, 0.09268029034137726, 0.09810702502727509, 0.10038547217845917, 0.09760933369398117, 0.10482734441757202, 0.10623565316200256, 0.10525824874639511, 0.10252785682678223, 0.10216514766216278, 0.09204745292663574, 0.103504478931427, 0.09799270331859589, 0.10483494400978088, 0.10244829207658768, 0.09802071750164032, 0.09998679906129837, 0.10254201292991638, 0.10140862315893173, 0.09975522011518478, 0.10494451224803925, 0.09802108258008957, 0.0986081212759018, 0.09789053350687027, 0.1041564792394638, 0.10041050612926483, 0.10459551215171814, 0.10405474901199341, 0.09393985569477081, 0.09703332185745239, 0.09775704890489578, 0.09887263178825378, 0.10543844848871231, 0.09705344587564468, 0.10338160395622253, 0.1068723052740097, 0.10035279393196106, 0.0970449224114418, 0.09917967021465302, 0.10385073721408844, 0.09829860925674438, 0.1006852462887764, 0.10031270980834961, 0.10552141070365906, 0.10137924551963806, 0.09904163330793381, 0.0995323657989502, 0.0987408310174942, 0.0974213257431984, 0.09554597735404968, 0.09567955136299133, 0.10710998624563217, 0.10613956302404404, 0.10507993400096893, 0.09727317839860916, 0.09854280203580856, 0.10851076990365982, 0.09966754168272018, 0.09838423132896423, 0.10333167016506195, 0.10265599191188812, 0.10616794228553772, 0.10175247490406036, 0.10487202554941177, 0.10475355386734009, 0.09982835501432419, 0.09787718951702118, 0.09822644293308258, 0.10104332864284515, 0.10738907009363174, 0.10938098281621933, 0.10009534657001495, 0.09614410996437073, 0.0998491644859314, 0.09638410806655884, 0.10727238655090332, 0.10744501650333405, 0.09893542528152466, 0.10621791332960129, 0.09870872646570206, 0.099319227039814, 0.09998959302902222, 0.097604900598526, 0.09474164247512817, 0.10227126628160477, 0.1024610847234726, 0.10275403410196304, 0.09938394278287888, 0.1009838655591011, 0.10409827530384064, 0.09924492239952087, 0.09571506828069687, 0.10339941084384918, 0.09793177247047424, 0.0982009768486023, 0.10058209300041199, 0.09414981305599213, 0.09769970178604126, 0.10064361989498138, 0.10301356017589569, 0.09917193651199341, 0.10369952768087387, 0.10463576018810272, 0.09963241964578629, 0.09512448310852051, 0.10010747611522675, 0.10036268830299377, 0.10265840590000153, 0.10003507137298584, 0.09951555728912354, 0.09950412809848785, 0.10556554049253464, 0.09988000988960266, 0.10421949625015259, 0.11042816191911697, 0.09823683649301529, 0.09948444366455078, 0.09863722324371338, 0.1057172492146492, 0.11167458444833755, 0.10121949017047882, 0.09335823357105255, 0.10066326707601547, 0.09993300586938858, 0.09606865048408508, 0.10429378598928452, 0.10466501116752625, 0.09832846373319626, 0.10187463462352753, 0.10247928649187088, 0.10214376449584961, 0.10524117946624756, 0.0985984280705452, 0.09613829106092453, 0.09239745885133743, 0.10293170064687729, 0.09970483928918839, 0.10303215682506561, 0.10085447132587433, 0.09770191460847855, 0.10480304062366486, 0.1022559255361557, 0.10272708535194397, 0.10222332179546356, 0.09917175769805908, 0.0999070256948471, 0.10542066395282745, 0.10543762147426605, 0.09340476989746094, 0.0983269140124321, 0.09776507318019867, 0.10814373195171356, 0.09948918223381042, 0.1041170060634613, 0.10324298590421677, 0.10672640800476074, 0.10364019870758057, 0.10279881954193115, 0.10459904372692108, 0.10346765071153641, 0.10445259511470795, 0.1030101478099823, 0.10245721787214279, 0.10485155135393143, 0.10026504099369049, 0.09584580361843109, 0.10371896624565125, 0.09870494902133942, 0.10128594934940338, 0.09922319650650024, 0.10037083178758621, 0.0985255241394043, 0.09891963005065918, 0.1062290370464325, 0.10498102009296417, 0.10145630687475204, 0.10304777324199677, 0.10656293481588364, 0.09836048632860184, 0.10454169660806656, 0.09881079941987991, 0.100667804479599, 0.10287154465913773, 0.10104383528232574, 0.10191665589809418, 0.09838218241930008, 0.10196320712566376, 0.09001845121383667, 0.10010284930467606, 0.10943979769945145, 0.10015639662742615, 0.10033970326185226, 0.10214889794588089, 0.10596764832735062, 0.09509281814098358, 0.0994088351726532, 0.10113077610731125, 0.10501108318567276, 0.10661963373422623, 0.10035024583339691, 0.10426437109708786, 0.10116748511791229, 0.10060757398605347, 0.10287102311849594, 0.0989295244216919, 0.10052976757287979, 0.10162327438592911, 0.10146129131317139, 0.10343273729085922, 0.10129726678133011, 0.09760183840990067, 0.09992261230945587, 0.09938941150903702, 0.09327971190214157, 0.10205323249101639, 0.10317113995552063, 0.10496774315834045, 0.10404208302497864, 0.10046059638261795, 0.10691862553358078, 0.09966805577278137, 0.10293025523424149, 0.0960683599114418, 0.10210021585226059, 0.09999416768550873, 0.10552464425563812, 0.1000576764345169, 0.10084440559148788, 0.10010526329278946, 0.10724928230047226, 0.09792209416627884, 0.09447211027145386, 0.09484429657459259, 0.102457694709301, 0.10213527083396912, 0.1080404669046402, 0.10312257707118988, 0.10480576008558273, 0.09908226132392883, 0.10351553559303284, 0.10233263671398163, 0.10031161457300186, 0.09806132316589355, 0.10151661932468414, 0.10202212631702423, 0.09676341712474823, 0.10478635132312775, 0.10252261906862259, 0.09330464899539948, 0.10904520750045776, 0.09772755950689316, 0.10131551325321198, 0.0952938050031662, 0.10004949569702148, 0.10041806101799011, 0.09883619099855423, 0.10293667763471603, 0.10455282032489777, 0.10024325549602509, 0.10178273916244507, 0.10749927163124084, 0.10174392908811569, 0.10417823493480682, 0.10358000546693802, 0.1000000610947609, 0.09824814647436142, 0.1007133200764656, 0.09623203426599503, 0.09958287328481674, 0.104754239320755, 0.10233106464147568, 0.09753603488206863, 0.10037237405776978, 0.10273796319961548, 0.10385596007108688, 0.09712451696395874, 0.10005493462085724, 0.09957307577133179, 0.09951236844062805, 0.10155096650123596, 0.10335402190685272, 0.10088904201984406, 0.10061642527580261, 0.1030208021402359, 0.09554731845855713, 0.10118812322616577, 0.10034758597612381, 0.09982603043317795, 0.09879817813634872, 0.10610164701938629, 0.10069258511066437, 0.09692056477069855, 0.09910739958286285, 0.10452739894390106, 0.10214068740606308, 0.09964961558580399, 0.10247232764959335, 0.09621503949165344, 0.10388336330652237, 0.10109489411115646, 0.09978976100683212, 0.10571794956922531, 0.10010190308094025, 0.10259987413883209, 0.10499498248100281, 0.10429373383522034, 0.09778402745723724, 0.09828024357557297, 0.10113183408975601, 0.0988328754901886, 0.10183167457580566, 0.1021869033575058, 0.10240672528743744, 0.1009991317987442, 0.10089956223964691, 0.10126624256372452, 0.10209447890520096, 0.09794355183839798, 0.09191107749938965, 0.09941854327917099, 0.09793926775455475, 0.0981251448392868, 0.09671099483966827, 0.10409731417894363, 0.10263121873140335, 0.10094857215881348, 0.10340392589569092, 0.10243390500545502, 0.1038854569196701, 0.10576856881380081, 0.10234928131103516, 0.09777548164129257, 0.09949450939893723, 0.09784024208784103, 0.09898267686367035, 0.10562724620103836, 0.10209877789020538, 0.09668118506669998, 0.09860198944807053, 0.10083988308906555, 0.09750429540872574, 0.10544319450855255, 0.10055501759052277, 0.11078828573226929, 0.0989881157875061, 0.09607407450675964, 0.0953848734498024, 0.10498072952032089, 0.10062812268733978, 0.09807882457971573, 0.09799564629793167, 0.10062585026025772, 0.1007152795791626, 0.0963049978017807, 0.10070223361253738, 0.1023404449224472, 0.10382043570280075, 0.10254678130149841, 0.09881926327943802, 0.10201167315244675, 0.09909738600254059, 0.0989794209599495, 0.09957447648048401, 0.10165628790855408, 0.10746479034423828, 0.09788233041763306, 0.10324295610189438, 0.1082211285829544, 0.0968651995062828, 0.10317457467317581, 0.10688095539808273, 0.09863662719726562, 0.10508911311626434, 0.10705416649580002, 0.0996282696723938, 0.0898861289024353, 0.09409230947494507, 0.09860891103744507, 0.10546895861625671, 0.10436004400253296, 0.09600451588630676, 0.10419875383377075, 0.10727408528327942, 0.1042751669883728, 0.10772164165973663, 0.10241665691137314, 0.10742992162704468, 0.09930850565433502, 0.10482591390609741, 0.10013150423765182, 0.1012994647026062, 0.09968694299459457, 0.09892404079437256, 0.10188362747430801, 0.09615548700094223, 0.1029382050037384, 0.09424047917127609, 0.10063739120960236, 0.09953060746192932, 0.09953278303146362, 0.10725001990795135, 0.10365757346153259, 0.09704039245843887, 0.0968158096075058, 0.10068440437316895, 0.09842906892299652, 0.099535271525383, 0.09410492330789566, 0.10062400996685028, 0.09944029152393341, 0.09755609929561615, 0.10660451650619507, 0.10672158002853394, 0.109627865254879, 0.10992374271154404, 0.09914988279342651, 0.10221541672945023, 0.10163150727748871, 0.09916764497756958, 0.10675167292356491, 0.09870272129774094, 0.10124132037162781, 0.11005625128746033, 0.100981704890728, 0.10304104536771774, 0.1058245301246643, 0.1010359525680542, 0.10130617767572403, 0.1063639223575592, 0.10407903790473938, 0.09232590347528458, 0.09542436897754669, 0.09324232488870621, 0.10242882370948792, 0.09896762669086456, 0.09870443493127823, 0.09900696575641632, 0.10308864712715149, 0.09771127253770828, 0.09954378008842468, 0.09677082300186157, 0.09956979751586914, 0.09682214260101318, 0.10086087882518768, 0.10700162500143051, 0.10000863671302795, 0.10116541385650635, 0.09972482919692993, 0.10046502947807312, 0.10088330507278442, 0.09510722756385803, 0.1048269048333168, 0.10320299863815308, 0.10075482726097107, 0.10558254271745682, 0.10292787104845047, 0.09691211581230164, 0.1031939834356308, 0.10082206130027771, 0.09986238926649094, 0.09965970367193222, 0.1023770123720169, 0.10077366232872009, 0.09924067556858063, 0.10201439261436462, 0.10341113805770874, 0.10097325593233109, 0.1021493449807167, 0.09763084352016449, 0.10083494335412979, 0.10259640216827393, 0.09981781244277954, 0.10121949762105942, 0.10045824944972992, 0.09975199401378632, 0.10348613560199738, 0.10194292664527893, 0.10437967628240585, 0.10036373138427734, 0.10137635469436646, 0.10044833272695541, 0.10098342597484589, 0.09818683564662933, 0.10156594961881638, 0.10071344673633575, 0.10368204116821289, 0.10274578630924225, 0.10031761974096298, 0.10110779851675034, 0.1044045239686966, 0.10043510794639587, 0.09994794428348541, 0.09886683523654938, 0.10323964059352875, 0.1020476296544075, 0.09795047342777252, 0.09876894950866699, 0.10792997479438782, 0.10041949152946472, 0.10307034850120544, 0.10331663489341736, 0.10652747005224228, 0.10086379945278168, 0.09871045500040054, 0.10619232803583145, 0.10104360431432724, 0.10208465903997421, 0.09742729365825653, 0.10707496851682663, 0.1006193608045578, 0.10313545167446136, 0.10421542078256607, 0.10050814598798752, 0.10419362038373947, 0.1026560440659523, 0.10198035836219788, 0.1047947034239769, 0.09934315830469131, 0.10266326367855072, 0.1025238186120987, 0.10057427734136581, 0.10973921418190002, 0.10172315686941147, 0.1001034677028656, 0.09611266851425171, 0.10317026078701019, 0.1018001139163971, 0.09958025813102722, 0.10825735330581665, 0.10442304611206055, 0.10441899299621582, 0.09839712083339691, 0.1002378761768341, 0.09977087378501892, 0.10571707785129547, 0.10039043426513672, 0.10042524337768555, 0.10436666756868362, 0.09683534502983093, 0.09971700608730316, 0.10103318840265274, 0.09804333746433258, 0.0999382734298706, 0.10296991467475891, 0.09958472847938538, 0.10269685834646225, 0.10118122398853302, 0.10101187229156494, 0.10028671473264694, 0.10222986340522766, 0.10224319994449615, 0.10141736268997192, 0.09753485023975372, 0.09677989035844803, 0.09789585322141647, 0.09892183542251587, 0.10685746371746063, 0.10536914318799973, 0.10270339250564575, 0.10481222718954086, 0.1025746613740921, 0.09560135006904602, 0.10116270929574966, 0.10296487808227539, 0.10001520067453384, 0.09838297218084335, 0.09940245747566223, 0.10185692459344864, 0.10211502760648727, 0.09944818913936615, 0.1046581119298935, 0.1041349470615387, 0.0979321300983429, 0.0982871949672699, 0.10264628380537033, 0.09910643845796585, 0.09965074062347412, 0.10220413655042648, 0.09898418933153152, 0.0999814048409462, 0.10094653069972992, 0.10734011232852936, 0.10356754064559937, 0.10207942873239517, 0.09886635094881058, 0.09518162161111832, 0.10057360678911209, 0.0847850888967514, 0.10350538790225983, 0.10069141536951065, 0.09541419893503189, 0.10072332620620728, 0.10216964036226273, 0.0959169864654541, 0.10022267699241638, 0.10090426355600357, 0.10310390591621399, 0.09622117131948471, 0.09321489930152893, 0.09874320775270462, 0.10650748014450073, 0.10012170672416687, 0.09905199706554413, 0.0993998721241951, 0.09624961018562317, 0.10206098109483719, 0.10151585191488266, 0.1059245765209198, 0.1052355021238327, 0.10320819914340973, 0.09743139147758484, 0.103009894490242, 0.09530042856931686, 0.09870346635580063, 0.10148131847381592, 0.10711467266082764, 0.09922187030315399, 0.09674184024333954, 0.09370247274637222, 0.09875071793794632, 0.1050664559006691, 0.09777526557445526, 0.0961054340004921, 0.09413372725248337, 0.10006942600011826, 0.10620525479316711, 0.09890738874673843, 0.10101470351219177, 0.0961189717054367, 0.10145110636949539, 0.10372209548950195, 0.10188963264226913, 0.1024087592959404, 0.0993376076221466, 0.10879257321357727, 0.1013641506433487, 0.10197673738002777, 0.10096243768930435, 0.10823284834623337, 0.10168460011482239, 0.09733462333679199, 0.10342401266098022, 0.10208839178085327, 0.10573727637529373, 0.10014361143112183, 0.09633760899305344, 0.10321003198623657, 0.10341408848762512, 0.0999683365225792, 0.0937316045165062, 0.10365079343318939, 0.10490572452545166, 0.1060655415058136, 0.09791408479213715, 0.10358116030693054, 0.09885476529598236, 0.10045118629932404, 0.10117118060588837, 0.1010231003165245, 0.09771884977817535, 0.09938299655914307, 0.10133760422468185, 0.1021420806646347, 0.10116392374038696, 0.10807301104068756, 0.10057012736797333, 0.10285136103630066, 0.1051335334777832, 0.10357653349637985, 0.10125543177127838, 0.09702134132385254, 0.09740829467773438, 0.09759281575679779, 0.09587180614471436, 0.10005874931812286, 0.10059534758329391, 0.1020849198102951, 0.1024249941110611, 0.10062326490879059, 0.09861908853054047, 0.09918463975191116, 0.09866943210363388, 0.0999651700258255, 0.10102344304323196, 0.10401739180088043, 0.09475106745958328, 0.10162557661533356, 0.10079236328601837, 0.09777535498142242, 0.10248635709285736, 0.09958798438310623, 0.10525965690612793, 0.1066247746348381, 0.1049618124961853, 0.10328437387943268, 0.10298945009708405, 0.10120636969804764, 0.10211235284805298, 0.1003040075302124, 0.10495597869157791, 0.10032352805137634, 0.10331137478351593, 0.10542571544647217, 0.1061323881149292, 0.09907791018486023, 0.09996084123849869, 0.10267134010791779, 0.10774968564510345, 0.0957937091588974, 0.09986615926027298, 0.10182986408472061, 0.10073444247245789, 0.09735623747110367, 0.10417917370796204, 0.1066870242357254, 0.09986084699630737, 0.10092465579509735, 0.10929526388645172, 0.09693829715251923, 0.10164107382297516, 0.10339710861444473, 0.10153503715991974, 0.09288057684898376, 0.10116259008646011, 0.09956733137369156, 0.10217367112636566, 0.10463175177574158, 0.1007496565580368, 0.10195617377758026, 0.10212254524230957, 0.10145110636949539, 0.1044747531414032, 0.1014016717672348, 0.1010696142911911, 0.09342245012521744, 0.10161501169204712, 0.1037188172340393, 0.1055203452706337, 0.09492631256580353, 0.0982438325881958, 0.09724602848291397, 0.11511939764022827, 0.10545438528060913, 0.09637578576803207, 0.10082446038722992, 0.09683944284915924, 0.0992872416973114, 0.10221944749355316, 0.09972386807203293, 0.10536853224039078, 0.101253941655159, 0.1055895984172821, 0.10370280593633652, 0.10041457414627075, 0.098886638879776, 0.09754128754138947, 0.09532898664474487, 0.09949875622987747, 0.1023113802075386, 0.09876079112291336, 0.09682165831327438, 0.0978255346417427, 0.10116115212440491, 0.0998176708817482, 0.10736030340194702, 0.09983149915933609, 0.10019738227128983, 0.09878970682621002, 0.10466410219669342, 0.09842145442962646, 0.10115008056163788, 0.10034899413585663, 0.09848073869943619, 0.10116638243198395, 0.1025569885969162, 0.10201093554496765, 0.10445953160524368, 0.09710662066936493, 0.0991855263710022, 0.09957484900951385, 0.10031157732009888, 0.10353104770183563, 0.09819614887237549, 0.10194952040910721, 0.10235723853111267, 0.0994090586900711, 0.10005425661802292, 0.09869872033596039, 0.10905234515666962, 0.0985683724284172, 0.10536953061819077, 0.10392867028713226, 0.10096792876720428, 0.09967359155416489, 0.0962194949388504, 0.1008945181965828, 0.09992332756519318, 0.10017861425876617, 0.10001534968614578, 0.1030120849609375, 0.11172482371330261, 0.09925621747970581, 0.09951077401638031, 0.09713583439588547, 0.09933099150657654, 0.10471612960100174, 0.09978355467319489, 0.10182039439678192, 0.0990065336227417, 0.09834538400173187, 0.10583452135324478, 0.09823582321405411, 0.09755712747573853, 0.09964849799871445, 0.10102963447570801, 0.10227292776107788, 0.09584984183311462, 0.10466346144676208, 0.09273350238800049, 0.10008886456489563, 0.09955739974975586, 0.11245617270469666, 0.09947583079338074, 0.10060427337884903, 0.10196854919195175, 0.08941502869129181, 0.09874869138002396, 0.09954589605331421, 0.0995665043592453, 0.10297276079654694, 0.09346519410610199, 0.10182680189609528, 0.10566423088312149, 0.09996010363101959, 0.09604401886463165, 0.10450456291437149, 0.10085205733776093, 0.10052849352359772, 0.09382231533527374, 0.09903237223625183, 0.09951665997505188, 0.10294385999441147, 0.0987776666879654, 0.10454919189214706, 0.10011541843414307, 0.09771734476089478, 0.10198802500963211, 0.10684051364660263, 0.10244986414909363, 0.10031399875879288, 0.10720548033714294, 0.10105577111244202, 0.09939023107290268, 0.1095561757683754, 0.09958959370851517, 0.09268029034137726, 0.0975262001156807, 0.09810881316661835, 0.0991050973534584, 0.09889599680900574, 0.09973207116127014, 0.1014295145869255, 0.10500144958496094, 0.10098116099834442, 0.09784651547670364, 0.0988999754190445, 0.10334283113479614, 0.10587617754936218, 0.10290966928005219, 0.10186847299337387, 0.10352582484483719, 0.09656769782304764, 0.10296143591403961, 0.09901744872331619, 0.10011644661426544, 0.10029736161231995, 0.10522177070379257, 0.10502255707979202, 0.10003402084112167, 0.10480281710624695, 0.10354208946228027, 0.10023698210716248, 0.10348842293024063, 0.09829012304544449, 0.10361921042203903, 0.10113991796970367, 0.09701217710971832, 0.1036917120218277, 0.10057548433542252, 0.10168161988258362, 0.09915366023778915, 0.10299836844205856, 0.09914810955524445, 0.09889836609363556, 0.09736800193786621, 0.10671928524971008, 0.099213145673275, 0.1037270650267601, 0.10086192190647125, 0.09805214405059814, 0.10071346908807755, 0.10327933728694916, 0.09860756993293762, 0.10626165568828583, 0.09636396169662476, 0.0946129634976387, 0.0981929823756218, 0.09500005841255188, 0.10600174963474274, 0.10129722207784653, 0.10169361531734467, 0.10022362321615219, 0.10192649066448212, 0.09528075903654099, 0.10055804997682571, 0.10366424173116684, 0.09930118918418884, 0.10387734323740005, 0.09831477701663971, 0.10290296375751495, 0.09841950237751007, 0.10274423658847809, 0.101714126765728, 0.08379504084587097, 0.09810216724872589, 0.09917674213647842, 0.09573912620544434, 0.10427822172641754, 0.10554447025060654, 0.10173998028039932, 0.10602171719074249, 0.0979531928896904, 0.10202072560787201, 0.10099027305841446, 0.09969431906938553, 0.10456013679504395, 0.106938436627388, 0.10154297947883606, 0.10259103029966354, 0.10436698794364929, 0.10199540853500366, 0.10327073931694031, 0.10355003923177719, 0.09870494902133942, 0.09951159358024597, 0.09539686143398285, 0.10130735486745834, 0.09731203317642212, 0.09934413433074951, 0.09805358946323395, 0.10084614902734756, 0.09784480929374695, 0.1045166477560997, 0.10331602394580841, 0.0938449501991272, 0.1073562428355217, 0.10375791043043137, 0.10377802699804306, 0.10286600887775421, 0.09871126711368561, 0.09864768385887146, 0.10486333817243576, 0.09934905916452408, 0.10204014927148819, 0.10023796558380127, 0.09928295761346817, 0.09942804276943207, 0.09665653109550476, 0.09993276745080948, 0.10063609480857849, 0.10440693795681, 0.09851881861686707, 0.09896595776081085, 0.09743539988994598, 0.103019118309021, 0.09754301607608795, 0.09733540564775467, 0.1038375049829483, 0.09646351635456085, 0.1022554561495781, 0.09742133319377899, 0.09946183860301971, 0.10233964025974274, 0.10406389087438583, 0.09873560816049576, 0.10212668031454086, 0.10299557447433472, 0.11182846128940582, 0.09757855534553528, 0.10652747005224228, 0.10932032018899918, 0.10235253721475601, 0.09702330082654953, 0.09908805787563324, 0.10501820594072342, 0.09562298655509949, 0.09657813608646393, 0.09501302242279053, 0.10179886221885681, 0.09679663181304932, 0.10548277199268341, 0.10682045668363571, 0.09876054525375366, 0.10839579999446869, 0.09401845186948776, 0.09908697009086609, 0.09952285140752792, 0.10338706523180008, 0.10218296200037003, 0.09861273318529129, 0.09712185710668564, 0.10244503617286682, 0.10185006260871887, 0.10151131451129913, 0.09791126847267151, 0.0985606387257576, 0.1038895696401596, 0.10541921854019165, 0.10195346176624298, 0.1011621356010437, 0.10144560039043427, 0.10186652839183807, 0.09973970055580139, 0.09967199712991714, 0.10640228539705276, 0.09431857615709305, 0.10301922261714935, 0.09987389296293259, 0.0984744131565094, 0.10072747617959976, 0.10196244716644287, 0.10153476148843765, 0.0982457622885704, 0.09845089167356491, 0.10026901960372925, 0.09567083418369293, 0.10096804052591324, 0.09841693192720413, 0.10128096491098404, 0.09770915657281876, 0.1021323949098587, 0.09914664924144745, 0.10195676237344742, 0.10087800025939941, 0.10165029019117355, 0.09995228797197342, 0.09968698769807816, 0.10188622772693634, 0.1018088236451149, 0.09734287858009338, 0.0994989350438118, 0.10053122043609619, 0.10325948148965836, 0.09649032354354858, 0.10520853102207184, 0.099636971950531, 0.10536541044712067, 0.10477184504270554, 0.10344403982162476, 0.10297214984893799, 0.11019109189510345, 0.10177192091941833, 0.102511927485466, 0.10361463576555252, 0.10322574526071548, 0.09831187129020691, 0.1089094877243042, 0.1033361405134201, 0.10079073905944824, 0.10156232118606567, 0.09902064502239227, 0.10837019234895706, 0.10805901885032654, 0.09901173412799835, 0.09963608533143997, 0.10314861685037613, 0.09981455653905869, 0.10138179361820221, 0.10262265056371689, 0.10165826976299286, 0.10234398394823074, 0.10570700466632843, 0.09878619760274887, 0.09381455183029175, 0.09811817109584808, 0.10385699570178986, 0.1017727255821228, 0.10221840441226959, 0.10498543828725815, 0.10114772617816925, 0.09912161529064178, 0.09989053755998611, 0.10080312192440033, 0.09893535077571869, 0.09929128736257553, 0.09721096605062485, 0.10021357238292694, 0.10019606351852417, 0.09974540770053864, 0.09667789191007614, 0.0979316458106041, 0.10524744540452957, 0.10207182914018631, 0.09919336438179016, 0.09877198934555054, 0.09742118418216705, 0.09740438312292099, 0.10320412367582321, 0.09913362562656403, 0.10244674980640411, 0.10031239688396454, 0.10578900575637817, 0.10168997943401337, 0.08876389265060425, 0.10050348937511444, 0.10246819257736206, 0.107154481112957, 0.10846906900405884, 0.10248865932226181, 0.0998997837305069, 0.09936568140983582, 0.10649137943983078, 0.10040339082479477, 0.10278376936912537, 0.10491746664047241, 0.09930987656116486, 0.09997333586215973, 0.09610845893621445, 0.10759963095188141, 0.1025584414601326, 0.09845421463251114, 0.09920891374349594, 0.1002923995256424, 0.09770417213439941, 0.09648831188678741, 0.09512826055288315, 0.099968321621418, 0.10327798128128052, 0.10132542252540588, 0.09974048286676407, 0.0989864245057106, 0.09944774210453033, 0.09780598431825638, 0.09553828090429306, 0.10195612907409668, 0.10101020336151123, 0.09854473918676376, 0.10195573419332504, 0.10164205729961395, 0.09307610243558884, 0.10130009055137634, 0.09949874877929688, 0.09830384701490402, 0.1062755212187767, 0.10217142105102539, 0.09972188621759415, 0.09957640618085861, 0.10174303501844406, 0.08928164094686508, 0.10098227858543396, 0.10715742409229279, 0.10609699040651321, 0.09870395809412003, 0.09670457243919373, 0.09961704909801483, 0.10064481198787689, 0.09755778312683105, 0.10290474444627762, 0.0994681566953659, 0.10145046561956406, 0.10418091714382172, 0.09870859980583191, 0.1032269075512886, 0.10075010359287262, 0.09864431619644165, 0.1020566076040268, 0.11391565948724747, 0.1015971302986145, 0.10366601496934891, 0.09891942888498306, 0.10377275198698044, 0.0983799397945404, 0.10167843103408813, 0.11229009181261063, 0.10100564360618591, 0.09834270924329758, 0.09656760841608047, 0.09814128279685974, 0.10147193819284439, 0.10121506452560425, 0.09807509183883667, 0.09564906358718872, 0.09735546261072159, 0.10215550661087036, 0.104134202003479, 0.10439810901880264, 0.10201455652713776, 0.10098583251237869, 0.10204008221626282, 0.10756133496761322, 0.1024426519870758, 0.09692174196243286, 0.10013405978679657, 0.09299375861883163, 0.09776002168655396, 0.09692418575286865, 0.1008218452334404, 0.104435496032238, 0.09962987154722214, 0.10272689908742905, 0.10392146557569504, 0.099368155002594, 0.0986122190952301, 0.10815075784921646, 0.09387297928333282, 0.09583600610494614, 0.1015276312828064, 0.10098278522491455, 0.10235480219125748, 0.09977501630783081, 0.10102173686027527, 0.10728287696838379, 0.10778213292360306, 0.10108685493469238, 0.09704034775495529, 0.10176005959510803, 0.10071001201868057, 0.1039847582578659, 0.10289958119392395, 0.10083324462175369, 0.09899518638849258, 0.1024773120880127, 0.10351619124412537, 0.10089678317308426, 0.09446711093187332, 0.09975117444992065, 0.10579846054315567, 0.09995388984680176, 0.09702759236097336, 0.10012239217758179, 0.09664137661457062, 0.10294130444526672, 0.09985780715942383, 0.103016197681427, 0.09928655624389648, 0.10379183292388916, 0.10445056110620499, 0.10316281765699387, 0.0974135771393776, 0.1008119285106659, 0.10539373755455017, 0.10225895047187805, 0.10483671724796295, 0.09486280381679535, 0.10818158090114594, 0.10282400995492935, 0.10159067809581757, 0.10567605495452881, 0.09694564342498779, 0.09765072166919708, 0.1010495275259018, 0.10089142620563507, 0.10092559456825256, 0.1009032279253006, 0.0992690771818161, 0.09791728854179382, 0.09569137543439865, 0.10432897508144379, 0.10043394565582275, 0.10252711176872253, 0.09969636797904968, 0.09665022790431976, 0.10036253929138184, 0.10281050205230713, 0.09830452501773834, 0.1017400324344635, 0.10296808183193207, 0.09842842817306519, 0.09802015125751495, 0.09645118564367294, 0.09892895072698593, 0.10137950628995895, 0.09883610159158707, 0.10431570559740067, 0.0900363177061081, 0.10431654751300812, 0.10150917619466782, 0.10341490805149078, 0.10441401600837708, 0.0960300862789154, 0.10316191613674164, 0.09798182547092438, 0.10176727920770645, 0.10001680999994278, 0.09821588546037674, 0.10131283849477768, 0.10363409668207169, 0.09900873899459839, 0.10052158683538437, 0.0959353968501091, 0.10367580503225327, 0.10006898641586304, 0.10068099200725555, 0.10504022240638733, 0.09612756967544556, 0.09855819493532181, 0.10622663050889969, 0.10793279111385345, 0.10319767892360687, 0.09871536493301392, 0.10856493562459946, 0.0936727300286293, 0.099076047539711, 0.10702446848154068, 0.10114933550357819, 0.10579777508974075, 0.09727267920970917, 0.09801605343818665, 0.10212793201208115, 0.1081525981426239, 0.10315893590450287, 0.10498625040054321, 0.10194219648838043, 0.1064397320151329, 0.09813814610242844, 0.09773778915405273, 0.0983349084854126, 0.09207561612129211, 0.10361094027757645, 0.09624581038951874, 0.10913944244384766, 0.105095773935318, 0.09970755875110626, 0.09732691198587418, 0.09979311376810074, 0.10020794719457626, 0.10134591907262802, 0.0982668548822403, 0.10351735353469849, 0.0957600399851799, 0.09912949800491333, 0.09693486988544464, 0.10597115755081177, 0.10608029365539551, 0.0970311239361763, 0.0959220677614212, 0.09949404001235962, 0.10263573378324509, 0.10741429775953293, 0.10134980827569962, 0.10122878104448318, 0.10010650753974915, 0.09619159996509552, 0.10577131062746048, 0.10002408176660538, 0.10549020767211914, 0.10067397356033325, 0.1000283882021904, 0.10025686770677567, 0.10220539569854736, 0.1059487983584404, 0.10186856240034103, 0.09996345639228821, 0.10043273866176605, 0.09817995876073837, 0.10419294238090515, 0.10036083310842514, 0.09881584346294403, 0.09568662941455841, 0.09785772860050201, 0.10272274166345596, 0.10259444266557693, 0.1010533943772316, 0.10205692052841187, 0.0965842679142952, 0.10138967633247375, 0.10211513191461563, 0.09792632609605789, 0.09788209944963455, 0.10139985382556915, 0.0893249660730362, 0.10174280405044556, 0.09940828382968903, 0.09724738448858261, 0.09829504787921906, 0.10538686066865921, 0.09981025010347366, 0.09938980638980865, 0.10281363874673843, 0.10209725797176361, 0.10132984817028046, 0.09548082947731018, 0.10242095589637756, 0.09902097284793854, 0.09729825705289841, 0.10473531484603882, 0.09824207425117493, 0.1027662605047226, 0.10457286983728409, 0.10375957190990448, 0.10191698372364044, 0.09678222239017487, 0.09954652190208435, 0.0994766429066658, 0.1076299175620079, 0.10192754119634628, 0.10472176223993301, 0.09849344938993454, 0.10844794660806656, 0.0995783656835556, 0.10747986286878586, 0.09743378311395645, 0.10263804346323013, 0.09928617626428604, 0.09930934011936188, 0.1001163199543953, 0.09827055782079697, 0.10070504248142242, 0.10388347506523132, 0.10208958387374878, 0.09963973611593246, 0.09716606885194778, 0.10024027526378632, 0.09761021286249161, 0.10348406434059143, 0.10192059725522995, 0.09905550628900528, 0.11139893531799316, 0.10245972871780396, 0.09886299818754196, 0.09917908906936646, 0.10118745267391205, 0.0989619567990303, 0.09875021874904633, 0.10663561522960663, 0.09894978255033493, 0.11279846727848053, 0.10450533777475357, 0.10180607438087463, 0.1054677739739418, 0.10166463255882263, 0.1048571988940239, 0.09719626605510712, 0.10176268219947815, 0.09869855642318726, 0.09622375667095184, 0.10048098862171173, 0.09939439594745636, 0.09877026826143265, 0.09697209298610687, 0.10486876964569092, 0.09871012717485428, 0.10025462508201599, 0.1018330529332161, 0.09601797163486481, 0.1000327542424202, 0.10527646541595459, 0.09830274432897568, 0.09216713905334473, 0.10138770192861557, 0.10271446406841278, 0.10048723220825195, 0.10029981285333633, 0.10349035263061523, 0.10463590174913406, 0.10353788733482361, 0.0984002947807312, 0.10145408660173416, 0.0991460382938385, 0.10142534971237183, 0.10182032734155655, 0.09791181981563568, 0.10144387185573578, 0.10020102560520172, 0.10442593693733215, 0.1010386273264885, 0.0952407717704773, 0.09842592477798462, 0.10289963334798813, 0.09972258657217026, 0.09949687868356705, 0.10362521559000015, 0.10288751870393753, 0.09821547567844391, 0.09653443098068237, 0.09947863221168518, 0.10348586738109589, 0.10129464417695999, 0.09807360917329788, 0.10300488770008087, 0.09932452440261841, 0.09941767901182175, 0.0996503084897995, 0.09953881800174713, 0.1004822701215744, 0.11000864207744598, 0.09818210452795029, 0.10034403949975967, 0.10364903509616852, 0.10180778801441193, 0.09947134554386139, 0.09795435518026352, 0.1009356826543808, 0.10151904821395874, 0.10285477340221405, 0.09692428261041641, 0.09878140687942505, 0.10560137033462524, 0.10461824387311935, 0.10552775859832764, 0.10483752936124802, 0.10099516808986664, 0.1106455996632576, 0.0997476726770401, 0.10502700507640839, 0.10157273709774017, 0.10099530965089798, 0.1010594516992569, 0.09818645566701889, 0.10201894491910934, 0.09974949061870575, 0.10002285242080688, 0.10429521650075912, 0.09807390719652176, 0.09791164100170135, 0.10656730830669403, 0.10437434911727905, 0.11020557582378387, 0.0955781415104866, 0.09820989519357681, 0.10125469416379929, 0.09889689087867737, 0.1026458889245987, 0.10704256594181061, 0.0992569848895073, 0.10159040987491608, 0.10284695029258728, 0.09878930449485779, 0.1040496677160263, 0.10175010561943054, 0.10244986414909363, 0.10127917677164078, 0.09542573988437653, 0.1004192978143692, 0.09908981621265411, 0.1012454479932785, 0.10078471899032593, 0.10325534641742706, 0.10183868557214737, 0.10704256594181061, 0.10196205228567123, 0.10180961340665817, 0.10175127536058426, 0.10131579637527466, 0.10058493167161942, 0.10132207721471786, 0.09890437871217728, 0.09848770499229431, 0.0958942249417305, 0.10066988319158554, 0.10330219566822052, 0.10298283398151398, 0.10686247050762177, 0.09761976450681686, 0.10241682082414627, 0.10121224075555801, 0.09439719468355179, 0.09686629474163055, 0.10080885887145996, 0.09981455653905869, 0.10405982285737991, 0.09841775894165039, 0.1017996072769165, 0.10057628154754639, 0.10254628956317902, 0.09843643009662628, 0.10601811110973358, 0.10047595202922821, 0.1037578284740448, 0.09725167602300644, 0.09699633717536926, 0.10564582794904709, 0.10281294584274292, 0.09859112650156021, 0.09854043275117874, 0.10165621340274811, 0.10059978067874908, 0.10004973411560059, 0.09866777062416077, 0.1028592512011528, 0.10337100178003311, 0.10177092999219894, 0.10554313659667969, 0.10394252091646194, 0.09996750950813293, 0.1025303453207016, 0.10023139417171478, 0.10128089040517807, 0.10189381241798401, 0.10144378244876862, 0.10456666350364685, 0.10097045451402664, 0.10636615008115768, 0.09877689927816391, 0.09811879694461823, 0.0987023413181305, 0.09926006197929382, 0.10040146112442017, 0.09502654522657394, 0.10604479163885117, 0.0984373688697815, 0.10790162533521652, 0.09843987226486206, 0.10019384324550629, 0.10190572589635849, 0.09902499616146088, 0.09966574609279633, 0.1025436595082283, 0.09838592261075974, 0.10102301836013794, 0.10595892369747162, 0.09882242977619171, 0.10115568339824677, 0.10051068663597107, 0.10073772072792053, 0.10304964333772659, 0.1032484844326973, 0.10026504099369049, 0.10465583950281143, 0.0986548364162445, 0.10413818061351776, 0.10298044979572296, 0.10013575106859207, 0.0968029648065567, 0.09963522851467133, 0.09316061437129974, 0.10070915520191193, 0.09944787621498108, 0.09972383081912994, 0.1043703556060791, 0.10194425284862518, 0.10542365163564682, 0.10113468021154404, 0.1028408631682396, 0.10191492736339569, 0.09891825914382935, 0.10102337598800659, 0.10553167015314102, 0.10168737918138504, 0.10645453631877899, 0.09865950047969818, 0.1007971465587616, 0.09605459123849869, 0.09985500574111938, 0.09988722205162048, 0.09886713325977325, 0.1065538227558136, 0.10097935050725937, 0.09946154803037643, 0.10149015486240387, 0.10076627880334854, 0.11041267961263657, 0.1008739098906517, 0.10131252557039261, 0.10163518041372299, 0.10123306512832642, 0.10190121829509735, 0.10160249471664429, 0.10104023665189743, 0.10235859453678131, 0.1005447655916214, 0.10629568248987198, 0.0992986261844635, 0.10302114486694336, 0.09989076852798462, 0.10163282603025436, 0.09863701462745667, 0.09726732969284058, 0.09831054508686066, 0.1029166430234909, 0.1024371087551117, 0.09268029034137726, 0.10294440388679504, 0.09341467171907425, 0.09880292415618896, 0.10708911716938019, 0.0998239666223526, 0.10147977620363235, 0.09976121783256531, 0.09854330867528915, 0.09943622350692749, 0.1028205007314682, 0.09757974743843079, 0.10039003193378448, 0.10375455766916275, 0.10070183128118515, 0.10122458636760712, 0.10808667540550232, 0.09780912101268768, 0.10197766125202179, 0.10414255410432816, 0.09303979575634003, 0.10123024880886078, 0.0951104462146759, 0.09728040546178818, 0.10371564328670502, 0.10030950605869293, 0.09966101497411728, 0.09778900444507599, 0.10384107381105423, 0.10159622132778168, 0.10117572546005249, 0.09699437022209167, 0.10022282600402832, 0.1014188900589943, 0.0994003489613533, 0.09714307636022568, 0.10135439038276672, 0.10098157823085785, 0.09562254697084427, 0.0999189168214798, 0.10259947925806046, 0.10105354338884354, 0.10367593169212341, 0.09822266548871994, 0.10032616555690765, 0.10011256486177444, 0.10491985827684402, 0.09949138015508652, 0.10530063509941101, 0.0998990535736084, 0.1061239242553711, 0.10887262225151062, 0.09703855216503143, 0.10504148155450821, 0.09771145135164261, 0.09893116354942322, 0.10133248567581177, 0.1000063344836235, 0.10475295782089233, 0.10374432802200317, 0.10082869976758957, 0.09613990783691406, 0.09839639812707901, 0.10352186113595963, 0.10004134476184845, 0.09842906892299652, 0.09858101606369019, 0.10586771368980408, 0.09779013693332672, 0.09915317595005035, 0.10340477526187897, 0.10551580786705017, 0.10615123808383942, 0.09747135639190674, 0.103400319814682, 0.10139572620391846, 0.09948532283306122, 0.09934765100479126, 0.09920916706323624, 0.10105463117361069, 0.10468019545078278, 0.1002848744392395, 0.10691997408866882, 0.10003485530614853, 0.10367580503225327, 0.1084117740392685, 0.10132461786270142, 0.09939734637737274, 0.10642091184854507, 0.09794668853282928, 0.08980056643486023, 0.10098319500684738, 0.09922584146261215, 0.1063050627708435, 0.10366566479206085, 0.10193255543708801, 0.09856917709112167, 0.09813255816698074, 0.10070624947547913, 0.0996520146727562, 0.09636416286230087, 0.10297413170337677, 0.10074229538440704, 0.09742844849824905, 0.10406789183616638, 0.10012141615152359, 0.10051140189170837, 0.10020030289888382, 0.09771288931369781, 0.09874224662780762, 0.10506659746170044, 0.10424435138702393, 0.09831187129020691, 0.09921088814735413, 0.10210374742746353, 0.10240232944488525, 0.10574224591255188, 0.09258204698562622, 0.10010360181331635, 0.09639154374599457, 0.09937746822834015, 0.09938335418701172, 0.10287860035896301, 0.09766785055398941, 0.10317857563495636, 0.10070320963859558, 0.10350316762924194, 0.1012793779373169, 0.09838233143091202, 0.10651262104511261, 0.10014926642179489, 0.10039925575256348, 0.10114216804504395, 0.10246112942695618, 0.10053622722625732, 0.10199790447950363, 0.09629406034946442, 0.0970921516418457, 0.10189392417669296, 0.10035692900419235, 0.09920144826173782, 0.103610560297966, 0.10060712695121765, 0.1014268547296524, 0.10001041740179062, 0.09867939352989197, 0.10493946820497513, 0.09670956432819366, 0.10179486870765686, 0.09848183393478394, 0.10024090111255646, 0.0977822095155716, 0.09953371435403824, 0.10150366276502609, 0.10016798228025436, 0.09892153739929199, 0.10098820924758911, 0.1030750572681427, 0.0942734032869339, 0.10685740411281586, 0.10073821991682053, 0.10201063752174377, 0.09799496084451675, 0.10599708557128906, 0.10210444033145905, 0.10109089314937592, 0.0952836349606514, 0.09871208667755127, 0.10047607123851776, 0.09903138130903244, 0.10437311232089996, 0.10222692787647247, 0.09886819869279861, 0.09738920629024506, 0.101512610912323, 0.09870974719524384, 0.097327820956707, 0.10106167942285538, 0.10212546586990356, 0.0997234582901001, 0.10069336742162704, 0.10261952131986618, 0.0998380184173584, 0.10174097865819931, 0.10791262239217758, 0.09926650673151016, 0.10361839830875397, 0.10405927151441574, 0.10038355737924576, 0.10334689915180206, 0.09948617964982986, 0.1019379273056984, 0.10569658875465393, 0.1001807153224945, 0.10278759896755219, 0.10428952425718307, 0.0994744822382927, 0.09912203252315521, 0.09991830587387085, 0.10136144608259201, 0.09984703361988068, 0.10026378929615021, 0.10252755880355835, 0.09990771114826202, 0.09596040099859238, 0.10235363245010376, 0.0975601077079773, 0.10638409852981567, 0.10357597470283508, 0.09980309009552002, 0.10365384817123413, 0.09629834443330765, 0.10071000456809998, 0.10501831769943237, 0.11155466735363007, 0.09783510863780975, 0.09555573761463165, 0.10214352607727051, 0.0990617573261261, 0.09334193170070648, 0.0986161082983017, 0.10480546951293945, 0.10059215873479843, 0.10264758765697479, 0.10802233219146729, 0.09343293309211731, 0.10132963955402374, 0.10046412795782089, 0.0987720713019371, 0.10519007593393326, 0.09534594416618347, 0.08971863985061646, 0.10448925197124481, 0.10243073850870132, 0.09995198994874954, 0.09996680170297623, 0.09853281080722809, 0.10466709733009338, 0.10139158368110657, 0.10921964794397354, 0.0977320745587349, 0.10606928914785385, 0.10226599872112274, 0.09752742201089859, 0.0949668362736702, 0.1036648377776146, 0.09205803275108337, 0.10061322152614594, 0.10384399443864822, 0.10517235845327377, 0.10790053755044937, 0.10005699843168259, 0.10023796558380127, 0.09993021190166473, 0.10170953720808029, 0.105500727891922, 0.09688722342252731, 0.10580722987651825, 0.0988365039229393, 0.10167628526687622, 0.1003662496805191, 0.09645947813987732, 0.09856899082660675, 0.09967294335365295, 0.10196290165185928, 0.1044122725725174, 0.09779144823551178, 0.1046236976981163, 0.10002778470516205, 0.09899573028087616, 0.09948605298995972, 0.1004994586110115, 0.09939493983983994, 0.10306832939386368, 0.10501138120889664, 0.10848617553710938, 0.09524781256914139, 0.10679638385772705, 0.10422810912132263, 0.09680280834436417, 0.10534689575433731, 0.10200241208076477, 0.10975562036037445, 0.1021222472190857, 0.10224790871143341, 0.09553683549165726, 0.10068286210298538, 0.09955818951129913, 0.09860873967409134, 0.10801426321268082, 0.09925936162471771, 0.10064235329627991, 0.09620579332113266, 0.09978654980659485, 0.10336878895759583, 0.10331286489963531, 0.10555306822061539, 0.10265408456325531, 0.10419434309005737, 0.09987727552652359, 0.10686536878347397, 0.10545787215232849, 0.09652350097894669, 0.09955480694770813, 0.09876538068056107, 0.10398991405963898, 0.10034826397895813, 0.09965287148952484, 0.09967552125453949, 0.10263995826244354, 0.10073219984769821, 0.10392323136329651, 0.1029675230383873, 0.10216469317674637, 0.09811558574438095, 0.10027866065502167, 0.09950080513954163, 0.10357366502285004, 0.10487132519483566, 0.0961957648396492, 0.10410758852958679, 0.09925238788127899, 0.10131824016571045, 0.09959340840578079, 0.09621954709291458, 0.10601399838924408, 0.10111471265554428, 0.0978580117225647, 0.09929554909467697, 0.0977531224489212, 0.10329797863960266, 0.10214990377426147, 0.10092832148075104, 0.10241600126028061, 0.09954690933227539, 0.10181279480457306, 0.09915071725845337, 0.10001534968614578, 0.10479066520929337, 0.10620110481977463, 0.09800352156162262, 0.10614752769470215, 0.10057360678911209, 0.1017814576625824, 0.08910588920116425, 0.107143335044384, 0.09749788045883179, 0.0995795801281929, 0.10382296144962311, 0.09704314172267914, 0.10408727824687958, 0.09616764634847641, 0.10090477019548416, 0.10211978107690811, 0.10406246036291122, 0.09949731826782227, 0.10430595278739929, 0.10089156031608582, 0.10277155041694641, 0.10224072635173798, 0.1003817692399025, 0.10217682272195816, 0.09760245680809021, 0.09763174504041672, 0.10259175300598145, 0.10045111924409866, 0.09913022816181183, 0.10136637091636658, 0.09575552493333817, 0.10130268335342407, 0.10395825654268265, 0.10199405252933502, 0.09903636574745178, 0.10519282519817352, 0.10779344290494919, 0.09692072123289108, 0.10027352720499039, 0.09922544658184052, 0.1029740497469902, 0.0949212908744812, 0.10294036567211151, 0.0969935953617096, 0.09754970669746399, 0.09983297437429428, 0.10118433088064194, 0.10166267305612564, 0.0999763160943985, 0.0958770364522934, 0.10078535974025726, 0.10550475865602493, 0.10108177363872528, 0.09772659838199615, 0.10404814779758453, 0.10066820681095123, 0.09872500598430634, 0.09175830334424973, 0.10140765458345413, 0.09994713217020035, 0.09893478453159332, 0.10626900941133499, 0.09852626174688339, 0.10428464412689209, 0.1046956479549408, 0.10296568274497986, 0.09491544216871262, 0.09793833643198013, 0.10341934859752655, 0.09628230333328247, 0.09805232286453247, 0.10744787752628326, 0.09936775267124176, 0.09948159009218216, 0.1012478768825531, 0.10199293494224548, 0.09723500907421112, 0.10750867426395416, 0.10500713437795639, 0.1050635501742363, 0.10100234299898148, 0.10537377744913101, 0.09911574423313141, 0.09812351316213608, 0.10019940137863159, 0.10356458276510239, 0.09779061377048492, 0.10413842648267746, 0.10262063890695572, 0.09766123443841934, 0.09836697578430176, 0.10078433156013489, 0.10140791535377502, 0.09651092439889908, 0.09814123064279556, 0.10384666174650192, 0.10619398206472397, 0.09744042158126831, 0.10690092295408249, 0.10709203779697418, 0.10077693313360214, 0.10336215794086456, 0.1014663353562355, 0.10664813965559006, 0.09561784565448761, 0.10628658533096313, 0.10005199164152145, 0.10131260752677917, 0.10241935402154922, 0.09891261160373688, 0.10742376744747162, 0.09777894616127014, 0.10066276043653488, 0.10106798261404037, 0.10361745953559875, 0.1025787815451622, 0.1012062281370163, 0.10056415945291519, 0.10206354409456253, 0.09882709383964539, 0.0984247624874115, 0.0998813584446907, 0.09699095040559769, 0.1022120788693428, 0.09908254444599152, 0.10061484575271606, 0.10109839588403702, 0.1063486635684967, 0.09675341844558716, 0.10151803493499756, 0.09873354434967041, 0.10006023943424225, 0.09888125956058502, 0.10228047519922256, 0.10224530100822449, 0.08506843447685242, 0.10711397975683212, 0.1009652316570282, 0.09697384387254715, 0.10422724485397339, 0.10134236514568329, 0.09841416776180267, 0.0989752858877182, 0.09368717670440674, 0.09995437413454056, 0.10380730032920837, 0.09907367080450058, 0.09863819181919098, 0.0994313657283783, 0.09877736866474152, 0.09543740749359131, 0.10846665501594543, 0.09396813064813614, 0.09973247349262238, 0.09762711077928543, 0.09699717909097672, 0.09880487620830536, 0.10294035822153091, 0.10689093172550201, 0.09868917614221573, 0.1024981215596199, 0.11068741232156754, 0.09934980422258377, 0.10188379138708115, 0.11062508076429367, 0.1017482727766037, 0.09267182648181915, 0.09995454549789429, 0.11224956065416336, 0.10348586738109589, 0.09554174542427063, 0.09980273246765137, 0.10179934650659561, 0.0997631773352623, 0.09848490357398987, 0.09681452065706253, 0.09793131053447723, 0.10618554055690765, 0.10633601248264313, 0.09795376658439636, 0.09392035007476807, 0.10376996546983719, 0.09651754796504974, 0.10206972062587738, 0.1058448851108551, 0.10527868568897247, 0.10429420322179794, 0.10119674354791641, 0.10268247872591019, 0.10205000638961792, 0.10646097362041473, 0.10212277621030807, 0.09940866380929947, 0.09865950047969818, 0.09887183457612991, 0.09919468313455582, 0.1039813682436943, 0.09746506810188293, 0.09977921843528748, 0.09775900840759277, 0.10057848691940308, 0.09752555936574936, 0.10180309414863586, 0.09922931343317032, 0.09845516830682755, 0.10205201804637909, 0.097269706428051, 0.09730156511068344, 0.1013946533203125, 0.10510540008544922, 0.09490887820720673, 0.10033351927995682, 0.0992337018251419, 0.09571743756532669, 0.09160732477903366, 0.10592343658208847, 0.10116668790578842, 0.10290366411209106, 0.10069610178470612, 0.09759411215782166, 0.09970111399888992, 0.0980953499674797, 0.09552587568759918, 0.10327392816543579, 0.1043473407626152, 0.09981867671012878, 0.10628989338874817, 0.08990266919136047, 0.1041993796825409, 0.10516481846570969, 0.10184460133314133, 0.10515005141496658, 0.09408819675445557, 0.09863533079624176, 0.09654814004898071, 0.10740231722593307, 0.10134454071521759, 0.10243579000234604, 0.10419571399688721, 0.09903967380523682, 0.10039573907852173, 0.09942551702260971, 0.10663967579603195, 0.09973110258579254, 0.09915511310100555, 0.10445758700370789, 0.09957447648048401, 0.1015467494726181, 0.10668011754751205, 0.10766874253749847, 0.10544942319393158, 0.09975888580083847, 0.10216006636619568, 0.09869751334190369, 0.09742750227451324, 0.09954109787940979, 0.10728780925273895, 0.10306195914745331, 0.09935502707958221, 0.09935502707958221, 0.1006915420293808, 0.10531537979841232, 0.09752900898456573, 0.09803850948810577, 0.10205438733100891, 0.09591180831193924, 0.10110415518283844, 0.10421648621559143, 0.09571138769388199, 0.10400170087814331, 0.09886105358600616, 0.1081693023443222, 0.10395191609859467, 0.10212307423353195, 0.10572285205125809, 0.09938791394233704, 0.10255815088748932, 0.10087113827466965, 0.09792612493038177, 0.10127069801092148, 0.0982702225446701, 0.09744317829608917, 0.098701611161232, 0.10066913068294525, 0.10165391117334366, 0.10420465469360352, 0.09745098650455475, 0.0978134348988533, 0.09662263840436935, 0.11363535374403, 0.10520170629024506, 0.10295256227254868, 0.10571381449699402, 0.09817162156105042, 0.09791181981563568, 0.09812031686306, 0.09993234276771545, 0.09965422749519348, 0.10429882258176804, 0.09260684251785278, 0.09667599201202393, 0.09978627413511276, 0.09844498336315155, 0.10246523469686508, 0.09910643845796585, 0.09179788082838058, 0.1012367382645607, 0.10015659779310226, 0.10209603607654572, 0.09727310389280319, 0.09967949986457825, 0.10112866014242172, 0.10274506360292435, 0.10843388736248016, 0.09892414510250092, 0.09773727506399155, 0.10482367128133774, 0.09864234179258347, 0.10792328417301178, 0.09892096370458603, 0.10121859610080719, 0.10763490945100784, 0.10788938403129578, 0.10062453895807266, 0.10417608916759491, 0.10263628512620926, 0.10883855819702148, 0.09827473014593124, 0.1030900850892067, 0.10253355652093887, 0.10166721791028976, 0.10132431983947754, 0.1059655100107193, 0.10216964036226273, 0.10080360621213913, 0.09939610958099365, 0.09997053444385529, 0.10191451013088226, 0.10714760422706604, 0.0968322828412056, 0.09965172410011292, 0.0920407846570015, 0.10017090290784836, 0.09786590933799744, 0.10598938167095184, 0.10193118453025818, 0.10161372274160385, 0.10190744698047638, 0.09749196469783783, 0.09963405132293701, 0.0975717231631279, 0.09885688871145248, 0.10099850594997406, 0.10137791931629181, 0.10050085186958313, 0.10183257609605789, 0.10204032063484192, 0.10391424596309662, 0.10171021521091461, 0.10387358069419861, 0.10051834583282471, 0.10099374502897263, 0.09622933715581894, 0.10025150328874588, 0.09956495463848114, 0.09950203448534012, 0.10312730073928833, 0.10041270405054092, 0.09833021461963654, 0.10037180036306381, 0.10044237971305847, 0.09894904494285583, 0.10189715772867203, 0.09814812988042831, 0.10149477422237396, 0.10067112743854523, 0.1025688499212265, 0.10402319580316544, 0.10249177366495132, 0.10342731326818466, 0.1022210493683815, 0.10216818004846573, 0.09590257704257965, 0.0990118682384491, 0.10083862394094467, 0.09698638319969177, 0.10050111263990402, 0.10113061219453812, 0.09823109209537506, 0.09786996990442276, 0.09856227040290833, 0.10806448757648468, 0.1016937792301178, 0.10117242485284805, 0.10336028039455414, 0.0998644307255745, 0.09679357707500458, 0.10927929729223251, 0.10151683539152145, 0.09839487820863724, 0.09203959256410599, 0.10407111793756485, 0.10757184773683548, 0.09904985874891281, 0.1011979803442955, 0.09827041625976562, 0.10539859533309937, 0.09867168217897415, 0.10600540041923523, 0.10212752968072891, 0.10407954454421997, 0.09722331911325455, 0.10291098803281784, 0.10046391934156418, 0.10397697240114212, 0.09804065525531769, 0.09904062747955322, 0.10819807648658752, 0.09437602013349533, 0.10257091373205185, 0.09781060367822647, 0.1027323380112648, 0.10010041296482086, 0.09974536299705505, 0.10374165326356888, 0.09483535587787628, 0.09929269552230835, 0.09834310412406921, 0.10482555627822876, 0.09464329481124878, 0.09885586053133011, 0.107756607234478, 0.09962283819913864, 0.09927583485841751, 0.09868784248828888, 0.09870494902133942, 0.10067517310380936, 0.10154002159833908, 0.10185576975345612, 0.0972217470407486, 0.1031724289059639, 0.10268497467041016, 0.10143958032131195, 0.09785415232181549, 0.09892495721578598, 0.10103890299797058, 0.10227376967668533, 0.10362285375595093, 0.09645837545394897, 0.10638158023357391, 0.10110867023468018, 0.10327597707509995, 0.09809727966785431, 0.09321349859237671, 0.09801613539457321, 0.09886658191680908, 0.10022866725921631, 0.0988764539361, 0.10283250361680984, 0.09801775962114334, 0.10062593966722488, 0.09674864262342453, 0.10474838316440582, 0.09615248441696167, 0.09902968257665634, 0.10134704411029816, 0.10107821226119995, 0.09690794348716736, 0.10202719271183014, 0.10039430856704712, 0.09998694062232971, 0.09768877923488617, 0.1055077314376831, 0.10802385956048965, 0.09650909900665283, 0.10085400193929672, 0.1058574840426445, 0.09824273735284805, 0.10174669325351715, 0.10384261608123779, 0.09822022914886475, 0.10475020855665207, 0.10088875889778137, 0.0990997850894928, 0.09863648563623428, 0.09893979877233505, 0.10484649240970612, 0.09747271239757538, 0.10214860737323761, 0.10118724405765533, 0.09714958071708679, 0.10216006636619568, 0.10923150181770325, 0.09112979471683502, 0.10141627490520477, 0.10556405037641525, 0.1050465852022171, 0.10376675426959991, 0.10664813965559006, 0.10227753221988678, 0.09815045446157455, 0.10172730684280396, 0.09534278512001038, 0.09789365530014038, 0.1010371744632721, 0.10071191936731339, 0.10640408843755722, 0.09997709840536118, 0.10569415986537933, 0.09745210409164429, 0.09810888022184372, 0.09926655143499374, 0.09881703555583954, 0.09674596786499023, 0.09801094233989716, 0.09489224851131439, 0.10065297782421112, 0.09989835321903229, 0.0957924872636795, 0.09897676110267639, 0.09969592094421387, 0.1035577580332756, 0.09350154548883438, 0.10449566692113876, 0.10343664884567261, 0.09865713119506836, 0.1000378355383873, 0.09982305765151978, 0.10276390612125397, 0.09812799096107483, 0.09775844216346741, 0.09753783792257309, 0.09941679239273071, 0.10116327553987503, 0.09835001081228256, 0.10477427393198013, 0.10213694721460342, 0.10065159946680069, 0.10140545666217804, 0.10353896021842957, 0.10410977154970169, 0.09588407725095749, 0.09437020123004913, 0.10184334218502045, 0.09986528754234314, 0.10404351353645325, 0.10634652525186539, 0.10172262787818909, 0.09745544195175171, 0.10346370935440063, 0.09981561452150345, 0.10045118629932404, 0.10381687432527542, 0.10100145637989044, 0.10547077655792236, 0.102044016122818, 0.10513049364089966, 0.10144001990556717, 0.09649599343538284, 0.1002124473452568, 0.09707336872816086, 0.10090477019548416, 0.10237495601177216, 0.10185716301202774, 0.10358899086713791, 0.09928054362535477, 0.08643838763237, 0.10066871345043182, 0.0993414893746376, 0.09779775142669678, 0.10254545509815216, 0.10043162852525711, 0.10419900715351105, 0.09925489127635956, 0.09837812930345535, 0.09479901194572449, 0.10091936588287354, 0.10087752342224121, 0.10403063893318176, 0.10375411063432693, 0.10606438666582108, 0.10170488059520721, 0.09952450543642044, 0.10200650244951248, 0.10158589482307434, 0.0990680456161499, 0.10711842775344849, 0.10424916446208954, 0.10414648801088333, 0.09678556025028229, 0.09998980164527893, 0.09988197684288025, 0.09501009434461594, 0.09997393190860748, 0.09808472543954849, 0.10235871374607086, 0.10055568814277649, 0.10111341625452042, 0.10647083073854446, 0.10335151106119156, 0.1030208021402359, 0.09482444822788239, 0.10119810700416565, 0.10152556747198105, 0.10147752612829208, 0.1086934506893158, 0.10376414656639099, 0.10051500052213669, 0.09956859052181244, 0.09904266148805618, 0.10111626237630844, 0.09717933088541031, 0.10026275366544724, 0.106450654566288, 0.10293667763471603, 0.10173357278108597, 0.10028710961341858, 0.10097038000822067, 0.10160985589027405, 0.10193311423063278, 0.10389289259910583, 0.10252723097801208, 0.09647345542907715, 0.10331409424543381, 0.09849396347999573, 0.10782325267791748, 0.0962783694267273, 0.10119523108005524, 0.09795127063989639, 0.10116318613290787, 0.09968800842761993, 0.10001600533723831, 0.09337527304887772, 0.09855201840400696, 0.0990380346775055, 0.10140977799892426, 0.10713822394609451, 0.10023927688598633, 0.10269661247730255, 0.10091345757246017, 0.10466454923152924, 0.0991549864411354, 0.10667107254266739, 0.1015838012099266, 0.10057874023914337, 0.10328173637390137, 0.10276944935321808, 0.10332821309566498, 0.10216901451349258, 0.10183101892471313, 0.09857583045959473, 0.09755492210388184, 0.11129797995090485, 0.10035142302513123, 0.10493345558643341, 0.10041949152946472, 0.10031957924365997, 0.10629559308290482, 0.10022318363189697, 0.1025690957903862, 0.1036650538444519, 0.10553345084190369, 0.09791475534439087, 0.09733540564775467, 0.10252176970243454, 0.09896379709243774, 0.10212984681129456, 0.09853965044021606, 0.09724388271570206, 0.09730840474367142, 0.10463026165962219, 0.10036665201187134, 0.10620547086000443, 0.10278508812189102, 0.1041073203086853, 0.10595378279685974, 0.10015872865915298, 0.10107453912496567, 0.10439467430114746, 0.09867023676633835, 0.09921403229236603, 0.10309071838855743, 0.10186487436294556, 0.09747517108917236, 0.09941745549440384, 0.10258662700653076, 0.1011619120836258, 0.09909812361001968, 0.10350054502487183, 0.10456882417201996, 0.09787289798259735, 0.10316264629364014, 0.0965290516614914, 0.1064995527267456, 0.09564151614904404, 0.09805049002170563, 0.10550810396671295, 0.10779020190238953, 0.10264910012483597, 0.1043659895658493, 0.11025536060333252, 0.1004384383559227, 0.10155084729194641, 0.09893512725830078, 0.10614307224750519, 0.09900857508182526, 0.10578121989965439, 0.09998495131731033, 0.09561969339847565, 0.09989480674266815, 0.0986161082983017, 0.10242543369531631, 0.10033564269542694, 0.10090193152427673, 0.09867766499519348, 0.10951288789510727, 0.1014912948012352, 0.09981990605592728, 0.10230693966150284, 0.10175768285989761, 0.10507594794034958, 0.10014453530311584, 0.0994371622800827, 0.10312141478061676, 0.10092391818761826, 0.09970085322856903, 0.10375460237264633, 0.10258188843727112, 0.10474173724651337, 0.10477427393198013, 0.091435506939888, 0.10617362707853317, 0.1060539186000824, 0.09496305882930756, 0.10731149464845657, 0.10123445838689804, 0.09783978015184402, 0.10326268523931503, 0.09986932575702667, 0.10139487683773041, 0.10165862739086151, 0.10052908211946487, 0.1023835614323616, 0.09954139590263367, 0.09900501370429993, 0.09761201590299606, 0.10415174067020416, 0.10317596793174744, 0.09702129662036896, 0.10365722328424454, 0.1009029746055603, 0.10370146483182907, 0.10300887376070023, 0.10035239160060883, 0.09804991632699966, 0.09861017763614655, 0.10262370109558105, 0.0958077609539032, 0.10100813955068588, 0.10100618749856949, 0.10690036416053772, 0.09627218544483185, 0.09383104741573334, 0.09895811975002289, 0.0962563008069992, 0.10064634680747986, 0.09937062859535217, 0.09805214405059814, 0.09955166280269623, 0.10220900177955627, 0.09262032806873322, 0.09755920618772507, 0.09937935322523117, 0.10746977478265762, 0.09957776218652725, 0.09997758269309998, 0.09828481823205948, 0.10707331448793411, 0.09932708740234375, 0.10101102292537689, 0.10063609480857849, 0.10291808098554611, 0.09796508401632309, 0.1063322052359581, 0.10263218730688095, 0.09798558801412582, 0.10528551787137985, 0.10767471045255661, 0.10148555040359497, 0.10236160457134247, 0.1028180941939354, 0.10046854615211487, 0.09885688871145248, 0.09806398302316666, 0.10882896184921265, 0.1007126048207283, 0.09734968096017838, 0.1034850925207138, 0.10474409908056259, 0.10736868530511856, 0.09977208077907562, 0.10321714729070663, 0.09769199788570404, 0.09872383624315262, 0.09793602675199509, 0.10264638066291809, 0.10417373478412628, 0.10072671622037888, 0.09929810464382172, 0.09380039572715759, 0.10182654857635498, 0.1034422218799591, 0.09774497151374817, 0.1012285128235817, 0.10394202917814255, 0.10302874445915222, 0.10491860657930374, 0.09784325957298279, 0.10073439031839371, 0.10143248736858368, 0.10144080221652985, 0.09602468460798264, 0.099219411611557, 0.10169960558414459, 0.10185887664556503, 0.10382288694381714, 0.11279846727848053, 0.10086794197559357, 0.11767671257257462, 0.10107732564210892, 0.09866400808095932, 0.09981703013181686, 0.0972614511847496, 0.1033543273806572, 0.10381687432527542, 0.105256587266922, 0.10168606787919998, 0.09835824370384216, 0.1004088893532753, 0.10873937606811523, 0.09869785606861115, 0.10150353610515594, 0.10291581600904465, 0.10282815247774124, 0.0983072817325592, 0.10076333582401276, 0.10042540729045868, 0.10040213912725449, 0.10340817272663116, 0.0944526195526123, 0.0979558452963829, 0.10698267817497253, 0.1036907359957695, 0.1039256826043129, 0.10344544053077698, 0.09735798835754395, 0.10029671341180801, 0.09631512314081192, 0.09818871319293976, 0.10321367532014847, 0.10506395995616913, 0.10123272240161896, 0.1061740294098854, 0.09131094813346863, 0.10208135843276978, 0.09831748902797699, 0.10116583853960037, 0.10610633343458176, 0.10135349631309509, 0.10154803842306137, 0.09932000935077667, 0.10123755037784576, 0.1057557538151741, 0.10477329045534134, 0.10176440328359604, 0.11128036677837372, 0.09985214471817017, 0.10141134262084961, 0.10549909621477127, 0.10807767510414124, 0.09864887595176697, 0.09924895316362381, 0.10161464661359787, 0.10319949686527252, 0.1017289012670517, 0.10840529203414917, 0.10520201176404953, 0.10260072350502014, 0.09331165254116058, 0.09866410493850708, 0.1018921360373497, 0.10296919196844101, 0.09628623723983765, 0.105851911008358, 0.1008368730545044, 0.10031070560216904, 0.10482647269964218, 0.10387419909238815, 0.09963291883468628, 0.09993725270032883, 0.09830789268016815, 0.10864771902561188, 0.09762841463088989, 0.1037578284740448, 0.10616849362850189, 0.09949275106191635, 0.09726254642009735, 0.10463568568229675, 0.10389979928731918, 0.10084252804517746, 0.10131856799125671, 0.08159704506397247, 0.10399523377418518, 0.10364078730344772, 0.10009549558162689, 0.10113455355167389, 0.1052137240767479, 0.10088377445936203, 0.10564623773097992, 0.10233842581510544, 0.09661947935819626, 0.1008218452334404, 0.10014417767524719, 0.10104367882013321, 0.10100402683019638, 0.09803640842437744, 0.10209338366985321, 0.09750996530056, 0.10101967304944992, 0.09389111399650574, 0.09545682370662689, 0.10594723373651505, 0.10412280261516571, 0.10323271155357361, 0.09995502978563309, 0.09525172412395477, 0.10191641747951508, 0.10141652077436447, 0.0992264673113823, 0.10099463909864426, 0.09803129732608795, 0.09841736406087875, 0.10001102834939957, 0.10489077866077423, 0.10005172342061996, 0.09904792159795761, 0.10040783882141113, 0.10043247044086456, 0.09665258228778839, 0.10272436589002609, 0.0979321300983429, 0.10240410268306732, 0.099699467420578, 0.09985323250293732, 0.09996577352285385, 0.10184753686189651, 0.10493341833353043, 0.09958259761333466, 0.10374270379543304, 0.10143210738897324, 0.10562867671251297, 0.10139960795640945, 0.10416384041309357, 0.1017720103263855, 0.09952162206172943, 0.09147096425294876, 0.0954640805721283, 0.10377886146306992, 0.10642370581626892, 0.10453400015830994, 0.10111123323440552, 0.10322540253400803, 0.10044936090707779, 0.09945237636566162, 0.09989482164382935, 0.10677999258041382, 0.10418591648340225, 0.09867875277996063, 0.10289948433637619, 0.1037081778049469, 0.09902258217334747, 0.10387104004621506, 0.09572345018386841, 0.1006808653473854, 0.10304506868124008, 0.09937524050474167, 0.09980382025241852, 0.10350580513477325, 0.10092513263225555, 0.09970738738775253, 0.09557637572288513, 0.10549072921276093, 0.10580433160066605, 0.09879609942436218, 0.1006217747926712, 0.09674090147018433, 0.10478711873292923, 0.1007734164595604, 0.09895764291286469, 0.09928707033395767, 0.10021400451660156, 0.102383553981781, 0.10056081414222717, 0.1001376286149025, 0.09424164146184921, 0.09653410315513611, 0.10225846618413925, 0.09748496860265732, 0.10094238072633743, 0.1023603081703186, 0.10401196777820587, 0.102596715092659, 0.10509489476680756, 0.10299793630838394, 0.0901704728603363, 0.10215526074171066, 0.09872650355100632, 0.09394504874944687, 0.09709523618221283, 0.09824451804161072, 0.10671359300613403, 0.10170449316501617, 0.10142292082309723, 0.10687556862831116, 0.09817516058683395, 0.10211843252182007, 0.09733285754919052, 0.09798933565616608, 0.09985402971506119, 0.10138438642024994, 0.10262663662433624, 0.10414991527795792, 0.09704455733299255, 0.09857778996229172, 0.1029871329665184, 0.09409169852733612, 0.0966726765036583, 0.10343620181083679, 0.092778779566288, 0.10597056150436401, 0.10150521993637085, 0.09975957870483398, 0.10113786160945892, 0.09383464604616165, 0.10086724907159805, 0.1087283194065094, 0.1085655614733696, 0.10826285183429718, 0.10505418479442596, 0.09819421917200089, 0.10615728795528412, 0.10107218474149704, 0.10351657122373581, 0.1031215488910675, 0.10088793188333511, 0.10181324928998947, 0.09918956458568573, 0.09824669361114502, 0.10041457414627075, 0.09810689091682434, 0.09482376277446747, 0.09980203211307526, 0.10147368907928467, 0.10171718150377274, 0.10083962976932526, 0.1035783588886261, 0.09970634430646896, 0.10476222634315491, 0.09810186922550201, 0.10062053799629211, 0.10218962281942368, 0.10148860514163971, 0.10143595933914185, 0.10058373957872391, 0.10133916139602661, 0.09931416809558868, 0.10034138709306717, 0.09437636286020279, 0.10172000527381897, 0.10270678997039795, 0.09867196530103683, 0.08987260609865189, 0.10051944851875305, 0.09999963641166687, 0.10245956480503082, 0.10461683571338654, 0.10519282519817352, 0.09972929209470749, 0.10312126576900482, 0.09944958984851837, 0.10031691938638687, 0.10123593360185623, 0.10393558442592621, 0.09822243452072144, 0.09854523837566376, 0.10036743432283401, 0.10039021074771881, 0.10051285475492477, 0.08913533389568329, 0.09793217480182648, 0.09730156511068344, 0.10075147449970245, 0.10774640738964081, 0.10335490107536316, 0.10428713262081146, 0.10805206000804901, 0.10541218519210815, 0.0936727300286293, 0.09182193875312805, 0.10331258177757263, 0.10333294421434402, 0.09502020478248596, 0.10322020202875137, 0.10545194149017334, 0.09981687366962433, 0.09526517242193222, 0.09712281823158264, 0.0993478000164032, 0.10451053082942963, 0.10147888213396072, 0.0975206196308136, 0.10531911253929138, 0.09538827836513519, 0.10225100070238113, 0.10073134303092957, 0.1042991653084755, 0.09633806347846985, 0.09579963982105255, 0.10140281915664673, 0.10174375027418137, 0.09450089931488037, 0.10543690621852875, 0.10168997943401337, 0.10215426981449127, 0.0980176255106926, 0.09866982698440552, 0.10439164191484451, 0.09453801810741425, 0.09697962552309036, 0.09829834848642349, 0.10321910679340363, 0.10297439992427826, 0.10066013038158417, 0.1040075495839119, 0.09788545221090317, 0.09843994677066803, 0.08693654835224152, 0.10127957165241241, 0.09989414364099503, 0.10740752518177032, 0.09903977066278458, 0.09495225548744202, 0.0999358743429184, 0.10234246402978897, 0.10181142389774323, 0.08838386833667755, 0.10383064299821854, 0.10178260505199432, 0.10039104521274567, 0.10371212661266327, 0.09967834502458572, 0.09847241640090942, 0.10039801150560379, 0.10194115340709686, 0.10220823436975479, 0.09791597723960876, 0.10007782280445099, 0.09346266090869904, 0.10102476179599762, 0.1016516238451004, 0.10960006713867188, 0.09858234226703644, 0.10186870396137238, 0.10853458940982819, 0.0989915281534195, 0.09997045993804932, 0.10000649094581604, 0.10056821256875992, 0.09838616847991943, 0.09827609360218048, 0.10289735347032547, 0.10280775278806686, 0.10332968086004257, 0.1020878329873085, 0.10242854803800583, 0.09533245116472244, 0.09874783456325531, 0.10375207662582397, 0.10005742311477661, 0.10019323229789734, 0.10181371867656708, 0.10630880296230316, 0.09745226055383682, 0.10504991561174393, 0.0994660034775734, 0.09946668893098831, 0.09842012822628021, 0.1028393805027008, 0.1023140400648117, 0.09810002893209457, 0.10178444534540176, 0.09170560538768768, 0.10171337425708771, 0.09921403229236603, 0.09905122220516205, 0.10363810509443283, 0.0995342805981636, 0.10613779723644257, 0.09208228439092636, 0.10055311024188995, 0.09855776280164719, 0.0980677455663681, 0.10270959138870239, 0.10049239546060562, 0.09532548487186432, 0.10261111706495285, 0.10231718420982361, 0.09995502978563309, 0.09801401197910309, 0.10156945884227753, 0.10495080798864365, 0.09900784492492676, 0.09903883934020996, 0.10188769549131393, 0.0987691879272461, 0.09925894439220428, 0.10108012706041336, 0.0999876856803894, 0.10055858641862869, 0.0997786894440651, 0.10108685493469238, 0.09432481974363327, 0.10150901973247528, 0.10249973088502884, 0.10976295173168182, 0.10015764832496643, 0.10316522419452667, 0.10581295937299728, 0.09995803982019424, 0.10166031867265701, 0.09710167348384857, 0.09978362917900085, 0.10231823474168777, 0.10545194149017334, 0.09787033498287201, 0.09683605283498764, 0.0993218868970871, 0.10435868799686432, 0.0946083813905716, 0.10549680888652802, 0.10038149356842041, 0.10742658376693726, 0.09944181889295578, 0.10155617445707321, 0.09813220053911209, 0.10186555236577988, 0.10155457258224487, 0.09953078627586365, 0.10103888809680939, 0.10374438017606735, 0.10136499255895615, 0.1020292341709137, 0.10351349413394928, 0.10539945960044861, 0.09990301728248596, 0.09648358076810837, 0.1023704782128334, 0.10089369118213654, 0.10168100148439407, 0.100666843354702, 0.10673198848962784, 0.1016760841012001, 0.10389893501996994, 0.10019034892320633, 0.10049744695425034, 0.1001610979437828, 0.10505843162536621, 0.10118544101715088, 0.10847417265176773, 0.0972423106431961, 0.10144530981779099, 0.10146738588809967, 0.10472278296947479, 0.10001534968614578, 0.09987560659646988, 0.10087932646274567, 0.10317672789096832, 0.1034495010972023, 0.10813010483980179, 0.10027337819337845, 0.10012596100568771, 0.10458163917064667, 0.10288145393133163, 0.10288713127374649, 0.0966707393527031, 0.10539930313825607, 0.10237472504377365, 0.0957416221499443, 0.10005372017621994, 0.10104161500930786, 0.09610701352357864, 0.09644562751054764, 0.09731381386518478, 0.10405562072992325, 0.09788176417350769, 0.0944829136133194, 0.11167062819004059, 0.09769701957702637, 0.0996849536895752, 0.09815983474254608, 0.09890700876712799, 0.09663565456867218, 0.09060831367969513, 0.10512833297252655, 0.10554757714271545, 0.09968724101781845, 0.09983965754508972, 0.09718049317598343, 0.10441268980503082, 0.09665590524673462, 0.09497427940368652, 0.09562018513679504, 0.09702670574188232, 0.09713366627693176, 0.1087745726108551, 0.09890104830265045, 0.0992390513420105, 0.10262621939182281, 0.10307204723358154, 0.09843874722719193, 0.10443655401468277, 0.09587045013904572, 0.10494446009397507, 0.10510540008544922, 0.10732457041740417, 0.09772525727748871, 0.1039014607667923, 0.10026033967733383, 0.09594486653804779, 0.10100849717855453, 0.10308030247688293, 0.10193486511707306, 0.09753867238759995, 0.10434102267026901, 0.11040548980236053, 0.09919245541095734, 0.09713193029165268, 0.09983405470848083, 0.10205554217100143, 0.10648015141487122, 0.10302604734897614, 0.10486900806427002, 0.10501667857170105, 0.09768987447023392, 0.09887389093637466, 0.1008521318435669, 0.09881100803613663, 0.09920310229063034, 0.10155674070119858, 0.10472054034471512, 0.0989776998758316, 0.09815894067287445, 0.09414944797754288, 0.09775947034358978, 0.0976046547293663, 0.10287836194038391]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Likelihood of genders for true negative (male)\"}, \"xaxis\": {\"title\": {\"text\": \"Likelihood male\"}}, \"yaxis\": {\"title\": {\"text\": \"Likelihood female\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('02a8abc6-41c5-4693-aad9-6e5135894d38');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}